jean.leon.v@outlook.com


* IAAS vs PAAS:
	- App Service => Platform as a Service
	- Virtual Machines => Infrastructure as a Service  
Repasar imagen


* AZURE APP SERVICE
	* FEATURES:
		- Windows (all TIERS)
		- Linux (from FREE)
		- Built-in AUTO scale (scale up/down -> RESOURCES, scale out/in -> INSTANCES) (from BASIC)
		- Containers support
			- Windows (from PREMIUM)
			- Linux (from FREE)
			- Pull images from private registry, docker hub or Azure Container Registry
		- CI/CD
			- Azure DevOps, Github, Bitbucket, FTP, local git repo, Azure Container Registry
		- Custom domains (From BASIC)
		- Azure app service certiticate (From BASIC)
		- SSL Connection (From BASIC)
		* DEPLOY:
			- Automatic: Using CI/CD
			- Manual: Git, CLI (az webapp up), ZIP (using curl), FTPS/FTP
			- Containers:
				- Build and tag the image (Build pipeline step)
				- Push tagged image (Push to registry)
				- Update the slot with new image tag (Updating the tag automaticlly restarts the app app and pulls the new image)
		* COMMANDS:
			- create/update webapp (static page)
				- az webapp up -g $resourceGroup -n $appName --html
				â€¦ New-AzWebApp
		* NETWORKING:
			- INBOUND FEATURES: App-assigned address, Access restrictions, Service endpoints, Private endpoints
			- OUTBOUND FEATURES: Hybrid Connections, Gateway-required virtual network integration, Virtual network integration
	
	* SLOTS (from STANDARD):
		- Live apps with their own host names (separate deployment)
		- Standard (5 max), Premium (20 max)
		- App content and config can be swapped between two slots (for example, staging <-> production)
		- Recommended: Deploy new app version to "staging" slot, then swap "staging" and "production" slots. This allows fast rollback if needed
		- You can't swap webapps slots between different app service plans
		- Internal Swap process:
			1) Apply settings (slot app settings, conn strings, CD settings, auth settings) from target slot to all instances of source slot
				(after this, all source instances are triggered to restart)
			2) Wait for every source slot instance to complete their restart (if restart fails, swap is reverted)
			3) If local cache is enabled, trigger local cache init by making an HTTP request to app root ("/") on each slot instance. Wait for any instance response.
				This triggers another restart on each instance
			4) If autoswapp is enabled with custom warm-up, trigger app init by making an HTTP request to app root ("/") on each slot instance.
				- If you need to run scripts or some init-resources before swap, you need to use "applicationInitialization" in web.config file (custom init action)
					(WEBSITE_SWAP_WARMUP_PING_PATH and WEBSITE_SWAP_WARMUP_PING_STATUSES)
				- If an instance returns any response, it's considered warmed up.
			5) If all instances on the source slot are WARMED UP, swap by switching routing rules
			6) Apply all source settings to swapped content in source slot and restart again.
			#########################################################################################################################
			- All work of initializing the swapped apps (and warmed up) happens on the source slot, the target slot remains online.
			- The production slot should be always the target (so swap doesn't affect production app)
			- Settings that are swapped (NON-STICKY SETTINGS):
				- General settings, app settings, connection strings, handler/path mappings, public certificates, Webjobs content
			- Settings that aren't swapped (STICKY SETTINGS):
				- Publishing endpoints, custom domain, non public certs and tls/ssl settings, scale settings, webjos schedules, IP restrictions,
					always on, diagnostic log settings, CORS, virtual nw integration, managed identities
			- To make settings swappable set variable WEBSITE_OVERRIDE_PRESERVE_DEFAULT_STICKY_SLOT_SETTINGS = 0 / false
		- Swap with preview
			- Apply settings from target to source and you can validate the "result" before to swap to target (prod). You can continue or cancel the swap
		- Autoswap (only for Windows)
			- Enable and select target in general settings
			- For Azure DevOps scenarios (CI/CD, every time push the changes to slot, app service swap app into prod)
		- If swap fails, swap the same two slots INMEDIATELY and inspect activity log
		- Route traffic:
			- You can route a portion of the traffic (to prod slot) to another slot (to receive user feedback for features that aren't ready to release)
			- Once the client is routed to slot, it's pinned to that slot during the session's life
			- You give the user a button to "test beta" (url + route cookie) and route to non-prod slot.
			- Default routing rule (prod o specific slot) in new slot is 0%
	
	* APP SERVICE PLAN:
		- Define the set of compute resources and pricing:
			- Operating system
			- Region
			- Number of VMs
			- Size of VMs (small, medium, large)
			- Pricing tier categories:
				- SHARED COMPUTE (No scale-out)
					- Free (F1), 
					- Shared (D1)
				- DEDICATED COMPUTE (Allow scale-out, only apps in the same plan share resources)
					- Basic (B1, B2, B3) (3 instances)
					- Standard (S1, S2, S3) (10 instances max)
					- Premium, PremiumV2, PremiumV3 (30 instances max)
				- ISOLATED (Maximun scale-out capabilities)
					- Isolated, IsolatedV2
		- An app always runs in an app service plan.
		- App service plan ------many----> App
		- Instances:
			- App service plan tier has INSTANCES
			- App runs on all VMs configured in APP SERVICE PLAN
			- All apps in the same APP SERVICE PLAN share the same VMs
			- All app slots run on the same VMs
			- Diagnostic logs, perform backups, webjobs (if enabled) use CPU and memory on these VMs
	
	* SCALING:
		- Keys
			- Autoscaling performs scaling IN and OUT (not UP and DOWN). It doesn't have any effect on CPU, memory, etc, It only changes the number of web servers of the APP SERVICE PLAN
			- Autoscaling is not an effective approach when the web app performs resource-intensive as part of each requuest (It needs scale-up)
			- Autoscaling isn't the best approach to handling LONG-TERM growth because autoscaling will end up being more expensive (if you can anticipate the rate of growth, do it manually)
			- For STANDARD tier or better (MANUAL in BASIC)
			- If you want to improve an specific app performance, you can try to moving the app to a separate APP SERVICE PLAN (different region, resource-intensive app, etc)
			- Autoscaling can't create more instances than the limit (tier limit), to prevent runaway autoscaling
		- Autoscaling Types:
			- Autoscaling with Azure autoscale
				- Types of condition:
					- Based on metrics rules (you define)
					- To a specific number of instances according to a schedule (you define)
				- Metric rules:
					- Autoscaling -> conditions -> rules
					- Azure can monitor:
						- CPU%, memory%, disk queue length (I/O requests), http queue length (client requests), data in, data out
					- Azure analyzes metrics:
						- Aggregates values for a metric of ALL INSTANCES within the TIME GRAIN (1 min default)
						- DURATION time is defined to aggregates the aggregation values within TIME GRAIN
						- Aggregation can be:
							- AVERAGE, MINIMUM, MAXIMUM, SUM, LAST, COUNT
					- Autoscale actions is triggered when a metric treshold has crossed based on rule (operators like greater then, equal to, etc)
					- Autoscale has a cool down period to stabilize between autoscale events
				- Combining Rules in the same condition
					- SCALE OUT RULE is performed if ANY of the scale-out rules are met
					- SCALE IN RULE is performed if ALL of the scale-in rules are met
					- You can perform scale-in rules independentlly separating scale-in rule in different condition
				- You can create multiple conditions of the two types and you can combine them
				- There is a default condition, and this doesn't have schedule
				- A rule specifies threshold for a metric and triggers an autoscale event when threshold is crossed.
				- Deallocate resources when workload has diminished
			- Azure app service automatic scaling
				- For PREMIUMV2 or better
				- Based on parameters you've selected
				- Azure automaticlly handles scaling decisions for web apps and app service plans
				- Use it if you have you webapp connected to legacy system wich may not scale as fast as the webapp setting the maximun number of instances to scale)
		- Best practices
			- Ensure the maximum and minimum values are different and have an adequate margin between them
			- Choose the appropriate statistic for your diagnostics metric (AVERAGE, MINIMUM, MAXIMUM, SUM, LAST, COUNT)
			- Choose the thresholds carefully for all metric types
			- Always select a safe default instance count
			- Configure autoscale notifications


	* APP CONFIGURATION:
		- General settings:
			- Stack settings:
				- Language, SDK (pyton, php, node, java, .net), Start-up command (only for linux and containers)
			- Platform settings:
				- Platform bitness (32-b / 64-b, only for Windows)
				- FTP state
				- Http version
				- Web sockets (enable or disable)
				- Always On:
					- Enabled: Azure send continuos ping preventing the ap from being unloaded
					- Disabled: App is unloaded after 20 min without requests (high latency for new requests after sleep)
				- Session affinity (ensure a client is routed to the same instance during the session, for statefull apps)
				- Https only
				- Minimum TLS version
			- Debugging
			- Incoming client certificates (Require client certificates in mutual authentication)
		- Path mappings:
			- Handler mappings: If you need to csutomize how certain files are processde (e.g., directing .php files to specific handler)
				- Specific .py file should be handled for specific pyhton handler (define handler for .py)
			- Virtual application directory: If you need to manage app's folder structure or specific URL path (serve files via url)
				- https://yourapp.azurewebsites.net/media --> point to a sample storage directory
				- For custom paths, webapp serve files from site/wwwroot directly

		- ENVIRONMENT VARIABLES (application settings):
			- App service variables, for ASP.NET Core, It's like setting variables in appsettings.json (app service values override appsettings values).
			- App settings are always encrypted when stored
			- You can enable deployment slot settings (swappable setting)
			- For linux replace ":" by "_" for nested keys
			- Connection strings follow the same flow as app settings
		- Domain (DNS records you need to add with domain provider):
			- A RECORD: For root domain
			- CNAME RECORD: For subdomain or wildcard
			- TXT RECORD: Domain verification ID

	* DIAGNOSTIC LOGGING
		- Diagnose and solve problems: Use Azure service diagnostics (built-in) to look into deep insights (perfomance issues, memory leaks, etc)
		- Logging types:
			- Application logging (w, l): Messages and logs generated by the applitcation (framework/language) | app service file system / azure storage blobs
			- Web server logging (w): Raw http requests/responses data (emthod, uri, ip, port, status code, etc) | app service file system / azure storage blobs
			- Detailed error messages (w): Copies of the .html error pages (400 or greater, in no production env) | app service file system
			- Failed request tracing (w)
			- Deployment logging (w, l): Automatic logging for failed deployments | app service file system
		- Enable app logging in windows:
			- Enable file system (debug purposes) or blob logging (long-term), set category:
				Disabled: None
				Error: Error, Critical
				Warning: Warning, Error, Critical
				Information: Info, Warning, Error, Critical
				Verbose: Trace, Debug, Info, Warning, Error, Critical (all categories)
		- Enable app logging in linux/container:
			- Enable file system
			- Set disk quota and retention period.
		- Enable web server logging:
			- Enable file system or blob logging
			- Set retention days
		- ACCESS LOGS:
			- You can send specific traces to logs using packages of your language/framework
			- Log stream
			- az webapp log tail --name appname --resource-group myResourceGroup
			- Kudu
			- Consume blobs or download zip file if using file system

		- ADVANCED TOOLS (Kudu):
			- You can run cmd or powershell to interact with files of webapp in real time
			- You can view files and configuration of webapp (files deployed)
			- It's good for diagnostic (debug when It works in local but doesn't work in cloud environment)

	* SECURITY:
		- Certificates:
			- App service plan's resource group + region combination = WEBSPACE
			- A certifciate is accesible to other apps in the same WEBSPACE
			- Options:
				- Free app service managed private cert
				- Purchase app service private cert
				- Import cert from Azure Key Vault
				- Upload private/public cert
			- Free managed cert:
				- For BASIC tier or better
				- Renewed continuously and automaticlly in six month increments, 45 dayse before expiration
				- Vreate the cert and bind to a custom domain.

		- AUTHENTICATION / AUTHORIZATION:
			- Run in the same sandbox as the app code (for containers and linux, run in a separate container)
			- When enabled, it acts like a middleware (receive every incoming request)
			- Provides built-in authentication and authorization support
			- Out-of-the-box auth with federated identity providers (endpoints for each provider):
				- Microsoft identity platform, Facebook, Google, X, OpenID Connect provider, GitHub
			- Authentication Without SDK (with browser) / with SDK (browser-less app)
			- Authorization:
				- Allow unauthenticated request (authorization managed by the app)
				- Require authentication (apply to all requests)
			- Built-in token store
			- Auth traces are collected if application logging is enabled


* AZURE CONTAINERS

	* AZURE CONTAINER REGISTRY (ACR):
		- Store container images (Windows and Linux) (read only snapshots)
		- Pull/push images from azure services (AKS, App service, Service fabric, Batch) or in deployment workflows
		- You can use more than INCLUDED STORAGE up to storage limit (40TB for all ties) paying extra rate per GB
		- Tiers: (Similar capabilities, different storage and throughput)
				- BASIC = (INCLUDED STORAGE 10GB)
				- STANDARD = (INCLUDED STORAGE 100GB)
				- PREMIUM = (INCLUDED STORAGE 500GB) (high volume scenarios)
		- Authentication methods: azure entra Id, managed identity, admin user (recommended to restrict access: disable admin user and use RBAC)
		- Use microsoft defender for containers when need automatic image scanning for vulnerabilities

		* FEATURES:
			- Microsoft Entra authentication integration (ALL TIERS)
			- Image deletion (ALL TIERS)
			- Webhooks (ALL TIERS)
			- Encryption-at-rest (ALL TIERS, encrypt before storing images, decrypt when use)
			- Geo-replication (PREMIUM)
			- Zone redundancy (PREMIUM)
			- Scalable storage (ALL TIERS, up to storage limit)
			- Create -----> az acr create
			- List -----> az acr repository list
			- Versions -----> az acr repository show-tags
			- Run -----> az acr run (like docker run)

		* ACR Taks:
			- Builds images for LinuxOS and AMD64 arch.
			- Streamline building, testing, pushing, deployment in Azure (automation)
			- Has associated source code context (Git repo specifying branch), tasks use
			- Build automaticlly when commits code to source
			- USE-CASES:
				- docker push: to push images (independent)
				- Quick task -----> Like "docker build", "docker push" in azure (without docker engine, on demand) -----> az acr build (reduces local resources usage by building in cloud)
				- Triggered tasks -----> Trigges to build on code update, base image update, schedule -----> az acr task create / track base image dependency / schedule
				- Multi-step task -----> It uses a .yaml file to define steps (custom workflow) 

		* Role to allow image pulls = AcrPull

	* AZURE CONTAINER INSTANCES (ACI):
		- No VM is needed, allows Windows and Linux
		- Max resources: 4 CPU cores and 16 GB memory
		- Focus: Ideal for tasks like batch/background jobs, simple stateless apis, isolated workloads without scaling or complex orchestration (dependencies)
		- Container instances are billed by the second (you need to stop when not using)
		- No support for persistent storage (natively)
		- You can run ACI in a private network by deploying the app in a VNET

		* SCENARIOS:
			- Simple web application in a container (Frontend container + Backend container)
			- App container and logging container
			- App container and monitoring container

		* FEATURES:
			- Fast startup
			- Mount Azure Files shares directly to a container to retrieve and persist state

		* Container groups (multi container groups):
			- Max number of containers per group = 60
			- Max number of volumes per group = 20
			- Similar to kubernetes POD, collection of container on the same host machine
			- You can specify the volumes to create, and the AZURE FILES SHARES to mount, you can only mount AZURE FILES SHARES to Linux containers
				("volumeMounts" and "volumes" properties of .yaml), you can do that using .yaml, command or Resource manager template
			- Exposes single public IP with port, can be assigned to a DNS name label, use AZURE FILES SHARES as volume mount (for each container)
			- Multi-containers for LINUX containers, Single-container for WINDOWS containers
			- Deploy using command, .YAML or Resource manager template (recommended .yaml)
			- All containers share one IP address and a common port namespace
			- Doesn't support port mapping (exposed port is the same as container port, if exposed). Containers internally comunicates each other via localhost and port
			- Restart policy:
				- ACI stops the container when its app, or script exits
				- ALWAYS (default), NEVER, ONFAILURE

		* Environment Variables:
			- az container create ... --environment-variables 'MaxProcesses'='10'
			- To secure values use "secureValue" property in .yaml file when set the variable

		* Commands:
			- az container create ... --file deploy.yaml
			- az container show
			- az container create ... --restart-policy OnFailure

	* AZURE CONTAINER APPS (ACA):
		- Supports any Linux-based x86-64 linux/amd64 image
		- No overhead of managing complex infrastructure (this service runs on top of AKS)
		- ACA doesn't support direct integration with AKS
		- Focus: Microservices applications and robust autoscaling, even-driven apps, background procesing apps, api endpoints
		- Scaling based on: Http traffic, event-driven processing, CPU, memory, etc (KEDA-supported scaler)
		- Can split traffic across multiple versions of an app
		- A container app can contain multiple containers (SIDECAR pattern == app and a log agent for example) (Advanced scenario)
		- It doesn't have multi container groups, instead it has container app containing multiple containers
		
		- MICROSERVICES:
			- Independent scaling, versioning, upgrades
			- Service discovery
			- Native DAPR integration
			- DAPR integration provides: observability, pub/sub, service-to-service, retries, etc.
			- DAPR is used for service discovery and communication
			- Use kubernetes-based service mesh for distributed applications
			- Persistent storage
			- Custom domains
			- Built-in load balancing

		* CONTAINER APPS ENVIRONMENT:
			- Secure boundary around groups of container apps
			- An environment can contain many container apps
			- Manage related services
			- Instrument DAPR applications
			- Same virtual network and log to the same LOG ANALYTICS WORKSPACE, and share the same DAPR conf
			- Different environment = apps never share the same compute resources, DAPR apps can't communicate
			- Environment ---contains---> container apps ---contains---> revisions, replica

		* KEY FEATURES:
			- Built-in autoscaling based on HTTP traffic

		* REVISIONS:
			- Azure container apps create versioning by creating REVISIONS (immutable snapshot)
			- Quickly revert to an earlier version
			- New revision is created when REVISION-SCOPE changes are applied (revision suffix, container conf, image, scale rules)
			- By default a first revision is created (test-app -----> test-app--1st-revision, in this case, suffix = "1st-revision")

		* SECRETS:
			- Adding, removing or changing secrets doesn't create new revisions
			- az containerapp create ...  --secrets "super-secret=$CONNECTION_STRING"
			- You can use the secrets in env-vars when creating the container app
			- Container Apps doesn't support Azure Key Vault integration. Instead, enable managed identity in the container app and use the Key Vault SDK in your app to access secrets.

		* Commands:
			- az containerapp env create ...
			- az containerapp create ...
			- az containerapp create ...  --secrets "super-secret=$CONNECTION_STRING" ------> create container app with secrets
			- az containerapp update ... ------> modify env variables, compute resources, scale, deploy diff image.
			- az containerapp revision list ... -----> list rvisions of container app
			- func azurecontainerapps deploy ------> deploy a containerized app using functions core tools

		* AUTHORIZATION AND AUTHENTICATION:
			- Provides out-of-the-box auth (built-in auth features, low or no code) with federated identity providers
			- Built-in auth SHOULD be used with "allowInsecure" disabled and HTTPS
			- Identity providers: Microsoft identity platform, facebook, githun, google, x, openID
			- When use those providers, the SIGN-IN endpoint is available for auth token
			- The authentication and authorizations feature acts using a middleware component (for each HTTP request), runs as a SIDECAR container on each replica
			- You can use "With SDK flow" (federated) or "Without SDK flow" (client)


* AZURE FUNCTIONS:
	- Serverless solution that allows you to write less code
	- Less infrastructure and save on costs
	- It supports integration with logic apps
	- Does'nt support Windows containers
	- If an Azure Function does not complete within the timeout period, it may be retried, leading to duplicate processing

	* AZURE FUNCTIONS vs LOGIC APPS:
		- Both can create complex orchestrations (collection of steps or actions)
		- Azure functions use code (imperative), while logic apps use gui (declarative)
		- Azure functions are monitored by application insights, while logic apps are monitered by azure portal or azure monitor logs
		- Logic apps can execute in on premises context, azure functions cannot

	* AZURE FUNCTIONS vs WEBJOBS
		- Azure functions is built on the WebJobs SDK (it shares many of the eevnt triggers and connectors with other services)
		- Azure functions has features that webjobs doesn't
			- Serverless app with autoscaling
			- Pay-per-use pricing
			- Offers more triggers thatn webjobs (http/webhooks, event grid triggers)
			- More options for programming languages and azure service integration

	* HOSTING OPTIONS
		- functionTimeout in host.json (time out parameter, timeout duration for function executions)
		- Hosting option specify how the function scale, resource available for funtcion app instances, advanced functionalities, support linux container
		- Event driven Autoscale: Instances of the functions hosts are dinamiclly added and removed based on incoming events
		- When you create a function app, you must create or link to a general-purpose Azure Storage account that supports Blob, Queue, and Table storage (AzureWebJobsStorage)

		* CONSUMPTION PLAN
			- Default hosting plan
			- Pay for compute resources when functions are running (py-as-you-go)
			- WITHOUT virtual network integration
			- Autoscale: Event driven
			- Max scale out: 200 (W), 100 (L)
			- No containers support
			- Default functionTimeout: 5m
		* FLEX CONSUMPTION PLAN
			- High scalability with compute choices (key benefit) and pay-as-you-go
			- Virtual network integration 
			- Event driven autoscale based on the configured per instance concurrency and incoming events
			- Support pre-provisioned (always ready) instances
			- Max scale out: 1000 (L)
			- No containers support
			- Default functionTimeout: 30m
			- No support for windows
		* PREMIUM PLAN
			- Event driven using PREWARMED workers (no delay after being idle)
			- This plan can use VNET integration for webapps
			- More powerful instances (CPU, memory, maximun execution time)
			- For apps that runs continously
			- For many function apps on the same plan
			- Max scale out: 100 (w), 20-100 (L)
			- Linux containers support
			- Default functionTimeout: 30m
		* DEDICATED PLAN
			- Within app service plan
			- Manual/autoscale
			- For long-runing scenarios where durable functions can't be used
			- To run multiple webapps and functions on the same plan
			- Larger compute size
			- App service environment (ASE): secure network isolation, haigh memory usage and high scale
			- Max scale out: 40-60
			- Linux containers support
			- Default functionTimeout: 30m
		* CONTAINER APPS (ACA)
			- Event driven autoscale
			- Fully managed by azure container apps
			- Linux containers support
			- To package custom libraires
			- To migrate code from on-premises or legacy to cloud microservices (containers)
			- Max scale out: 300
			- Default functionTimeout: 30m

	* FUNCTION APP BASICS
		- Unit of deplyment and management for your functions
		- Functions within a function app are deployed, managed, scaled together and share the same pricing plan, runtime and deplyment method
		- You can develop and test functions locally
		- Use vscode to built azure functions app template (extension)

	* FUNCTION APP CONFIGURATION
		- host.json: Affect all functions (metadata, logging)
		- local.settings.json (like a local appsettings.json)
		- Application settings are in azure functions platform (settings deployed to azure)
		- When publish the project, be sure to add required ettings to ap settings in azure
		- Don't store secrets in loca.settings.json (don't push to code repository)
		- You can download app settings from azure function app to local project

	* TRIGGERS / BINDINGS:
		* TRIGGER:
			- Defines how a function is invoked. Azure function ----has only one----> trigger
			- Trigger have associated data (function payload)
			- Trigger types:
				QueueTrigger, HttpTrigger, BlobTrigger, TimerTrigger, EventHubTrigger, ServiceBusQueueTrigger, ServiceBusTopicTrigger, 
				EventGridTrigger, CosmosDBTrigger, DaprPublishOutputBinding, DaprServiceInvocationTrigger, DaprTopicTrigger

		* BINDINGS:
			- The way an azure functions connects to another resources
			- Can be OUTPUT BINDING or INPUT BINDING or both. You can mix different bindings
			- Data from bindings is provided as function parameters
			- Binding types:
				Blob storage, Azure Cosmos DB, Azure Data Explorer, Azure SQL, Dapr4, Event Grid, Event Hubs, HTTP & webhooks, IoT Hub, Kafka3, 
				Mobile Apps (only v1.x), Notification Hubs (only v1.x), Queue storage, Redis, RabbitMQ3, SendGrid, Service Bus, SignalR, Table storage, Timer, Twilio

		- Use case 1: Function + blob storage (INPUT and OUTPUT)
			I need each time a txt file is added to specific path of blob storage, a function read the content, process and store in another specific path of blob storage:
			- Use BlobTrigger because you need trigger actions based on blob updates
			- Use [BlobOutput], [BlobTrigger], [BlobInput] attributes and name wildcard
			- Use Connection parameter to set the app setting name instad of the actual conn string (best practice, app settings are stored encypted)

	* CONNECTION TO AZURE SERVICES
		- Azure functions can securely connect to many azure services using MANAGED IDENTITIES (system-assigned / user-assigned)
		- Enable managed identity and grant azure RBAC (permissions to access the target service)
		- Configure the target service to accept authentication from the function's managed identity
		- The function can access services via SDK using the managed identity
		- The managed identity is used to execute your OWN operations over the connected service.
		- AzureWebJobsStorage (Connection parameter) is still required for the function runtime but is unrelated to your blob access (it uses internal function app setting).

	* DURABLE FUNCTIONS:
		- Enable to have sequence of steps (functions). This steps use ActivityTrigger and the orchestrator uses OrchestrationTrigger and launch all the ActivityTrigger functions.
		- The OrchestrationTrigger can be called in another function of any trigger type.
		- Example:
			HttpTrigger -----calls in function-----> OrchestrationTrigger -----calls in function-----> ActivityTrigger1 ---output---> ActivityTrigger2 ---output---> output in orchestrator

		* DESIGN PATTERNS:
			- Function chaining pattern: (READYâ€“)
				A[Orchestration Function Starts] --> B[Function A]
			    B --> C[Function B]
			    C --> D[Function C]
			    D --> E[Orchestration Function Completes]

			- Fan in / Fan out pattern:
				F1 calls three functions in parallel (F2), It will wait for all three functions to finish before move to F3
				A[Orchestration Function Starts] --> B[Function Fan-Out]
			    B --> C1[Function C1]
			    B --> C2[Function C2]
			    B --> C3[Function C3]
			    C1 --> D[Function Fan-In]
			    C2 --> D[Function Fan-In]
			    C3 --> D[Function Fan-In]
			    D --> E[Orchestration Function Completes]

			- Asynchronous API pattern:
				F1 start long task and give register of execution as output. 
				Another function GetStatus can call the output and monitoring execution.
				Another function DoWork can call the output and monitoring execution finish to execute the work

			- Monitor pattern:
				A function is waiting something to happen in another function, and execute when "that something" has happened

			- Human interaction pattern:
				A function request approval to call other functions, the function have a return a timeout if human approval is not received

		* ENABLE DURABLE FUNCTIONS SUPPORT:
			- For Javascript runtime, create diretly from azure portal and download the npm durable-functions package, then restart the funcion app
			- For .NET runtime:
				- Create the project (durable function orchestrator) from vscode and use azurite extension for test local storage (Azurite: Start in command palette)
				- For run in mac:
					- download the nuget package: Contrib.Grpc.Core.M1
					- add this before last project tag:
						<Target Name="CopyGrpcNativeAssetsToOutDir" AfterTargets="Build"> <ItemGroup> <NativeAssetToCopy Condition="$([MSBuild]::IsOSPlatform('OSX'))" Include="$(OutDir)runtimes/osx-arm64/native/*" /> </ItemGroup> <Copy SourceFiles="@(NativeAssetToCopy)" DestinationFolder="$(OutDir).azurefunctions/runtimes/osx-arm64/native" /> </Target>
	
	* COMMANDS:
		- You can use Azure Functions Core Tools
		- func azure functionapp publish ------> deploy function app
		- func azurecontainerapps deploy ------> deploy a containerized function app


* AZURE STORAGE
	- An Azure Storage account is the top-level container for all of your Azure Blob storage
	- Provides a unique namespace for your azure storage
	- Default maximum storage account capacity: 5 PiB 

	* STORAGE ACCOUNT TYPES:
		* Standard general-purpose v2 (GPV2)
			- Most scenarios, supports all the storage services (including data lake)
			- Redundancy: LRZ, GRS, RA-GRS, ZRS, GZRS, RA-GZRS
		* Premium
			- For low-latency and frequent read/write operations
			- Redundancy: LRS, ZRS
			- Premium block blobs (blob storage including data lake)
			- Premium file shares (azure files)
			- Premium page blobs (page blobs only)

	* ACCESS TIERS
		- HOT: Online, For frequently accessed objects (highest storage cost, lowest access cost). Default tier
		- COOL: Online, For Infrequently accessed objects. Stored for a minimum of 30 days (lower storage cost, higher access cost than HOT)
		- COLD: Online, For Infrequently accessed objects. Stored for a minimum of 90 days (lower storage cost, higher access cost than COOL)
		- ARCHIVE: Offline, For data that can tolerate several hours of retrieval latency. Stored for a minimum of 180 days (lowest storage cost, highest access cost than COOL) (can't e read or modified)
		You can change the tier based on patters of your data

	* AZURE BLOB STORAGE
		- Objects storage solution for cloud (stores massiev ammount of unstructured data)
		- Use cases: Serving images, docs, files, streaming audio and video, write log files, for backup and restore, disaster recovery, archiving
		- Access blob storage via http/https from anywhere
		- Accessible via: Azrure storage rest api, azure powershell, azure CLI or SDK
		- Inmutable blob storage: Ensures data cannot be modified or deleted for specific time
		- Azure StorSimple: Hybrid, keeps frequently accessed data on-premises while storing large volumes in azure
		* RESOURCES:
			- Storage account: Unique namespace (URL: http://STORAGE_ACCOUNT_NAME.blob.core.windows.net)
			- Container:
				- Organize a set of blobs, like a directory
				- Storage account can contain unlimited containes, containers can store unlimited blobs
				- naming: lowercase, letters, numbers (no consecutive), dash, start with letter or number, 3 <= chars <= 63,
				- URL: http://STORAGE_ACCOUNT_NAME.blob.core.windows.net/CONTAINER_NAME
			- Blob:
				* Block blobs: For text and binary data. Store data up to 190.7-TiB size (individual blocks)
				* Append blobs: Optimized for apppend operations (logging use case)
				* Page blobs: Store random access files up to 8 TB in size. Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.
				- URL: http://STORAGE_ACCOUNT_NAME.blob.core.windows.net/CONTAINER_NAME/BLOB_NAME

	* ENCRYPTION:
		- All blobs, disks, files, queues, tables and metadata are encrypted for free (any region, tier, type)
		- Use SSE (service-side encryption) to store the data (256 AES)
		- Provide client-side encryption for customers who need it (in the SDKs using VERSION 2 GCM and VERSION 1 CBC)
		* MICROSOFT-MANAGED KEYS: By default
		* CUSTOMER-MANAGED KEYS: Azure key vault stores the keys
		* CUSTOMER-PROVIDED KEYS: Customer's own key store

	* DATA LIFECYCLE:
		* POLICIES (LIFECYCLE MANAGEMENT):
			- Rule-based policies that are used to change to the appropriate access tiers or to expire data
			- Policy = collection of rules (up to 100 rules)
			- Rule filters: limit rule to a subser (blobTypes, prefixMatch, blobIndexMatch)
			- Rule actions: tiering and deletion of blobs and snapshots (tierToCool, tierToCold, tierToArchive, delete, enableAutoTierToHotFromCool) (azure apply the least expensive action)
			- Rule time: Based on age, midified time. for blobs, creation time for snapshots (daysAfterModificationGreaterThan, daysAfterCreationGreaterThan, daysAfterLastAccessTimeGreaterThan, daysAfterLastTierChangeGreaterThan)
		* APPLY POLICIES:
			- az storage account management-policy create --account-name <storage-account> --policy @policy.json --resource-group <resource-group>
			- Using the code view of: Data management -> Lifecycle Management
		* REHYDRATATE:
			* Copy archived blob to an online tier (recommended): Using COPY BLOB operation (copy to other storage account but the same region)
			* Change blob's access tier to online tier: Using SET BLOB TIER operation (it can't be canceled. This doesn't affect the modified time)
			- Archived blobs need to be rehydratated (can take several hours)
			- Priority:
				Using header "x-ms-rehydrate-priority"in COPY and SET operations
				- Standard priority: Default, in order (up to 15h)
				- High priority: prioritized (under 1h for objects < 10GB)
	
	* CONSUME:
		* SDK CLIENTS:
			- BlobClient (blob level operations): interact with blob resource (reference and create blob, download blob)
			- BlobContainerClient (container level operations): create, delete, configure container, list, upload and delete blobs within the container
			- BlobServiceClient (storage account level operations): list, create, delete containers in storage account, retrieve and configure account properties
		- You can use the GetBlobContainerClient or GetBlobClient methods to create the specific level cliente from the BlobServiceClient
		- You can create a BlobContainerClient using the BlobServiceClient created
		- Pass the URI referencing the endpoint and use DefaultAzureCredential to use Managed identities

		* METADATA:
			* Up to 1,024 characters long
			* System properties: Some can be read or set, others are read-only
			* User-defined metadata: name-value pairs that user specify for blobs
			
			* SDK
				- Container properties: GetProperties and GetPropertiesAsync of BlobContainerClient
				- Metadata: 
					- Set: SetMetadata and SetMetadataAsync of BlobContainerClient (data is added as IDictionary)
					- Get: Use GetProperties and GetPropertiesAsync of BlobContainerClient, and inspect to get Metadata (properties.Value.Metadata)
				- You can also use REST

			* STANDARD PROPERTIES
				- represented as headers when using rest
				- containers: ETag, Last-Modified
				- blobs: ETag, Last-Modified, Content-Length, Content-Type, Content-MD5, Content-Encoding, Content-Language, Cache-Control, Origin, Range

			- StartCopyFromUriAsync and SetMetadataAsync as important methods.
			- AzCopy:
				- To copy files between container, even between different subscriptions (or different cloud providers)
				- To transfer large ammount of data (over 50TB) as quickly as possible.

	* ADDITIONAL:
		* Share access to storage account resource
			- Access Keys (if chosen storage account keys on creation):
				- Azure provide a pair of keys, they can be rotated manually (invalid old keys) or using a timmer (set rotation reminder)
				- YOU DONT WANT TO GIVE AWAY YOUR KEYS (use shared access signature instead)
			- Shared access signature (SAS)
				- To provide access (temporary access) to clients who should not be trusted.
				- You CANNOT revoke this access directly, unless rotate the signing key (based on access key). This invalids all the SAS generated using the rotated key.
				- There is another way to revoke the SAS acces, using stored access policy.
		* Recovery:
			- Operational backup: Using azure backup vault and a backup policy (retention days)
			- Point in time restore
			- Soft delete for blobs and containers (retention days)



* AZURE COSMOS DB
	- Fully managed NoSQL and relational database service for scalable, high performance applications.
	* KEYS:
		- Unlimited elastic write and read sclability
		- 99.999% read and write availability all around the world
		- Guarantedd read and writes serverd in < 10ms (near real time reads and writes)
		- The maximum size limit of an item in a cosmos DB container = 2MB (if you need to store larger objects, use blob storage)
	* APIs:
		- COSMOS DB FOR NOSQL: Core Native api for working with documents (JSON), supports familiar SQL language. Choose this when start from scratch (there is no data). ------> NON RELATIONAL
		- COSMOS DB FOR POSTGRESQL: OpenSource postgresql. Choose this if you need tables, foreign keys, primary keys, etc ------> RELATIONAL
		- COSMOS DB FOR MONGODB: MongoDb datbase for working with documents (BSON). Choose this if you have existing mongoDb data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR APACHE CASSANDRA: Cassandra database, mongodb and cassandra APIs are compatible (you can migrate without changes). Choose this if you have existing cassandra data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR TABLE: Compatible with azure table storage (you can migrate without changes). Choose this if you have existing azure table storage data ------> NON RELATIONAL
		- COSMOS DB FOR GREMLIN: Graph database service using gremlin query language (nodes, edges). Choose if you need relationships between data (hierarchical) ------> NON RELATIONAL
	* HIERARCHY:
		- Database account > database > container > item, sps, fucntions, triggers, conflicts
	* TYPES:
		- database: Depends on api (NOSQL, POSTGRESQL, MONGODB, CASSANDRA, TABLE, GREMLIN)
		- container and items:
			- NOSQL: container > item
			- POSTGRESQL: table > row
			- MONGODB: collection > document
			- CASSANDRA: table > row
			- TABLE: table > item
			- GREMLIN: graph > node/edge
	* DATA CONSISTENCY:
		- Define how data is read and written across multiple replicas in a globally distributed environment
		- Spectrum of well-defined choices (granular tradeoffs - high availability and performance)
		- Consistency levels are region-agnostic and are guaranteed for all operations
		- Cosmos uses "session" consistency as default
		- The consistency level configured on the account applies to all databases, containers under the account
		- 100% of read requests meet the consistency guarantee for the consistency level chosen

		- STRONG >>>>>> BOUNDED STALENESS >>>>>> SESSION >>>>>> CONSISTENT PREFIX >>>>>> EVENTUAL
		-------------- Higher availability, lower latency, higher throughput-------------------->

		* STRONG:
			- Reads are guaranteed to return the most recent version of an item (latest commited write)
			- Client never sees an uncommited or partial write
			- Use case: Financial transactions, critical real time apps, etc

		* BOUNDED STALENESS:
			- Lag of data between any two regions is always less than a specified amount:
				- Number of versions (K) of the item (e.g 100 updates)
				- Time window (T) reads may lag behind writes (e.g 5 seconds)
			- Beneficial to single-region write with more regions
			- Clients see updates in ORDER with a slight delay
			- Use case: Leaderboards, collaborative apps where order matters without critical real-time updates

		* SESSION:
			- A client always sees its own writes inmediately regardless of the region (read-your-own-writes) (No global order)
			- Other client might see slightly stale data
			- Use case: User profiles. shopping cart, etc

		* CONSISTENT PREFIX:
			- Client never sees updates out of order, but stale data is possible
			- Updates made as a batch within a transaction are returned to the transaction in which they were commited
			- Write operation within a transaction of multiple items are always visible together
			- If writes are "DocA v1 -> DocA v2 & DocB v1 -> DocB v2", the user can see either "DocA v2 & DocB v2" or "DocA v1 & DocB v1" but never "DocA v2 & DocB v1" for the same read
			- If writes are A -> B -> C, a read may return A -> B but never A -> C without B
			- Use case: Chat apps, notifications, apps where order matters but real-time freshness is not critical

		* EVENTUAL:
			- Db will eventually converge, but not guarantees the order
			- Client could see stale or out of order data
			- Use case: Social app feed, recommendations, count of likes, apps where the order really doesn't matter


	* CONSIDERATIONS:
		- Cost of all operations (point read, write, query) is normalized by request units (RUs)
		- RU represents the CPU, IOPS and memory required to perform the operation
			- e.g: Fetch a 1KB item by its ID and partiion key = 1RU
		- Capacity mode:
			* Provisioned throughput:
				- Fix an ammount of provisioned throughput to consume (RUs/s) regardless of consumption. Use free tier for this if possible
				- The throughput can be at container and database granularity level
				- If the traffic exceeds the RU/s limit, requests get throttled
			* Serverless: Pay as you use (RUs consumed by operations)
			* Autoscale mode: 
				- Automatically and instantly scale throughput (RUs/s) based on usage
				- For mission-critical workloads with variable traffic patterns
		- Limit throughput: Protects your account from cost overruns
		- Global distribution:
			- geo-redundancy: I created this in west and it will also create a version in east
			- multi-region writes
		- Backup Policy (NOT EXAM):
			- Periodic backup (interval, retention, copies, redundancy)
			- Continuos

	* GLOBAL REPLICATION:
		- You can add more regions (replications) as a READ REGIONS (you can program the application to read from the closest region to the client)
		- Muti region writes:
			- Enable WRITE and READ in all selected regions, this will double the cost
		- If the cosmos db has only a WRITE region (primary region), you can perform read and write operations in that region.

	* SDK:
		- Base client: CosmosClient, providing cosmos endpoint and credential (keys or DefaultAzureCredential for managed identities)
		- You can obtain a Database client from CosmosClient so you can perform operations on containers
		- You can obtain a Container from the datbase, so you can perform operations on items (CRUD and Query)
		- Methods:
			- container.CreateItemAsync() -----> crete new item in a container
			- container.CreateTransactionalBatch() -----> multiple create and update actions within the same partition

	* UTILITIES:
		- Sp, trigger, user-defined functions:
			- sp: for large data executions. You can execute sp from sdk. All sps have a context object, that provides access to all operations and req and res
			- triggers: to validate or add something before (pre trigger) and after (post trigger) creation/deletion/update
			- user-defined functions: function that you can use inside a query. To calculate fields.

		- Keys:
			- You can access to cosmos using the URI (or endpoint, exposed on internet or azure virtual network) and a primary or secondary key or without credentials using managed identities
			- You have primary/seconday keys for Read-write regions, and read-only regions
		- Data explorer:
			- Create new container:
				- Create new db or use existing
				- Autoscale or manual
				- Set the max RU/s
				- Partition key: Distribute data across partitions for scalability
				- Unique keys: Specific unique keys in a partition
				- id key is required in each item and is a default unique key
			- Connect
				- When new Item is created, cosmos add 5 properties to object:
					- rid (row id), _self (self reference)
		- Endpoint discovery enables applications to automatically route read requests to the closest Cosmos DB region, reducing latency.
		
		* INDEXING:
			- Cosmos DB automatically indexes all properties by default
		* PARTITION KEYS:
			- When you design a cosmosDB container, you must choose a partition key
			- This determines how data is distributed across logical partitions
			- A partition key with high cardinality (many unique values) ensures even distribution of data and workload
			- Choose as partiton key: Many unique values
			- If all the writes in a workload on a container target the same partition key, the partition gets all requests , while others reamin idle, RU/s consumed only in that partition (performance bottleneck)
			- The above issue is called  "hot partition"

	* CHANGE FEED NOTIFICATIONS:
		- Change feed in Azure Cosmos DB is a persistent record of changes to a container in the order they occur
		- This doesn't recognize item deletion or filter between insert and update operations. (Use soft delete + TTL to overcome deletion)
		- READ:
			- Push model: change feed push work to client, client has bussines loginc for processing work. Delegate hard work to cosmos (RECOMMENDED)
				- Azure functions:
					- Use azure functions (azure cosmos trigger) to execute some task when an item is created or updated.
				- Change feed processor library:
					- The monitored container: Container that has the data from with the change feed is generated
					- The lease container: state storage and coordinate processing the change feed across multiple workers
					- The compute instance: Instance that is listening for changes
					- The delegate: What the devs want to do with each bearch of changes

			- Pull model: client pull work from server, client has bussines logic for processing work, store state and handles load balancing. Extra low level control (NOT RECOMMENDED)


* MICROSOFT IDENTITY PLATFORM (ENTRA ID)

	* Components:
		- OAuth 2.0 and OpenID Connect standard-compliant authentication service
		- Open-source libraries
		- Microsoft identity platform endpoint
		- Application management portal
		- Application configuration API and powershell

	* Register app in the Azure portal:
		- When register an app with microsoft entra ID:
			- You're creating an IDENTITY CONFIGURATION.
			- Your must choose whether it is SINGLE TENANT or MULTI TENANT
			- An APPLICATION OBJECT (Globally unique app instance) is created in the home tenant.
			- A SERVICE PRINCIPAL OBJECT is created in the home tenant
			- You have a globally UNIQUE ID for the app (app or clientID)
			- You can add secrets, certificates, scopes and so on
	* IDENTITY CONFIGURATION:
		- Allows to integrate with microsoft entra ID
	* TENANT:
		- An instance of Azure Microsoft Entra ID that an organization or individual uses to manage users, apps, etc. This is true for both users (user principal) and applications (service principal)
	* APPLICATION OBJECT:
		- Application ------------ one to one ------------ Application object
		- Application ------------ one to many ------------ Service principal
	* SCHEMA:
		- https://app.diagrams.net/#G1l1P8atQcV2vNu8CWlNm4b7_yU5Etr9_D#%7B%22pageId%22%3A%22MnA-WK4r-ufKpTyJempP%22%7D
		- Security principal: 
			- To access resources secured by a Microsoft Entra tenant the entity that is requesting access must be represented by a security principal
			- This applies to users (user principal) and applications (service principal)
			- This defines the access policy and permissions for the user/app in the tenant
			- Application object: Global representation of the app for all the tenants
			- Service principal: Local representation for use in a specific tenant
			- TYPES OF SERVICE PRINCIPAL:
				* APPLICATION: Representation of a global application (app registration) in a tenant, defines who can access the app, and what resources the app can access
				* MANAGED IDENTITY: Represents a managed identity, an Identiy is provided for apps (app registration) to use when connecting to resources.
				* LEGACY: For apps created before app registrations were introduced
	* INTEGRATION:
		- You need AzureAd in appsettings.json and use:
			Install Microsoft.Identity.Web
		
			- Cookie based auth (stateless or session):
				services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApp(Configuration.GetSection("AzureAd"));

	        - JWT based auth (stateless):
	        	services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApi(Configuration.GetSection("AzureAd"));

		- AzureAd section:
			- Instance: From directory
			- Domain: From directory
			- ClientId: From application (application Id)
			- TenantId: From directory could be:
				- "common": Auth request an organization account, allow users (and personal account) from any organization, multitenant
				- "organizations": Intended only for organizational users, not Microsoft personal accounts
				- "specific-tenant": Specific tenantID of the directory}
	
	* PERMISISONS AND CONSENTS:
		- OAUTH2.0: 
			- Method through which a third-party app can access web-hosted resources on behalf of a user
			- permissions = scopes (string value) at application level (how an application can access to other application on behalf of a user or as application itself)
			- scopes control the actions over the app, like api.read, user.write, provides granularity
			- If you want to use user-permissions you can do it by creating custom roles (represent a set of permissions) (premium) in "App roles" and then assigning roles to user or groups in the tenant
			- An app requests permissions it needs by specifying the permission in "scope" query parameter
			- Permission is indicated appending the permission to resource's identitifer or app ID: https://graph.microsoft.com/Calendar.Read
			- Some high-privilege permission can be granted only through admin consent
			- If the resource identifier is omitter in scope, ther default resource is Microsoft Graph (scope=User.Read ==> https://graph.microsoft.com/User.Read)
		- PERMISSION TYPES:
			- DELEGATED ACCESS:
				- App has a signed-in user. The app acts as a signed-in users when it makes calls to target resource. (Microsoft entra id)
			- APP ONLY ACCESS:
				- App doesn't have a signed-in user. Only administrator can consent permissions for this app
		- CONSENTS:
			- STATIC USER CONSENT:
				- Azure portal defined permissions (app configuration)
				- You must specify all the permissions it needs
				- If the user hasn't granted consent for that app, microsoft identity platform prompts the user to allow them to give consent at this time
				- The app needs to request all the permission in the first sign-in (could be a long list of permissions)
				- The app needs to know all of the resources it would ever access before using them
			- INCREMENTAL AND DYNAMIC USER CONSENT:
				- Ignore static and request permissions incrementally
				- To do so, specify the scopes your app need at any time by including new scopes in "scope" parameter when requesting access token (without the need to predefine them in the app registration)
				- If the user hasn't yet consented the new scopes, the're prompted to consent only the new permissions
				- The user has to consent the new permissions (ONLY APPLIES TO DELEGATED PERMISSIONS!!!!!!)
			- ADMIN CONSENT:
				- Is required when your app needs access to certain high-privilege permissions
		- CONDITIONAL ACCESS:
			- Offers one of several wayt to secure an app:
				- Multifactor auth
				- Only Intune enrolled devices to access specific services
				- Restrictirng user locations and ip ranges
			- Sometimes it impacts an application, requiring code changes
			- Require code changes fot conditional access:
				- Apps performing On-behalf-of flow
				- Apps accessing multiple services/resources
				- SPAs using MSAL

	* FLOW - MICROSOFT IDENTITY PLATFORM:
		- Scenarios: https://learn.microsoft.com/en-us/aspnet/core/security/authentication/azure-active-directory/?view=aspnetcore-9.0
		* For web app (traditional server based app like Blazor or MVC) - (AUTHORIZATION CODE flow):
			- Create an app registration (represents the app)
			- Configures the OpenID connect options, so the authotity is the Micrososft identity platform
				- install Microsoft.Identity.Web and  Microsoft.Identity.Web.UI
 				- Add  
 					services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)
 						.AddMicrosoftIdentityWebApp(Configuration.GetSection("AzureAd"));
	        - Add redirect url as web or webapp to app registration configuration
	        - The url must match with the CallbackPath of AzuerAd configuration of appsettings.json
	        - The client secret must be aded in the azuread seciton because this is a server based app using authorization code
	        - The app needs to request the permissions as scopes

	    * For web app (traditional server based app like Blazor or MVC) - (AUTHORIZATION CODE HYBRID flow):
	    	- Create an app registration (represents the app)
			- Enable Id Token to hybrid apps in authentication section
			- Configures the OpenID connect options, so the authotity is the Micrososft identity platform
				- install Microsoft.Identity.Web and  Microsoft.Identity.Web.UI
 				- Add  
 					services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)
 						.AddMicrosoftIdentityWebApp(Configuration.GetSection("AzureAd"));
	        - Add redirect url as web or webapp to app registration configuration
	        - The url must match with the CallbackPath of AzuerAd configuration of appsettings.json
	        - The implicit flow must be allowed (in this use case)
	        - The app needs to request the permissions as scopes

	    * For client app (React, angular, vure, SPAs) - (using MSAL RECOMMENDED or IMPLICIT flow):

	    * For API + Client:

	* MSAL:
		- The library enables devs to get tokens from Microsoft identity platform to authenticate and access to apis like microsoft graph, thir party apis, your own web APIs, etc.
		- No need to directly the OAuth libraries
		- Acquires tokens on behalf of a user or an app
		- Platforms:
			Android, Angular, IOS and macOS, Go, Java, JavaScript, TypeScript, .NET, Node, Python, React
		- Authentication flows:
			- Authorization code (desktop, mobile, SPA pkce, Web)
			- Client credentials (Daemon)
			- Device code (Desktop, Mobile): Enables sign-in to a device by using another devide that has a browser
			- Implicit grant (SPA, Web)
			- On-behalf-of (OBO) - (Web Api)
			- Username/password (ROPC) - (Desktop, Mobile)
			- Integrated Windows authentication (IWA) - (Desktop, Mobile)
		- Client applications:
			- Public client app (desktop, browserless APIs, mobile, browser apps)
				- They can't be trusted to safely keep apps secrets, so they can only access web aPIs on behalf of the user
			- Confidential client app (web apps, web api apps, deamon/service apps)
				- They're considered difficult to access by users or attackers, they can hold config-time secrets (client secret or certificate X509Certificate2)
		- Client:
			- Install Microsoft.Identity.Client
			- PublicClientApplicationBuilder
			- ConfidentialClientApplicationBuilder
			- Both builders have modifiers to add configuration variables (tenant, clientId, etc) or add authority directly
			- WithCertificate and WithClientSecret methods are mutually exclusive
		- Complete frontend-backend flow:
			- Steps:
				*** DELEGATED ACCESS ***
				- Register client (spa) app registration
				- Register api app registration
				- Configure client app to authenticate against azure entra id using its client id and tenant of directory
				- Configure api app to validate incomming tokens using azure ad (jwt schema and azure configuration), and protect endpoints using Authorize
				- Add new scopes in api app registration in "expose API", set owner for this app
				- Register the created api scopes in client app registration, so the client can obtain a token to request api protected endpoints
				- Add the required scopes inthe login scopes in the client app so you can get a token authorized for all the needed scopes (the login prompt ask you for authorize that permissions)
				- Validate claims in the backend if needed (roles, policies, etc)
				- The client will be able to request microsoft graph or other resources (depending of the scopes) using its token (avoid this, and delegate this responsability for the backend)
				*** APP ONLY ACCESS ***
				- If you need request microsoft graph or other services from the backend, you need to use client credentials or certificate
			- https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-configure-app-expose-web-apis
			- samples:
				-https://learn.microsoft.com/en-us/entra/identity-platform/sample-v2-code?tabs=apptype#web-api

	* SAS (Shared access signature)
		- To provide access to clients who should not be trusted
		- Signed URI that point to one or more storage resources
		- Includes a Token that indicates how the resources might be accessed by the client
		
		- Azure storage SAS Types:
			- USER DELEGATION SAS: (RECOMMENDED)
				Delegation SAS secured with microsoft entra credentials and permissions for the SAS. (BLOB STORAGE only)
			- SERVICE SAS:
				Service SAS secured with storage account key. (BLOB, QUEUE, TABLE, AZURE FILES)
			- ACCOUNT SAS:
				Account SAS secured with storage account key. Delegate access to resources in one or more of the storage services.
			- SAS = URI + Authorization params
				- sp (access rights , a: access, c: create, d: delete, l: list, w: write)
				- st, se (start access datatime, end access datetime)
				- sv (version of the storage API)
				- sr (kind of storage, blob, queue, etc)
				- sig (cryptographic signature)
			- Best practices:
				- Use HTTPS
				- User delegation SAS is the most secure
				- Expiration time to smallest useful value
				- Minimun required privileges
			- When to use:
				- Service where users read and write their own data to my storage account. (front end proxy service / SAS provider service)
				- SAS is required when you want to copy Blobs or files to other blobs/storage accounts
			- STORED ACCESS POLICIES:
				- Consists of the start time, expiry time, and permissions for the signature (SERVICE LEVEL SAS).
				- You can create service level SAS (BLOB, QUEUE, TABLE, AZURE FILES) using existing stored access policies or setting custom properties
				- Services that support (BLOB, QUEUE, TABLE, AZURE FILES)

	* MANAGED IDENTITIES:
		* Apps can use managed identities to obtain Microsoft Entra tokens without having to manage any credentials.
		* It's like giving your webapp a unique "personality" in the form of credentials that can interact with other resources
		* YOU MUST CHOOSE BETWEEN TRADITIONAL AUTHENTICATION (DELEGATED OR APP ONLY ACCESS VIA MICROSOFT ENTRA ID) and MANAGED IDENTITIES depends on the type of the application when you need to consume azure resources from a resource
		* MANAGED IDENTITY is simpler to use when the app (webapp, function, etc) needs access to azure resources WITHOUT TO STORE OR MANAGE CREDENTIALS
		* IF YOU USE MANAGED IDENTITY, YOU'LL BE USING DefaultAzureCredential AS CREDENTIALS WHEN USE THE SDK OF THE RESOURCES TO CONSUME

		* SYSTEM ASSIGNED MANAGED IDENTITY:
			- Created as part of an azure resource (webapp, vm, etc, tied to the azure resource)
			- Shared lifecycle with the Azure resource that the managed identity is created with. When the parent resource is deleted, the managed identity is deleted as well.
			- Can't be shared (it's tied to the resource)
		* USER ASSIGNED MANAGED IDENTITY:
			- Created as stand-alone azure resource
			- Independent life-cycle. Must be explicitly deleted.
			- Can be shared (the same user managed identity can be associated with more than one resource)

		* Can be used to authenticate servicees that support Microsoft Entra auth.

		* WEB APP EXAMPLE:
			- ENABLE SYSTEM ASSIGNED MANAGED IDENTITY
			- IN WEBAPP SET THE SPECIFIC PERMISSION (in this case READER PERMISSION over a key vault rsource) (option 1)
				- Or Add the ROLE ASSIGNMENT in the key vault (IAM SECTION) pointing to the webapp (option 2)
				- The key vault must be RBAC (not access policy)
			- USE DefaultAzureCredential from code (SDK) so you can retrieve keys and secrets from key vault (DefaultAzureCredential try to authenticate via multiple mechanisms, environment variable, interactive, etc)

	* CONFIGURE MICROSOFT ENTRA ID ACCESS:
		* In some service, go to IAM, and add role assignment, add role, and member:
			- User, group, service principal (for a custom app - APP REGISTRATION - for example)
			- Managed Identity (related with an azure service)
		* For app registration:
			- Get secrets and consume the service from SDK using ClientSecretCredential or specific credentials type.
		* For delegated access:
			- Get the token using authentication/authorization at app level, or use PublicClientApplicationBuilder to acquire token manually, then make a request to the desired service (similar to consume graph).



* MICROSOFT GRAPH
	
	- Microsoft graph Connectors: Incoming data direction (external data sources: Jira, Google drive, etc)
	- Microsoft graph Data Connect: Cached data serves as data source for azure development tools
	- You need auth tokens of an app registration. to request microsoft graph api

		* Request:
		- You can use MSAL to acquire access token to microsoft graph (specifying scopes)
		- format:
			- {HTTP method} https://graph.microsoft.com/{version}/{resource}?{query-parameters}
			- versions: v1.0, beta
			- resource: me, user, group, chat, etc
			- query-parameters: OData system query options (?filter=emailAddress eq 'jon@contoso.com')

	* SDK
		- Sevice library: models, request builders from graph metadata
		- Core library: features to work with graph services
		- TO REQUEST:
			- YOU MUST CHOOSE THE AUTHENTICATION PROVIDER:
				- SPAs
					- Authorization Code with PKCE - (DELEGATED)
				- Web App that calls web APIs
					- Authorization code provider - (DELEGATED)
					- Client credentials provider - (APP ONLY)
				- Web API that calls web APIs:
					- On-behalf-of provider - (DELEGATED)
					- Client credentials provider - (APP ONLY)
				- Desktop app that calls web APIs
					- Interactive provider - (DELEGATED)
					- Integrated Windows provider - (DELEGATED)
					- Username/password provider - (DELEGATED)
					- Device code provider - (DELEGATED)
				- Daemon app
					- Client credentials provider - (APP ONLY)
				- Mobile app that calls web APIs
					- Interactive provider - (DELEGATED)

					########## - YOU NEED CERTIFICATE OR CLIENT SECRET in client credentials case ##########
					########## - YOU NEED TO ENABLE MOBILE AND DESKTOP FLOWS in device code case ##########
			- YOU MUST ADD THE ENOUGH PERMISSIONS FOR THE APP REGISTRATION
			- YOU NEED TO GRANT PERMISSION AS ADMIN USER IF NEEDED
			- You can interact with all the services of microsoft graph using the selected authentication provider
			- There are some endpints that don't work using client credentials (/me because this requires a delegated flow)
		- https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp

	* BEST PRACTICES:
		- Use least privilege
		- Use the correct permission type based on scenarios
		- Consider the end user and admin experience (consents)
		- Consider multi-tenant applications
		- Handle responses effectively (pagination, Evolvable enumerations)
		- Pagination response 100 by default
		- Cache locally for specific scenarios to avoid make a lot of requests to graph


* AZURE KEY VAULT:
	- Service for securely storing and accessing secrets, keys and certificates (API keys, passwords, certificates, cryptographic keys)
	- TIERS:
		- Standard: encrypts with a software key
		- Premium: includes hardware security module(HSM)-protectedd keys
	- BENEFITS:
		- Centralized application secrets
		- Securely store secrets and keys (required proper authentication and authorization before get access to secrets using Micrososft Entra ID)
		- Monitor access and use (By enabling logging for vaults)
			- Archive logs to storage account
			- Stream logs to an event hub
			- Send logs to Azure Monitor logs
		- Simplified administration of application secrets, Key Vault can:
			- Remove the need for in-house knowledge of HSM
			- Scale up
			- Region replication (within a region and to a secondary region)
			- Standard azure administration (Azure CLI and PowerShell)
			- Automate certificate tasks lie enrollment and renewal (from public CAs)
	- Container types:
		- Vaults (software and HSM-backed keys, secrets and certificates)
		- Managed hardware security module (HSM) pools (HSM-backed keys)
	- Access requires authentication and authorization (user o application)
		- Authentication: Microsoft Entra Id
		- Authorization: Azure RBAC (recommended) or Key Vault access policy
	- Diagnostic Settings:
		- Enable logging for vaults to:
			- Archive to a storage account
			- Stream to an event hub
			- Send logs to Azure Monitor Log
	- Azure automatically rotates the cretentials for user-assigned managed identities

	- BEST PRACTICES:
		- Authentication:
			- Managed identities for Azure resources (BEST PRACTICE)
			- Service principal and certificate (NOT RECOMMENDED)
			- Service principal and secret (NOT RECOMMENDED)
		- Azure key vault use TLS (to protect data when it's traveling between Key Vault and clients) and PFS (to protect connection between customers)
		- A VAULT PER APPLICATION PER ENVIRONMENT
		- Control access to vault (sensitive data)
		- Backup
		- Logging
		- Recovery
			- Soft delete: 90 days by default as retention days (7 to 90 days allowed)
			- Purge protection: When soft deletion is enabled, this ensures the retention policy is followed

	- AUTHENTICATION TO KEY VAULT:
		- System-assigned managed identity.
		- Using Azure SDK MSAL

	- USE:
		- SDK, CLI, PORTAL
		- CLI:
			- az group create
			- az keyvault create
			- az keyvault secret show
			- az keyvault secret set
		- POWERSHELL:
			- Set-AzKeyVaultSecret (new secret)


* AZURE APP CONFIGURATION
	- Azure App Configuration has a maximum size limit for stored data (currently 10 MB per store)
	- Service to centrally manage application settings
	- Stores data as key-value pairs (limit = 10KB)
	- App configuration doesn't version key values. Use labels to version.
	- Each key-value is uniquely identified by its key plus a label
	- Scenarios:
		- Centralize management and distribution of hierarchical configuration data for different environments and geographies
		- Dynamically change application settings without the need to redeploy or restart an app
		- Control feature availability in real time
	
	- TO USE IN CODE:
		- App Configuration Provider for the specific programming language/framework
	
	- FORMAT:
		AppName:Service1:ApiEndpoint
		AppName:Service2:ApiEndpoint
		(invaid: "*", ",", "\", escape invalid using "\" at the begining)

	- LABEL (default: key-value has no label, used to differentiate key-values with the same Key):
		Key = AppName:DbEndpoint1 & Label = Test
		Key = AppName:DbEndpoint2 & Label = Staging
		Key = AppName:DbEndpoint3 & Label = Production
		("\0" = explicitly reference key-value without a label)

	- APPLICATION FEATURE:
		- Feature flag: Flag (true or false, on or off) to enable or disable a code block
		- Feature manager: Handles the lifecycle of all the feature flags. Module of application manager to have extra functionality over feature (caching feature flas, update states)
		- Filter: Rule for evaluating the state of a feature (user groups, device or browser type, geo-location, time window, etc)
			- Types:
				- Targeting filter: test feature only for specific users or groups
				- Time window filter: test feature during a specific date range
				- Custom filter
		- Feature flag repository: Allows to change feature flags without modifying end redeploying the app.

		Example:
			"FeatureManagement": {
			    "FeatureA": true, // Feature flag set to on
			    "FeatureB": false, // Feature flag set to off
			    "FeatureC": {
			        "EnabledFor": [
			            {
			                "Name": "Percentage",
			                "Parameters": {
			                    "Value": 50
			                }
			            }
			        ]
			    }
			}

	- ENCRYPTION:
		- It encrypts sensitive information at rest using AES256
		- When customer-managed key capability is enabled, app configuration uses a manged identity assigned to the app config instance to authenticate with Microsoft Entra ID.
		- Managed identity then calls Azure Key Vault and wraps the app configuration instance's encryption key.
		- The wrapped encryption key is then stored and the unwrapped encryption key is cached within app config for ONE HOUR.
		- App config refreshes the unwrapped version of the app config instance's encryption hourly.

		- Custom-managed key capability:
			- Requires Standard tier Azure App Configuration instance
			- Requires Azure Key vault with SOFT DELETE and PURGE PROTECTION enabled
			- Requires an RSA or RSA-HSM key within KEY VAULT
			- Requires managed identity in the App Configuration instance
			- Requires GET, WRAP and UNWRAP permissions in the Key Vaults access policy

	- BEST PRACTICES:
		- All microservices should share a single App Configuration store

	- SDK:
		- For use app configuration: Azure.Data.AppConfiguration
		- For use feature flags: Microsoft.FeatureManagement.AspNetCore

	- You can use private endpoints to allow clients on a virtual network to securely access data over a private link.



* AZURE CACHE FOR REDIS:
	
	- Provides in memory data store based on REDIS
	- Improves the performance and scalability of an application that uses backend data stores heavily
	- To process large volumes of app requests by keeping frequently accessed data in the server memory (read and write quickly)
	- Offers REDIS OPEN SOURCE and REDIS ENTERPRISE
	- Microsoft operates the service, hosted on Azure, and usable by any application within or outside of Azure
	- Support data persistence (premium or better). Offers RDB (Redis database backup) and AOF (append only files) options
	- Eviction policy: Automaticalyy remove old or unused data, default: Least Recently Used (LRU) = ensures that the least used items are removed first when memory is full

	* SCENARIOS:
		- DATA CACHE (load data into the cache only as needed)
		- CONTENT CACHE (quick access to static content compared to backend datastores - templates, headers, footers)
		- SESSION STORE (user session data, shooping carts, us cookies as key to query the inmemory cache, faster than relational db)
		- JOB AND MESSAGE QUEUING (for requests that take time to execute)
		- DISTRIBUTED TRANSACTIONS (Redis supports executing a batch of commands as a single transaction)

	* TIERS: (determines the size and performance)
		- BASIC: 
			- OSS Redis on a Single VM. No SLA
			- NO REPLICATION
		- STANDARD: OSS Redis on two VMs
		- PREMIUM: 
			- High-performance OSS Redis on POWERFUL VMs. Higher throughput, lower latency
			- Passive Geo replication
			- Virtual network
			- Zone redundancy
			- Data persistence
		- ENTERPRISE: 
			- High-performance caches powered by Redis enterprise. Include RedisSearch, RedisBloom, RedisTimeSeries. Higher availability
			- Geo replication
			- Zone redundancy
			- Data persistence
		- ENTERPRISE FLASH: 
			- Cost-effective large caches powered by Redis enterprise. Reduces per-GB memory cost using nonvolatile memory (cheaper than DRAM)
			- Geo replication
			- Zone redundancy
			- Data persistence
		ENTERPRISE and ENTERPRISE FLASH allows clustering to automaticlly split dataset among multiple nodes

	* CONFIGURE AN INSTANCE:
		- You can create redis cache using Azure portal, Azure CLI or AZure PowerShell
		- The name contains numbers, letters and '-' (1 - 63 char), cannot startor end with '-', consecutive '-' invalid
		- Always place the cache instance and app in the same region (or as close as possible, even outside of azure), different region = more latency, less reliability
		- Microsoft recommends to use Standard tier or higher for production systems.

	* ACCESS:
		- Redis command line tool
		- Add TTL (Time to live - expiration time) to keys if needed. Redis stores the date when a key expires
		- Expire time resolution is always 1 ms, and expiration can be set using s or ms
		- Use from client:
			- The host name is the public internet addess of the cache instance. Use access key (primary or secondary)
			- Use from .NET
				- Add package StackExchange.Redis
				- Use host address, port, access key or CONNECTION STRING from azure
				- Connection string contains "ssl=True" (encrypted communication) and "abortConnection=False" (allows a connection to be created even if the server is unavailable at that moment)
				- The ConnectionMultiplexer class is optimized to manage connections efficiently
				- Get the db instance from the redis connection (after execute ConnectionMultiplexer.Connect)
				- You can execute specific commands in db object, or defined methods
					- db.Execute("ping"); // db is IDatabase
					- await db.ExecuteAsync("client", "list");
				- You can store get and set binary values (byte[] as key or value)
				- Common operations:
					- CreateBatch
					- CreateTransaction
					- KeyDelete
					- KeyExists
					- KeyExpire
					- KeyRename
					- KeyTimeToLive
					- KeyType
				- You can store complex values (classes) and then serialize to JSON or XML and then retrieve and deserialize to class again.
				- Dispose the connection object when finish


* AZURE CDN:
	- Content Delivery Network, uses CDN POPs (point of precense, this contains edge servers)
	- CDN ----uses----> POPs ----has many----> Edge servers
	- PROS:
		- Better performance
		- Improved user experience
		- Less traffic is sent to the origin server
		- Large scaling
	- HOW IT WORKS:
		- USER1 requests content
		- DNS routes the request to the best POP location (geographically closets)
		- If the edge servers in POP don't have the content in cache, POP requests the content from the origin server
		- Origin returns the content to edge server (updated copy)
		- An edge server in POP caches the content and return the content to USER1
		- Content remains cached on edge server until TTL (time to live, DEFAULT 7 DAYS if origin didn't specify that)
		- USER2 request content
		- If DNS routes to the same POP and the content hasn't expired, edge server returns the content from cache

	 - TIME TO LIVE:
	 	- TTL gets determined by the Cache-Control HEADER in the response from the origin
	 	- Default values:
	 		- Generalized web delivery optimizations: seven days (one week)
			- Large file optimizations: one day
			- Media streaming optimizations: one year

	- CACHING AND RULES:
		- CACHING RULES: (endpoint level)
			- Global rules (affects all requests to the endpoint, overrides any http cache header)
			- Custom rules (match specific paths, file extensions, overrides global caching)

			- STANDARD RULES ENGINE:
				- Rule = match conditions + action
				- Match rules: device type, http version, request cookis, post argument, query string

		- QUERY STRING CACHING:
			- Uses query string

	- REQUIREMENTS:
		- Azure subscription
		- Azure CDN profile (collection of cdn endpoints which users can customize with cdn behavior and access)
		- Pricing gets applied at CDN profile level
	- FEATURES:
		- Dynamic site acceleration
		- CDN caching rules
		- HTTPS custom domain support
		- Azure diagnostics logs
		- File compression
		- Geo-filtering (allow or block content in specific country/region)

	- PURGE CACHED CONTENT: (Force all edge nodes to retrieve new updated assets)
		- Endpoint level (one or more)
		- File level
		- Wildcard level (path)
		- Commands:
			 - az cdn endpoint purge: Removes specific assets from the CDN cache, forcing Azure CDN to fetch the latest version from the origin the next time they are requested.
			 - az cdn endpoint load: Pre-loads (warms up) specific assets into the CDN cache from the origin. (Caching the assets before requested)

	- SDK: Microsoft.Azure.Management.Cdn (client: CdnManagementClient)



* APPLICATION INSIGHTS
	- Extenssion of AZURE MONITOR
		- To monitoring performance, inspect METRICS and LOGS
		- Example: To identify amount of time required for an app to access an azure resource (redis, cosmos, etc) and how it contributes to response time: TrackDependency
	- Provides Application Performance monitoring (APM) features
	- Proactive (how an app is performing) and Reactive (determine the couse of an incident) monitoring
	- Store logging data, collect metrics and telemetry data (telemetry is sent to application insights for analysis and presentation)
	- Data is sent to an Application Insights Log Analytics workspace
	- You can choose the retention period for raw data, from 30 to 730 days. Aggregated data is retained for 90 days, and debug snapshots are retained for 15 days

	- FEATURES:
		- Live metrics (from your application in real time without effect on host)
		- Availability (to test external endpoints)
		- Github / Azure DevOps integration
		- Usage Analysis (which features are popular with users and how users interact and use your application - user flows)
		- Smart detection (Automatic failure and anomaly detection)
		- Application Map (high level top-down view of the application architecture)
		- Distributed tracing (end-to-end flow of transactions or executions)

	- MONITORS:
		- requests / failure rates, response times (internal and dependencies), where users are, etc
		- exceptions
		- page views load performance
		- ajax calls
		- performance counters, memory, CPU, network usage
		- host diagnostics from Docker or Azure
		- trace logs
		- custom events

	- USE
		- At run time: Instrument apps already deployed
		- At development time: Add application insights to code
		- Instrument web pages: For page views, ajax, client-side telemetry
		- Mobile apps: Visual Studio app center
		- Availability tests: ping website regularly

	- Metrics:
		- TYPES:
			- LOG-BASED metrics:
				- FOR DATA ANALYSIS DIAGNOSTICS
				- Behind the scene are translated into Kusto queries from stored events
				- Application insights stores all collected events as logs (manually sent from code skd or from autoinstrumentation)
				- Can be impractical or impossible for apps that generate a large volume of telemetry (application insights implements telemetry volume reduction techniques)
				- Fewer stored events -> less accuracy of metrics
			- STANDARD metrics (Preaggregated metrics):
				- FOR REAL TIME ALERTING
				- Stored as preaggregated time series (better performance at query time)
				- Application insights stores metrics as preaggregated time series, and only with key dimensions
				- Better choice for dashboarding and real time alerting

	- Connect to webapp:
		- Download the SDK (Microsoft.Application.Insights.AspNetCore), and add Services.AddApplicationInsightsTelemetry();
		- You don't need to add application insights connection string to appsettings because it comes by default in your webapp (as an environment variable) when you enable application insights when creating the app.

	- INSTRUMENTING:
		- Is enabling an app to capture telemetry
		- two ways:
			- Automatic instrumentation (AUTOINSTRUMENTATION)
			- Manual instrumentation

		- AUTOINSTRUMENTATION:
			- Enables collection through configuration without touching the app's code (LESS CONFIGURABLE)

		- MANUAL INSTRUMENTATION:
			- Coding against application insights or OpenTelemetry API (Install sdk, and use it from code to capture dependencies not captured with autoinstrumentation)
			- Use the SDK if you require csutom events and metrics, control over telemetry flow, autoinstrumentation isn't available
			- two ways:
				- Application insigths SDKs
				- Azure Monitor OpenTelemetry Distros

		- OPEN TELEMETRY TERMS (replacing legacy application insights terms):
			- autocollectors = instrumentation libraries
			- channel = exporter
			- codeless / agent-based = autoinstrumentation
			- traces = logs
			- requests = server spans
			- dependencies = client, internal, etc
			- operation ID = trace ID
			- ID or operation parent ID = span ID

	- AVAILABILITY TESTS AND TROUBLESHOOT:
		- Application insights sends requests to webapp at regular intervals from points around the world to alert if the app isn't responding or rsponds too slowly.
		- 100 availabilty tests per application insights.
		- Availabitlity tests types:
			- Standard test (single test http/https request)
			- Custom TrackAvailability test (from code using sdk in another app, complex requests, auth flows, etc)
			- URL ping test (classic ping, deprecated)

		- Application map (troubleshoot app performance)
			- You can see the full app topology
			- Components are different from "observed" external dependencies (other services)
			- To spot performance bottlenecks or failuers hotspots across all components of your distributed app
			- The app map finds components by following HTTP dependency calls made between servers with the Application Insights SDK installed. This applies DISCOVERY OF COMPONENTS
			- You need to configure the app (sdk installed) and resources (enabling Distributed Tracing) to use the application map and identify bottlenecks



* API MANAGEMENT:
	- Core functionalities to ensure a successful aPI program.
	- FEATURES:
		- Microsoft entra integration (not for consumption and basic)
		- Private endpoint (not for consumption and basicV2)
		- Multi-region deployment (Only for premium)
		- Availability zones (Only for premium)
		- Autoscaling (Only for basic, standard and premium)

	- Components:
		- API GATEWAY:
			- Route calls to backends
			- Verify request credentials
			- Enforces usage quotas and rate limits
			- Cache, logs, metrics, etc.
			- Executes CROSS-CUTTING tasks:
				- Authentication, SSL, rate limiting, quotas, routing.
			- Without API GATEWAY involves problems.
		- MANAGEMENT PLANE: api service settings, define api schema, package api -> products, set up policies like quotas or transformations, get insights, manage users
		- DEVELOPER PORTAL: api docs, call api via console, manage api keys, create account and subscribe to get API Keys, access analytics, download API definitions
	- PRODUCTS:
		- A PRODUCT has one or more APIs, it has a title, descriptio and terms of use.
			- Types:
				- OPEN: Can be used without subscription
				- PROTECTED: It needs subscription (require ADMIN APPROVAL or AUTOAPPROVED)
	- GROUPS: (limit the visibility of products to developers)
		- Administrators, Developers, Guests
		- Allows custom groups (or external microsoft entra)
	- POLICIES
		- Collection of statements executed sequentially on the request or response.
		- Simple XML doc that describes statements in sections:
			- inbound (request)
			- backend (before request is forwarded to backend)
			- outbound (response)
			- on-error (error)
		- You can use policy expressions if you need (C# statement, like razor pages), the context variable is provided by default (http context of .net)
		- There are a large number of pre defined policies you can use at different level and scope
		- You can use <base /> to inherit policies
		- Popular policies:
			
			- choose: classic if-else
							<choose>
							    <when condition="Boolean expression | Boolean constant">
							        <!â€” one or more policy statements to be applied if the above condition is true  -->
							    </when>
						    </choose>

			- forward: forward incoming request to specific backend
							<forward-request timeout="time in seconds" follow-redirects="true | false"/>

			- limit: limit number of requests
							<limit-concurrency key="expression" max-count="number">
							        <!â€” nested policy statements -->
							</limit-concurrency>

			- log: send messages to event hub
							<log-to-eventhub logger-id="id of the logger entity" partition-id="index of the partition where messages are sent" partition-key="value used for partition assignment">
							  Expression returning a string to be logged
							</log-to-eventhub>

			- mock: mock APIs and operations
							<mock-response status-code="code" content-type="media type"/>

			- retry: retries execution of child policies
							<retry
							    condition="boolean expression or literal"
							    count="number of retry attempts"
							    interval="retry interval in seconds"
							    max-interval="maximum retry interval in seconds"
							    delta="retry interval delta in seconds"
							    first-fast-retry="boolean expression or literal">
							        <!-- One or more child policies. No restrictions -->
							</retry>

			- return: abort pipeline execution policies
							<return-response response-variable-name="existing context variable">
							  <set-header/>
							  <set-body/>
							  <set-status/>
							</return-response>

	- HOSTING:
		- Managed (All API traffic flows through Azure)
		- Self-hosted (For hybrid and multicloud or on-premise scenarios)
	
	- SECURITY:
		- Subscription keys:
			- A subscription is essentially a named container for a pair of subscription keys
			- Developers who need to consume the published APIs can get subscriptions. And they don't need approval from API publishers. API publishers can also create subscriptions directly for API consumers
			- Subscriptions give you granular control over permissions and policies
			- Subscription has two keys (rotation or regeneration. When need to change the primary key, use the secondary key in apps to avoid downtime)
			- Including this in http request (header or query parameter) (Header: Ocp-Apim-Subscription-Key)
			- Devs who need to consume published APIs can get subscriptions (subscription requst for PROTECTED products)
			- Scopes:
				- ALL APIS (all the gateway)
				- SINGLE API (specific api)
				- PRODUCT (colelction of apis of product)
		- OAUTH2
		- IP whitelist
		- Client certificates:
			- TLR security
			- Are signed
			- API Management gateway inspect the certificate within the request and check properties: Certificate Authority (CA), Thumbprint, Subject, Expiration Date
			- Validation: Check who issued the certificate, If it's issued by partner, check that it came from them
		- Basic auth


* AZURE EVENT GRID
	- Pub Sub message distribution
	- For event driven serverless architecture
	- Uses HTTP and MQTT (support IoT solutions)
	- Event driven solutions: publisher service announces its system changes (events) to subbscriber apps

	- Maximum allowed size for event: 1 MB
	- Maximum size of events array received in Event grid: 1 MB

	* CONCEPTS:
		* (event source) publisher ----> azure grid -----> subscriberA, subscriberB, ... subscriberN (consumer can be the publisher too)
		* EVENT SOURCE:
			- Where the event happens
			- An Event source is related to one or more event types
			- Azure Storage, IoT Hub, Custom app, etc.
			- Your application is the event source for custom events that you define
			- Send events to Event Grid in an array
		* EVENT:
			- Smallest amount of information that fully describes something that happened in a system.
			- Follows the CloudEvents standard in JSON format
			- Maximum allowed event size: 1 MB (If an event or array > 1 MB -----> 413 PAYLOAD TOO LARGE)
			- Events over 64KB are charged in 64-KB increments (1 event of 130 KB ----> 3 event increments)
			- CloudEvents Structure:
				{
				    "specversion" : "1.0",
				    "type" : "com.yourcompany.order.created",
				    "source" : "https://yourcompany.com/orders/",
				    "subject" : "O-28964",
				    "id" : "A234-1234-1234",
				    "time" : "2018-04-05T17:31:00Z",
				    "comexampleextension1" : "value",
				    "comexampleothervalue" : 5,
				    "datacontenttype" : "application/json",
				    "data" : {
				       "orderId" : "O-28964",
				       "URL" : "https://com.yourcompany/orders/O-28964"
				    }
				}
		* TOPIC: 
			- A collection of related events
			- 100 Custom topics per Azure subscription (When the limit is reached, you can consider a different region or consider using domains, which can support 100,000 topics.)
			- Types:
				- SYSTEM TOPICS: 
					- Built-in topics provided by Azure services
					- Provide data about the specific Azureresource
					- If you have access to the resource, you can suscribe to events
					- You don't see topics in your Azure subscription because the publisher (Azure resource) owns the topics, but you can subscribe them
				- CUSTOM TOPICS: 
					- Custom apps (your apps) and third-party event sources
					- You can see the topic in the Azure subscription if you created it or have access to it
				- PARTNER TOPICS:
					- To subscribe to events published by a PARTNER
		* PUBLISHER: 
			- Application that sends events to Event Grid
			- It can be the same app where the events originated: event source
			- Services outside of Azure can publish events through Event Grid
			- Can be a PARTNER (sends events from its system to make them available to azure customers, partners can publish and receive events from Azure Event Grid)
		* SUBSCRIBER:
			- An Azure service or other application that decides witch topics to subscribe to
			- They receive the events from subscriptions
			- They can be Azure services or custom webhooks (apps)
		* EVENT SUBSCRIPTION: 
			- 500 subscriptions allowed per topic (canâ€™t be increased)
			- Tells grid which events on a topic you want to receive
			- You can filter by event type or subject pattern
			- You can set expiration for event subscription temporarily needed
		* EVENT HANDLER:
			- They receive the events
			- Part of grid where the event is sent. Process the event
			- Event Grid follows different mechanisms to guarantee the delivery of the event (http webhooks, azure storage queue, etc)
			- Handlers:
				- Webhooks. Azure Automation runbooks and Logic Apps are supported via webhooks.
				- Azure functions.
				- Event Hubs.
				- Service Bus queues and topics.
				- Relay hybrid connections.
				- Storage queues.
				- Azure Monitor alerts (from Azure Key Vault source only).
				- Event Grid namespace topics.
		* NAMESPACE TOPICS:
			- Topics that are created within an Event Grid namespace
			- For relatively large solutions, create a namespace topic for each category of related evetns 
		* SECURITY:
			- Usign managed identity with RBAC
			- Example: if sending events to Event Hubs, the managed identity used in the event subscription should be a member of the Event Hubs Data Sender role
		* EVENT DOMAINS:
			- They are used to group multiple topics

	* SCHEMAS:
		- Event grid schema

			[
			  {
			    "topic": string,  ------> (optional)
			    "subject": string,  ------> (REQUIRED) // to give the ability to filter events to the subscriber (file extensions, paths, blob containers, etc)
			    "id": string,   ------> (REQUIRED)
			    "eventType": string, ------> (REQUIRED)  // event type filtering
			    "eventTime": string,------> (REQUIRED)
			    "data":{. ------> (optional)
			      object-unique-to-each-publisher  // defined by publisher for custom topics
			    },
			    "dataVersion": string,  ------> (optional)
			    "metadataVersion": string. ------> (optional)
			  }
			]

			- different header: "content-type":"application/json; charset=utf-8"
			- For custom topics, the publisher HAVE TO define the same top level fields as standard resource-defined events
			- Used only within Azure (simplest integration with Event Grid subscribers)

		- Cloud event schema (open specification)

			{
			    "specversion": "1.0",
			    "type": "Microsoft.Storage.BlobCreated",  
			    "source": "/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.Storage/storageAccounts/{storage-account}",
			    "id": "9aeb0fdf-c01e-0131-0922-9eb54906e209",
			    "time": "2019-11-18T15:13:39.4589254Z",
			    "subject": "blobServices/default/containers/{storage-container}/blobs/{new-file}",
			    "dataschema": "#",
			    "data": {
			        object-unique-to-each-publisher
			    }
			}

			- different header: "content-type":"application/cloudevents+json; charset=utf-8"
			- With this schema, you can more easily integrate work across platforms
			- Used for cross-platform or multi-cloud event driven architectures
		* All remaining headers are the same

	* EVENT SUBSCRIPTION:
		- Grid tries to send each event at least once for each subscription, if there's a failure, grid retries based on RETRY SCHEDULE or RETRY POLICY
		- Grid doesn't guarantee order for event delivery (subscriber may receive events out of order)
		
		- RETRY SCHEDULE:
			- When Event Grid receives an error for an event delivery attempt, it decides wheter it should:
				- retry the delivery
				- dead-letter the event
				- drop the event based on error type
			
			- If error returned is a config-related error that can't be fixed with retries, Event Grid will dead-letter the events (or drop event if dead-letter isn't configured):
				- status code 400, 413 for azure resources -> dead-letter event or drop, doesn't retry
				- status code 400, 413, 401 for webhook -> dead-letter event or drop, doesn't retry
				(413: Request entity is too large)
				(400: Bad request)
				- You SHOULD configure dead-letter if you don't want events to be dropped
			
			- Normal scenario (error different from above):
				- Grid waits 30s for response after delivering a message, if endpoint hasn't respond, is queued for retry.
				- Exponential backoff retry policy
				- If endpoint responds within 3 minutes, Event Grid attempts to remoev event from the retry queue

		- RETRY POLICY
			- An event is dropped if either of the limits is reached:
				- Maximum number of attempts (1 - 30 attempts) (default = 30)
				- Event TTL (Time to live) (1 - 1440 min or 24h) (default = 1440 min or 24h)
			- If not, retry

		- OUTPT BATCHING: (high-throughput scenarios)
			- Turned off by default
			- Settings:
				- Max events per batch: 
					- Between 1 and 5000
					- Some events may be delivered if no other events are available
					- Event Grid doesn't delay  events to create batch if feer events are available
				- Preferred batch size in KB (target ceiling):
					- batch size < preferred (more events aren't available when publishing)
					- batch size > preferred (single event is larger than prefered)

		- DELAYED DELIVERY:
			- If the first 10 events fail, grid add delays to all subsequent retries
			- Without back-off and delay, retry policy and volume capabilities can easily overwhelm a system
		- DEAD-LETTER EVENTS:
			- Requires an storage account
			- If grid can't deliver the event within certain time or after trying to deliver a number of times, it can send the undelivered event to storage account.
			- It's disable by default (You MUST specify a storage account to hold undelivered events and turn on dead-letter)
			- This process is executed if:
				- Event isn't delivered whitin TTL
				- Number of tries > limit
			- There's 5 min delay between last attempt to deliver and delivery to dead-letter location (this is intended to reduce the number of blob operations)
			- If dead-letter location is unavailable for 4 hours, the event is dropped

		- CUSTOM DELIVERY PROPS:
			- YOU CAN ADD CUSTOM DELIVERY PROPERTIES (Headers) up to 10 headers

	* AUTHENTICATION:
		- Uses RBAC
		- Built-in roles:
			- Event Grid Subscription Reader (read event subscriptions)
			- Event Grid Subscription Contributor (manage event subscription operations)
			- Event Grid Contributor (manage Event Grid resources)
			- Event Grid Data Sender (send events to topics)

	* WEBHOOK
		- Send a POST http request to your endpoint
		- Endpoint validation:
			- SYNCHRONOUS HANDSHAKE (at subscription creation, grid sends an schema to validate)
			- ASYNCHRONOUS HANDSHAKE (for third-party services)
			- MANUAL VALIDATION (using a validationUrl valid for 5 min, during this time, provisioning state is set to "AwaitingManualAction") for version 2018-05-01-preview
			- Azure handles validation automatically when you use those services:
				- Azure Logic Apps with Event Grid Connector
				- Azure Automation via webhook
				- Azure Functions with Event Grid Trigger
			- Using self-signed certificates for validation isn't supported. Use a signed certificate from a commercial certificate authority (CA) instead

	* COMMANDS:
		- Create event subscription: az eventgrid event-subscription create
		- Create event topic: az eventgrid topic create

	* FILTERING:
		* event type
		* event subject
		* Grid allows ADVANCED FILTERING using operators based on event data:
			"filter": {
			  "advancedFilters": [
			    {
			      "operatorType": "NumberGreaterThanOrEquals",
			      "key": "Data.Key1",
			      "value": 5
			    },
			    {
			      "operatorType": "StringContains",
			      "key": "Subject",
			      "values": ["container1", "container2"]
			    }
			  ]
			}




* AZURE EVENT HUBS:
	- Native data-streaming service in the cloud can stream millions of events per second with LOW LATENCY
	- From Any SOURCE ----> To Any DESTINATION
	- Compatible with Azure functions (for serverless architectures), Apache KAFKA (It enables you to run existing Kafka workloads without any code changes, without set up or configure your kafka clusters)
	- Supports AMQP, Apache Kafka, HTTPS (multiprotocol)
	- You can ingest, buffer, store, and process data stream in real time to get actionable insights
	- Event hubs uses a partitioned consumer model (it could be multiple apps processing the stream concurrently)
	- Message size limit = 1MB
	
	- AZURE SCHEMA REGISTRY: managing schemas of event streaming applications
	- AZURE STREAM ANALYTICS: 
		- Enable real-time stream processing, Analyze streaming data.
		- It provides a built-in no-code editos, but you can also use the SQL-Based stream analytics query language, so you can perform real time processing
	- Allows automatically capture data in "Azure Blob Storage" or "Azure Data Lake Storage"

	* COMPONENTS:
		- Producer application (ingest data via SDK or Kafka producer client)
		- Namespace (Contains one or more event hubs or Kafka Topics. Streaming capacity, network security, geo-disaster recovery takes place here)
		- Event Hubs/Kafka topic (Organize events into a topic, it has partitions)
		- Partition (Used to scale an event hub, lane in a freeway. If you need more streaming throughput, add more partitions. Ordering is preserved in a partition)
		- Consumer application (consume data by seeking through the event log and maintaining consumer offset. "Kafka consumer client" or "Event Hub SDKs client")
		- Consumer group (logical group of consumer instances reads data from a topic)

	* CAPTURE DATA:
		- Store:
			- Azure blob storage and Azure Data Lake Storage are used to capture data (same or different region).
			- You can specify time or size interval
			- No administrative costs to run capturing
			- Scales automatically with:
				- Throughput units -> STANDARD TIER
				- Processing units -> PREMIUM TIER
		- Each partition is an independent segment of data and is consumed independently
		- Data in partition ages off based on configurable retention period
		- Captured data ----> APACHE AVRO FORMAT -> fast, binary format with rich data structure and inline schema
		- Commit possition of the last successfully processed event within a partition: Use OFFSET and SEQUENCE NUMBER
		- Capture window:
			- You can set up a window to control capturing
			- Window: Minimum size and time config with "first wins policy" (first trigger found causes a capture operation)
			- Name convention of capture:
				- {Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
			- Retention period:
				- 7d -> STANDARD TIER (Default)
				- 90d -> PREMIUM and DEDICATED TIER

	* TRAFFIC:
		- Controlled by THROUGHPUT UNITS
		- THROUGHPUT UNIT ----> 1MB -----> 1K events/second (ingress) - 2K events/second (egress)
		- Standard Event Hubs ---> 1 - 20 TU

	* PROCESSING APPLICATION:
		- Example:
			- There is a website for residents to monitor activity of their houses
			- 100K sensors sends data from houses (security)
			- Sensors pushes data to an Event Hub with 16 partitions
			- On the consuming end, it's necessary a mechanism that can read the events, consolidate them and upload to a blob storage
			- Then this data is ready to be used in the website for residents

		- Design the consumer (distributed environment):
			- SCALE: Create multiple consumers, with each consumer taking ownership of reading from a few partitions
			- LOAD BALANCE: Increase or reduce the consumers dynamically (for example: new sensor type -> number of events increases)
			- SEAMLESS RESUME ON FAILURES: If a consumer fails, then other consumers can pick p the partitions owned by the failed consumer and continue 
				(continuation point = CHECKPOINT or OFFSET = exact point of failure or slightly before that)
			- CONSUME EVENTS: Consumer code to do something with the events (aggregate, upload to storage, etc)

		- Consumer client:
			- The event procesing application may need scaling, running multiple instances of the app and balancing the load among themselves
			- EventProcessorClient (.NET and JAVA Azure Event Hubs SDKs) and EventHubConsumerClient (Python and JavaScript SDKs)
			- Partition Ownership:
				- Ownership of partitions is evenly distributed among all the active event processor instances associated with an event hub and consumer group combination
				- Each event processor is given a unique identitifer and claims ownership of partitions.
			- When create an EVENT PROCESSOR, you specify the functions that process events and errors (CLIENT RESPONSABILITY HANDLE THE EVENT)
			- Execute as little processing as possible over received events.
			- Code with Retry logic to ensure the consumer processes every message at least once

			* CHECKPOINT:
				- Checkpointing is a process by which an event processor marks or commits the position of the last successfully processed event within a partition. 
				- Marking a checkpoint is typically done within the function that processes the events and occurs on a per-partition basis within a consumer group.
				- If an event processor disconnects from a partition, another instance can resume processing the partition at the checkpoint that was previously committed by the last processor of that partition in that consumer group

			- Thread safety (Events from different partitions can be processed concurrently and any shared state that is accessed across partitions have to be synchronized)

		- For production:
			- EventProcessorClient > EventHubConsumerClient (performance)
			- EventHubProducerClient (publish events in batches)

	* ACCESS:
		- Supports Microsoft Entra ID and SAS
		- Supports authorization using MANAGED IDENTITIES, you need to sonfigure RBAC settings
		- Roles:
			- Azure event hubs DATA OWNER
			- Azure event hubs DATA SENDER
			- Azure event hubs DATA RECEIVER
		- SDK: Azure.Messaging.EventHubs




* AZURE MESSAGE QUEUES SOLUTIONS
	* COMPARISON
		* USE AZURE SERVICE BUS QUEUES
			(enterprise messaging)
			- Solution needs to receive messages without polling the queue (built in in bus SERVICE BUS)
			- Solution needs guaranteed FIFO order
			- Solution needs automatic duplicate detection
			- App requires to process messages as parallel long-running streams (Messages associated with stream using SESSION ID property)
			- Solution requires transactional behavior and atomicity when sending or receving multiple messages from a queue
			- 64KB < messages < 256KB or 1MB depending of tier (Service Bus supports > 100MB)
			- Solution requires rol-based access model to queues, permission for sender and receivers
		* USE AZURE STORAGE QUEUES
			(simple message buffering, scalable, low cost queueing)
			- NOT GUARANTEE FIFO
			- messages < 64KB
			- If the app must store > 80GB of messages in a queue
			- If want to track progress for processing a messaeg in queue (a worker can replace another based on tracking)
			- Solution requires server side logs

	* AZURE SERVICE BUS QUEUES
		- Message broker (messaging, transfer business data)
		- Decouple apps (client and service don't have to be online at the same time)
		- Data is transferred using messages, data can be JSON, XML, Apache Avro and plain text
		- TOPIC has many SUBSCRIPTIONS
		- ADVANCED FEATURES over storage queue
		- Protocols: AMQP

		- SCENARIOS:
			- Messaging: Transfer business data
			- Decouple applications: Reliability and scalability
			- Topics and subscriptions: (1:n) for pub-sub pattern
			- Message sessions: Workflows that require message ordering

		- TIERS:
			Premium / Standard
			- High througput / Variable throughput
			- Predictable performance / Variable latency
			- Fixed pricing / Pay as you go
			- Ability to scale workload up and down / N/A
			- messages < 100 MB / messages < 256 KB

		- ADVANCED FEATURES:
			Over storage queue
			- Message sessions
			- Autoforwarding
			- Dead-letter queue
			- Scheduled delivery
			- Message deferral
			- Transactions
			- Filtering and actions
			- Autodelete on idle
			- Duplicate detection
			- Security protocols
			- Geo-disaster recovery
			- Security
		
		- QUEUES:
			- Offer FIFO
			- Only one message consumer receives and processes each message
			- POINT TO POINT PATTERN (MESSAGE PROCESSING)
			- One to one communication
			- load-leveling: enables producers and consumers to send and receive messages at different rates
			- Consumer only has to be able to handle average load instead peak load (despite the load sent by the producer)
			- To intermediate between message producers and consumers (loose coupling)
			- Loose coupling between components
		
		- TOPICS AND SUBSCRIPTIONS:
			- PUBLISH SUBSCRIBE PATTERN (MESSAGE BROADCASTING)
			- One to many communication
			- Standard and next tiers offers this
			- Each published message is made available to each subscription registered with the topic
			- Consumer receive messages from subscriptions of the topic
			- A subscription resembles a virtual queue that receives copies of the messages that are sent to the topic
			- FILTER ACTIONS: Subscriptions can filter messages to the virtual subscription queue

		- RECEIVE MODES:
			- Receive and delete:
				- Message requested, bus marks message as consumed and returns it
				- If the consumer app crashes after bus marked message as consumed, it misses the message
				- For consumer apps that can tolerate not processing a message if failuer occurs.
			- Peek lock:
				- Finds the next message to be consumed, locks it to prevent other consumers from receiving it, and then, return the message to the app.
				- After the app finishes processing the message, it requests the Service Bus to complete the second stage of the receive process. Then, the service marks the message as consumed
				- If consumer app is unable to process the message, it can request to us to ABANDON the message so the message will be unlocked and will be available to be received again.
				- Timeout associated with the lock

		- MESSAGE:
			- Messages are transmitted as binary data (they need to be serialized to JSON, XML, etc)
			- Payload + metadata (key-value pairs)
			- Sometimes metadata alone is enough to carry information that the sender wants to communicate
			- BROKER PROPERTIES control message-level functionality, help the apps route messages to particualr destinations:
				- Simple request/reply
				- Multicast request/reply
				- Multiplexing
				- Multiplexed request/reply
			- Payload:
				- Opaque binary block

		- USE:
			- SDK for .NET
			- Dispose client and sender after usage
			- Initialize the client with conn string or credentials (AZURE Identity platform)
			- Initialize sender specifying queue name or topic


	* AZURE QUEUE STORAGE
		- For storing large numbers of messages
		- Access messages from anywhere in the world via authenticated calls using http or https
		- Max queue size: 64KB
		- Queue may contain millions of messages, up to total capacity limit of storage account.
		- Queues are commonly used to create a backlog of work to process asynchronously
		- COMPONENTS:
			- Url format (queues are addressable uing url)
			- Storage account
			- Queue (contains a set of messages, name of queue in lowercase)
			- Message (up to 64KB, supports TTL)
				- Up to 64KB
				- Supports TTL (time to live), -1 indicates no expiration, default TTL = 7 days

		- USE:
			- SDK for .NET
			- You need to manage deque if using PeekMessages (read the message without deletion, default messages number = 1)
			- If you use ReceiveMessages, the message is deleted from the queue after visibilityTimeout. You need to delete manually to ensure dequeue

















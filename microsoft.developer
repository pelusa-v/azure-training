jean.leon.v@outlook.com


* IAAS vs PAAS:
	- App Service => Platform as a Service
	- Virtual Machines => Infrastructure as a Service  
Repasar imagen


* WEBAPP:
	- App Service Plan: Pricing model for apps and hosting options. B1, S1, etc. Is a resource that
	- Manual Scaling: 
		- Scale up (B1, S1, Premium, etc), Scale out (number of instances).
		- B: 3 parallel instances
		- S: 10 parallel instances
		- When upgrade the number of instances in Scale out, Azure takes care of load balancing betweeen instances (traffic coming to a one url is divided)
	- Auto Scaling:
		- Based on rules
		- Based on defined number of instances
	- Publishing apps: 
		- I can publish from VS Code.
		- I can create slots for differents environments (staging or development for example)
		- I can deploy a different version of the app to a slot
		- There is the "swap" functionallity to exchange the content of the webapp (main) and any slot. This is important when we don't want to lose the production version, if there is an error or bug in the new version swapped to main webapp, we can swap another time to return the last production version to main webapp.
		- When swap occurs, the settings of the app aren't changed (for example consume staging or production database)
	- App service logs:
		- Application logging (Filesystem and blob), level, web server logging, retention period, failed request tracing.
	- Diagnostic settings:
		- Send to log analytics workspace
	- Log stream:
		Real time logging (It has delay)
	- To create (PowerShell, cli):
		- Create Resource group (Name, Location <====> --name or -n, --location or -l)
		- Create App service plan (Name, Location, ResourceGroupName, Tier <====> --name or -n, --location or -l, --resource-group or -g, --sku)
		- Create WebApp (Name, Location, ResourceGroupName, AppServicePlan <====> --name or -n, --location or -l, --resource-group or -g, --plan)
		- PowerShell:
			Connect-AzAccount
			New-AzResourceGroup -Location 'EastUs' -Name ''
			New-AzAppServicePlan -ResourceGroupName '' -Name '' -Location 'EastUS' -Tier 'Free'
			New-AzWebApp -ResourceGroupName '' -Name ''  -Location 'EastUS' -AppServicePlan ''
		- CLI:
			az login
			az group create --name .... --location eastus
			az appservice plan create --resource-group ... --name ... --sku FREE --location eastus
			az webapp create --resource-group ... --name ... --plan ...
	- To create resources and deploy quickly:
		- CLI:
			az webapp list-runtimes
			az webapp up -n ... --runtime dotnet:8 --location eastus
	- Console:
		- Interacting with webapp folder, can run read commands
	- Advanced Tools (Kudu):
		- You can run cmd or powershell to interact with files of webapp in real time
		- You can view files and configuration of webapp (files deployed)
		- It's good for diagnostic (debug when It works in local but doesn't work in cloud environment)
	- Docker Container:
		- You can create a webapp based on an image fo azure container registry


* CONTAINER INSTANCE:
	- Create container instance: You can specify the following features:
		- Images source: quickstart, azure registry, other registry
		- Networking configuration: Ports, dns, public or private network
		- When create container from azure registry, this identify the os type of the image.


* CONTAINER REGISTRY:
	- Create new registry: You can specify the following features:
		- Pracing plan: Basic, Standart, Premium
		- Networking: Private is only available for premium pricing plan
		- Encryption: Enable is only available for premium pricing plan
	- Prepare the images:
		Use vs for mac for use "add docker support", look for another solution in vscode or similar
	- You can deploy images using vscode or console:
		az acr login --name mytestcompany
		docker tag testcontainer:latest mytestcompany.azurecr.io/testcontainer:latest
		docker push mytestcompany.azurecr.io/testcontainer:latest

		If you want to use vscode, change the azure account for docker and azure extensions
	- You need to enable "Admin user" in order to use the user to deploy the docker (registry name as admin username)
	- You can create webapp using Docker container option


* FUNCTION APP:
	- Create function app:
		- Runtimes: It supports .net, nodejs, python, java, powershell, custom handler
			- Custom Handler: This options is used when need another runtime or language (go, php, etc)
		- Functions needs an storage account, that's because underneath the function model is a file system
		- Hosting plan:
			- Consumption (Serverless): Recommended for free
			- Flex Consumption
			- Functions premium
			- App Service
			- Container App environment
		- Networking: Private or public access
			- Enable networking injection is only available in functions premium
	- Create function:
		- Select template (Http trigger, timmer trigger, etc)
		- Authorization level:
			- Function (token)
			- Anonymous (open to everyone)
			- Admin (higher level of security required)
		- Test and code:
			- Get function url with key and execute
		- Monitoring:
			- Success and failed executions
			- Logs: real time logs
		- Using function core tools:
			- func init
			- func new
			- func start
			- func azure functionapp publish "fa name"
	- Trigger types:
		- QueueTrigger
		- HttpTrigger
		- BlobTrigger
		- TimerTrigger
		- EventHubTrigger
		- ServiceBusQueueTrigger
		- ServiceBusTopicTrigger
		- EventGridTrigger
		- CosmosDBTrigger
		- DaprPublishOutputBinding
		- DaprServiceInvocationTrigger
		- DaprTopicTrigger
	- Bindings:
		(https://learn.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings?tabs=isolated-process%2Cpython-v2&pivots=programming-language-csharp)
		- Output: output method to storage the response data of function app trigger
			- Blob storage: Create an storage account, and a container, create a new container and configure in C# using the container name, wildcard name and connection string and BlobOutput attribute
		- Types:
			Blob storage
			Azure Cosmos DB
			Azure Data Explorer
			Azure SQL
			Dapr4
			Event Grid
			Event Hubs
			HTTP & webhooks
			IoT Hub
			Kafka3
			Mobile Apps (only v1.x)
			Notification Hubs (only v1.x)
			Queue storage
			Redis
			RabbitMQ3
			SendGrid
			Service Bus
			SignalR
			Table storage
			Timer
			Twilio
		- Example:
			BlobInput: Some stored file in blob storage to read the content as input
			BlobOutput: File name with name parameter like this: blob/{name}-output.txt
			BlobTrigger: New or updated blob is detected (Stream or string)

	- Durable functions:
		- Enable to have sequence of steps (functions). This steps use ActivityTrigger and the orchestrator uses OrchestrationTrigger and launch all the ActivityTrigger functions.
		  The OrchestrationTrigger can be called in another function of any trigger type.
		  Example:
		  	HttpTrigger -----calls in function-----> OrchestrationTrigger -----calls in function-----> ActivityTrigger1 ---output---> ActivityTrigger2 ---output---> output in orchestrator

		- Patterns:
			- Function chaining pattern: (READYâ€“)
				A[Orchestration Function Starts] --> B[Function A]
			    B --> C[Function B]
			    C --> D[Function C]
			    D --> E[Orchestration Function Completes]

			- Fan in / Fan out pattern:
				F1 calls three functions in parallel (F2), It will wait for all three functions to finish before move to F3
				A[Orchestration Function Starts] --> B[Function Fan-Out]
			    B --> C1[Function C1]
			    B --> C2[Function C2]
			    B --> C3[Function C3]
			    C1 --> D[Function Fan-In]
			    C2 --> D[Function Fan-In]
			    C3 --> D[Function Fan-In]
			    D --> E[Orchestration Function Completes]

			- Asynchronous API pattern:
				F1 start long task and give register of execution as output. 
				Another function GetStatus can call the output and monitoring execution.
				Another function DoWork can call the output and monitoring execution finish to execute the work

			- Monitor pattern:
				A function is waiting something to happen in another function, and execute when "that something" has happened

			- Human interaction pattern:
				A function request approval to call other functions, the function have a return a timeout if human approval is not received

		- Enable durable functions support:
			- For Javascript runtime, create diretly from azure portal and download the npm durable-functions package, then restart the funcion app
			- For .NET runtime:
				- Create the project (durable function orchestrator) from vscode and use azurite extension for test local storage (Azurite: Start in command palette)
				- For run in mac:
					- download the nuget package: Contrib.Grpc.Core.M1
					- add this before last project tag:
						<Target Name="CopyGrpcNativeAssetsToOutDir" AfterTargets="Build"> <ItemGroup> <NativeAssetToCopy Condition="$([MSBuild]::IsOSPlatform('OSX'))" Include="$(OutDir)runtimes/osx-arm64/native/*" /> </ItemGroup> <Copy SourceFiles="@(NativeAssetToCopy)" DestinationFolder="$(OutDir).azurefunctions/runtimes/osx-arm64/native" /> </Target>
				

* AZURE STORAGE ACCOUNT
	- Managed disks (usually fixed) are more expensive than blob storage (on demand).
	- Blob storage (or unmanaged storage) is the cheapest way to store files within azure.
	- Price depends on region
	- Storage account performance:
		* You pay for storage and you pay for the number of request per month.
		- GPV2 = Standard storage: 
		- Premium: It costs way more to store it, but you save a lot of money on the access. (If you need read thousands of files per second)
	- Storage account redundancy:
		- LRS: Locally
		- GRS
		- ZRS
		- GZRS
	- Blob storage tiers:
		- Premium
		- Hot
		- Cool
		- Archive: Hard to access, intended for backups and files that are rarely used.
	- Create storage account
		- performance: GPV2 or premium
		- redundancy
		- secure transfer
		- public access
		- Secutiry access: storage account keys or/and azure AD (entra)
		- data lake storage gen2: big data scenarios, change the file system
		- Networking: 
			- public, selected virtual networks, private (private endpoint)
			- microsoft network routing VS internet routing (slower)
		- Data protection:
			- soft delete
			- versioning and tracking
		- Encryption:
			- MMK
			- CMK
	- Access tier:
		- Hot: Frequently accesed, day-to-day usage scenarios
		- Cool: Infrequently accessed, backup scenarios
	- Storage browser:
		- Tool for search data in storage account easyly (containers, files, table, queues)
	- Azure storage account contains those storage services:
		- AZURE BLOB STORAGE
		- AZURE TABLE STORAGE
		- AZURE FILE STORAGE
		- AZURE QUEUE STORAGE
	- Blob storage vs File storage:
		- file storage uses a file system. (classic) hosted on azure, blob storage is highly scalable object storage solution.

	- AZURE BLOB STORAGE (Containers):
		- To access to blobs programaticlly
		- Use the SDK to manage container, blob level objects (BlobServiceClient, BlobContainerClient, BlobCLient)
		- StartCopyFromUriAsync and SetMetadataAsync as important methods.
		- AzCopy:
			- To copy files between container, even between different subscriptions (or different cloud providers)

	- Access Keys (if chosen storage account keys on creation):
		- Azure provide a pair of keys, they can be rotated manually (invalid old keys) or using a timmer (set rotation reminder)
		- YOU DONT WANT TO GIVE AWAY YOUR KEYS (use shared access signature instead)
	- Shared access signature (SAS)
		- To provide access to clients who should not be trusted.
		- You CANNOT revoke this access directly, unless rotate the signing key (based on access key). This invalids all the SAS generated using the rotated key.
		- There is another way to revoke the SAS acces, using stored access policy.

	- Data protection:
		- Recovery:
			- Operational backup: Using azure backup vault and a backup policy (retention days)
			- Point in time restore
			- Soft delete for blobs and containers (retention days)
	- Object replication
	- Lifecycle management:
		- Add rules that will move files from the hot tier to the cool or archive tier based on the last access date (you can configure to delete instead to move files)

* AZURE COSMOS DB
	- Fully managed NoSQL and relational database service for scalable, high performance applications.
	* KEYS:
		- Unlimited elastic write and read sclability
		- 99.999% read and write availability all around the world
		- Guarantedd read and writes serverd in < 10ms (near real time reads and writes)
	* APIs:
		- COSMOS DB FOR NOSQL: Core Native api for working with documents (JSON), supports familiar SQL language. Choose this when start from scratch (there is no data). ------> NON RELATIONAL
		- COSMOS DB FOR POSTGRESQL: OpenSource postgresql. Choose this if you need tables, foreign keys, primary keys, etc ------> RELATIONAL
		- COSMOS DB FOR MONGODB: MongoDb datbase for working with documents (BSON). Choose this if you have existing mongoDb data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR APACHE CASSANDRA: Cassandra database, mongodb and cassandra APIs are compatible (you can migrate without changes). Choose this if you have existing cassandra data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR TABLE: Compatible with azure table storage (you can migrate without changes). Choose this if you have existing azure table storage data ------> NON RELATIONAL
		- COSMOS DB FOR GREMLIN: Graph database service using gremlin query language (nodes, edges). Choose if you need relationships between data ------> NON RELATIONAL
	* HIERARCHY:
		- Database account > database > container > item, sps, fucntions, triggers, conflicts
	* TYPES:
		- database: Depends on api (NOSQL, POSTGRESQL, MONGODB, CASSANDRA, TABLE, GREMLIN)
		- container and items:
			- NOSQL: container > item
			- POSTGRESQL: table > row
			- MONGODB: collection > document
			- CASSANDRA: table > row
			- TABLE: table > item
			- GREMLIN: graph > node/edge
	- Create NOSQL:
		- Capacity mode:
			- Provisioned throughput: Fix an ammount of provisioned throughput to consume (RUs/s) regardless of consumption. Use free tier for this if possible
			- Serverless: Pay as you use
		- Limit throughput: Protects your account from cost overruns
		- Global distribution:
			- geo-redundancy: I created this in west and it will also create a version in east
			- multi-region writes
		- Backup Policy (NOT EXAM):
			- Periodic backup (interval, retention, copies, redundancy)
			- Continuos
	* Global replication:
		- You can add more regions (replications) as a READ REGIONS (you can program the application to read from the closest region to the client)
		- Muti region writes:
			- Enable WRITE and READ in all selected regions, this will double the cost
		- If the cosmos db has only a WRITE region (primary region), you can perform read and write operations in that region.
	- Keys:
		- You can access to cosmos using the URI (or endpoint, exposed on internet or azure virtual network) and a primary or secondary key
		- You have primary/seconday keys for Read-write regions, and read-only regions
	- Data explorer:
		- Create new container:
			- Create new db or use existing
			- Autoscale or manual
			- Set the max RU/s
			- Partition key: Distribute data across partitions for scalability
			- Unique keys: Specific unique keys in a partition
			- id key is required in each item and is a default unique key
		- Connect
			- When new Item is created, cosmos add 5 properties to object:
				- rid (row id), _self (self reference)
	* Data consistency:
		- Strong > Bounded staleness > session > consistent prefix > eventual
		- Cosmos uses "session" consistency as default
	* SDK:
		- code
	- Sp, trigger, user-defined functions:
		- sp: for large data executions. You can execute sp from sdk
		- triggers: to validate or add something before (pre trigger) and after (post trigger) creation/deletion/update
		- user-defined functions: function that you can use inside a query. To calculate fields.

	* CHANGE FEED NOTIFICATIONS:
		- Change feed in Azure Cosmos DB is a persistent record of changes to a container in the order they occur
		- This doesn't recognize item deletion or filter between insert and update operations. (Use soft delete + TTL to overcome deletion)
		- READ:
			- Push model: change feed push work to client, client has bussines loginc for processing work. Delegate hard work to cosmos (RECOMMENDED)
				- Azure functions:
					- Use azure functions (azure cosmos trigger) to execute some task when an item is created or updated.
				- Change feed processor library:
					- The monitored container: Container that has the data from with the change feed is generated
					- The lease container: state storage and coordinate processing the change feed across multiple workers
					- The compute instance: Instance that is listening for changes
					- The delegate: What the devs want to do with each bearch of changes

			- Pull model: client pull work from server, client has bussines logic for processing work, store state and handles load balancing. Extra low level control (NOT RECOMMENDED)


* MICROSOFT IDENTITY PLATFORM (ENTRA ID)

	* Components:
		- OAuth 2.0 and OpenID Connect standard-compliant authentication service
		- Open-source libraries
		- Microsoft identity platform endpoint
		- Application management portal
		- Application configuration API and powershell

	* Register app in the Azure portal:
		- When register an app with microsoft entra ID:
			- You're creating an IDENTITY CONFIGURATION.
			- Your must choose whether it is SINGLE TENANT or MULTI TENANT
			- An APPLICATION OBJECT (Globally unique app instance) is created in the home tenant.
			- A SERVICE PRINCIPAL OBJECT is created in the home tenant
			- You have a globally UNIQUE ID for the app (app or clientID)
			- You can add secrets, certificates, scopes and so on
	* IDENTITY CONFIGURATION:
		- Allows to integrate with microsoft entra ID
	* TENANT:
		- An instance of Azure Microsoft Entra ID that an organization or individual uses to manage users, apps, etc.
	* APPLICATION OBJECT:
		- Application ------------ one to one ------------ Application object
	* SCHEMA:
		- https://app.diagrams.net/#G1l1P8atQcV2vNu8CWlNm4b7_yU5Etr9_D#%7B%22pageId%22%3A%22MnA-WK4r-ufKpTyJempP%22%7D
	* INTEGRATION:
		- You need AzureAd in appsettings.json and use:
		
			- Cookie based auth (stateless or session):
				services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApp(Configuration.GetSection("AzureAd"));

	        - JWT based auth (stateless):
	        	services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApi(Configuration.GetSection("AzureAd"));

		- AzureAd section:
			- Instance: From directory
			- Domain: From directory
			- ClientId: From application (application Id)
			- TenantId: From directory could be:
				- "common": Auth request an organization account, allow users (and personal account) from any organization, multitenant
				- "organizations": Intended only for organizational users, not Microsoft personal accounts
				- "specific-tenant": Specific tenantID of the directory}
	* Permissions and consent:
		- permissions = scopes
		- Permission types:
			- DELEGATED ACCESS:
				- App has a signed-in user. The app acts as a signed-in users when it makes calls to target resource. (Microsoft entra id)
			- APP ONLY ACCESS:
				- App doesn't have a signed-in user. Only administrator can consent permissions for this app
		- Consents:
			- STATIC USER CONSENT:
				- Azure portal defined permissions (app configuration)
			- INCREMENTAL AND DYNAMIC USER CONSENT:
				- Ignore static and request permissions incrementally
				- To do so, specify the scopes your app need at any time by including new scopes in "scope" parameter when requesting access token (without the need to predefine them in the app registration)
				- The user has to consent the new permissions (ONLY APPLIES TO DELEGATED PERMISSIONS!!!!!!)
			- ADMIN CONSENT:
				- Is required when your app needs access to certain high-privilege permissions
		- Conditional access: TO PRACTICE

	* MSAL:
		The library enables devs to get tokens from Microsoft identity platform to authenticate and access to apis like microsoft graph, thir party apis, etc.
		Authentication flows:
			- Authorization code
			- Client credentials
			- Device code
			- Implicit grant
			- On-behalf-of (OBO)
			- Username/password (ROPC)
			- Integrated Windows authentication (IWA)
		Client applications:
			- Public (desktop, browserless APIs, mobile, browser apps)
			- Confidentials (web apps, web api apps, deamon/service apps)
		Complete frontend-backend flow:
			- Steps:
				*** DELEGATED ACCESS ***
				- Register client (spa) app registration
				- Register api app registration
				- Configure client app to authenticate against azure entra id using its client id and tenant of directory
				- Configure api app to validate incomming tokens using azure ad (jwt schema and azure configuration), and protect endpoints using Authorize
				- Add new scopes in api app registration in "expose API", set owner for this app
				- Register the created api scopes in client app registration, so the client can obtain a token to request api protected endpoints
				- Add the required scopes inthe login scopes in the client app so you can get a token authorized for all the needed scopes (the login prompt ask you for authorize that permissions)
				- Validate claims in the backend if needed (roles, policies, etc)
				- The client will be able to request microsoft graph or other resources (depending of the scopes) using its token (avoid this, and delegate this responsability for the backend)
				*** APP ONLY ACCESS ***
				- If you need request microsoft graph or other services from the backend, you need to use client credentials or certificate
			- https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-configure-app-expose-web-apis

	* SAS
		- To provide access to clients who should not be trusted
		- Signed URI that point to one or more storage resources
		- Includes a Token that indicates how the resources might be accessed by the client
		
		- Types:
			- USER DELEGATION SAS: (RECOMMENDED)
				Delegation SAS secured with microsoft entra credentials and permissions for the SAS. (BLOB STORAGE)
			- SERVICE SAS:
				Service SAS secured with storage account key. (BLOB, QUEUE, TABLE, AZURE FILES)
			- ACCOUNT SAS:
				Account SAS secured with storage account key. Delegate access to resources in one or more of the storage services.
			- SAS = URI + Authorization params
				- sp (access rights)
				- st, se (start access datatime, end access datetime)
				- sv (version of the storage API)
				- sr (kind of storage, blob, queue, etc)
				- sig (cryptographic signature)
			- When to use:
				- Service where users read and write their own data to my storage account. (front end proxy service / SAS provider service)
				- SAS is required when you want to copy Blobs or files to other blobs/storage accounts
			- STORED ACCESS POLICIES:
				- Consists of the start time, expiry time, and permissions for the signature (SERVICE LEVEL SAS).
				- You can create service level SAS (BLOB, QUEUE, TABLE, AZURE FILES) using existing stored access policies or setting custom properties
				- Services that support (BLOB, QUEUE, TABLE, AZURE FILES)

	* MANAGED IDENTITIES:
		* Apps can use managed identities to obtain Microsoft Entra tokens withous having to manage any credentials.
		* It's like giving your webapp a unique "personality" in the form of credentials that can interact with other resources
		* YOU MUST CHOOSE BETWEEN TRADITIONAL AUTHENTICATION (DELEGATED OR APP ONLY ACCESS VIA MICROSOFT ENTRA ID) and MANAGED IDENTITIES depends on the type of the application when you need to consume azure resources from a resource
		* MANAGED IDENTITY is simpler to use when the app (webapp, function, etc) needs access to azure resources WITHOUT TO STORE OR MANAGE CREDENTIALS
		* IF YOU USE MANAGED IDENTITY, YOU'LL BE USING DefaultAzureCredential AS CREDENTIALS WHEN USE THE SDK OF THE RESOURCES TO CONSUME

		* SYSTEM ASSIGNED MANAGED IDENTITY:
			- Created as part of an azure resource (webapp, vm, etc, tied to the azure resource)
			- Can't be shared
		* USER ASSIGNED MANAGED IDENTITY:
			- Created as stand-alone azure resource
			- Can be shared

		* Can be used to authenticate servicees that support Microsoft Entra auth.

		* WEB APP EXAMPLE:
			- ENABLE SYSTEM ASSIGNED MANAGED IDENTITY
			- IN WEBAPP SET THE SPECIFIC PERMISSION (in this case READER PERMISSION over a key vault rsource) (option 1)
				- Or Add the ROLE ASSIGNMENT in the key vault (IAM SECTION) pointing to the webapp (option 2)
				- The key vault must be RBAC (not access policy)
			- USE DefaultAzureCredential from code (SDK) so you can retrieve keys and secrets from key vault (DefaultAzureCredential try to authenticate via multiple mechanisms, environment variable, interactive, etc)

	* CONFIGURE MICROSOFT ENTRA ID ACCESS:
		* In some service, go to IAM, and add role assignment, add role, and member:
			- User, group, service principal (for a custom app - APP REGISTRATION - for example)
			- Managed Identity (related with an azure service)
		* For app registration:
			- Get secrets and consume the service from SDK using ClientSecretCredential or specific credentials type.
		* For delegated access:
			- Get the token using authentication/authorization at app level, or use PublicClientApplicationBuilder to acquire token manually, then make a request to the desired service (similar to consume graph).



* MICROSOFT GRAPH
	
	- Microsoft graph Connectors: Incoming data direction (external data sources: Jira, Google drive, etc)
	- Microsoft graph Data Connect: Cached data serves as data source for azure development tools
	- You need auth tokens of an app registration. to request microsoft graph api
	- Resources: me, teams, groups, etc
	- Query parameters: filters

	* SDK
		- Sevice library: models, request builders from graph metadata
		- Core library: features to work with graph services
		- TO REQUEST:
			- YOU MUST CHOOSE THE AUTHENTICATION PROVIDER:
				- Authorization code provider - (DELEGATED)
				- Client credentials provider - (APP ONLY)
				- On-behalf-of provider - (DELEGATED)
				- Interactive provider - (DELEGATED)
				- Integrated Windows provider - (DELEGATED)
				- Username/password provider - (DELEGATED)
				- Device code provider - (DELEGATED)
					########## - YOU NEED CERTIFICATE OR CLIENT SECRET in client credentials case ##########
					########## - YOU NEED TO ENABLE MOBILE AND DESKTOP FLOWS in device code case ##########
			- YOU MUST ADD THE ENOUGH PERMISSIONS FOR THE APP REGISTRATION
			- YOU NEED TO GRANT PERMISSION AS ADMIN USER IF NEEDED
			- You can interact with all the services of microsoft graph using the selected authentication provider
			- There are some endpints that don't work using client credentials (/me because this requires a delegated flow)
		- https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp

	* Request:
		- You can use MSAL to acquire access token to microsoft graph (specifying scopes)

	* BEST PRACTICES:
		- Use least privilege
		- Use the correct permission type based on scenarios
		- Consider the end user and admin experience (consents)
		- Consider multi-tenant applications
		- Handle responses effectively (pagination, Evolvable enumerations)
		- Cache locally for specific scenarios to avoid make a lot of requests to graph


* AZURE KEY VAULT:
	- Service for securely storing and accessing secrets (API keys, passwords, certificates, cryptographic keys)
	- Container types:
		- Vaults (software and HSM-backed keys, secrets and certificates)
		- Managed hardware security module (HSM) pools (HSM-backed keys)
	- Access requires authentication and authorization (user o application)
		- Authentication: Microsoft Entra Id
		- Authorization: Azure RBAC or Key Vault access policy
	- Diagnostic Settings:
		- Enable logging for vaults to:
			- Archive to a storage account
			- Stream to an event hub
			- Send logs to Azure Monitor Log

	- BEST PRACTICES:
		- Authentication:
			- Managed identities for Azure resources (BEST PRACTICE)
			- Service principal and certificate (NOT RECOMMENDED)
			- Service principal and secret (NOT RECOMMENDED)
		- Azure key vault use TLS (to protect data when it's traveling between Key Vault and clients) and PFS (to protect connection between customers)
		
		- A vault per application per environment
		- Control access to vault (sensitive data)
		- Backup
		- Logging
		- Recovery (soft delete)

	- AUTHENTICATION TO KEY VAULT:
		- System-assigned managed identity.
		- Using Azure SDK MSAL

	- USE:
		- SDK, CLI, PORTAL
		- CLI:
			- az group create
			- az keyvault create
			- az keyvault secret show
			- az keyvault secret set



* AZURE CACHE FOR REDIS:
	
	- Provides in memory data store absed on REDIS
	- To process large volumes of app requests bykeeping frequently accessed data in the server memory (read and write quickly)
	- Offers REDIS OPEN SOURCE and REDIS ENTERPRISE

	* SCENARIOS:
		- DATA CACHE (load data into the cache only as needed)
		- CONTENT CACHE (quick access to static content compared to backend datastores - templates, headers, footers)
		- SESSION STORE (user session data, shooping carts, us cookies as key to query the inmemory cache, faster than relational db)
		- JOB AND MESSAGE QUEUING (for requests that take time to execute)
		- DISTRIBUTED TRANSACTIONS (Redis supports executing a batch of commands as a single transaction)

	* TIERS: (determines the size and performance)
		- BASIC < STANDARD < PREMIUM < ENTERPRISE < ENTERPRISE FLASH
		- ENTERPRISE and ENTERPRISE FLASH allows clustering to automaticlly split dataset amoing multiple nodes

	* CONFIGURE AN INSTANCE:
		- The name contains numbers, letters and '-' (1 - 63 char), cannot startor end with '-', consecutive '-' invalid
		- Always place the cache instance and app in the same region (or as close as possible, even outside of azure), different region = more latency, less reliability
		- Microsoft recommends to use Standard tier or higher for production systems.

	* ACCESS:
		- Redis command line tool
		- Add TTL (Time to live - expiration time) to keys if needed. Redis stores the date when a key expires
		- Expire time resolution is always 1 ms, and expiration can be set using s or ms
		- Use from client:
			- The host name is the public internet addess of the cache instance. Use access key (primary or secondary)
			- Use from .NET
				- Add package StackExchange.Redis
				- Use host address, port, access key or CONNECTION STRING from azure
				- Connection string contains "ssl" (encrypted communication) and "abortConnection" (allows a connection to be created even if the server is unavailable at that moment)
				- The ConnectionMultiplexer class is optimized to manage connections efficiently
				- Get the db instance from the redis connection (after execute ConnectionMultiplexer.Connect)
				- You can execute specific commands in db object, or defined methods
				- You can store complex values (classes) and then serialize to JSON or XML and then retrieve and deserialize to class again.
				- Dispose the connection object when finish


* CDN:
	- Content Delivery Network, uses CDN POPs (point of precense this contains edge servers)
	- PROS:
		- Better performance
		- Improved user experience
		- Less traffic is sent to the origin server
		- Large scaling
	- HOW IT WORKS:
		- USER1 request content
		- DNS routes the request to the best POP location (geographically closets)
		- If the edge servers in POP don't have the content in cache, POP request the content from the origin server
		- Origin returns the content to edge server (updated copy)
		- An edge server in POP caches the content and return the content to USER1
		- Content remains cached on edge server until TTL (time to live, DEFAULT 7 DAYS if origin didn't specify that)
		- USER2 request content
		- If DNS routes to the same POP and the content hasn't expired, edge server returns the content from cache

	 - TIME TO LIVE:
	 	- TTL gets determined by the Cache-Control HEADER in the response from the origin
	 	- Default values:
	 		- Generalized web delivery optimizations: seven days (one week)
			- Large file optimizations: one day
			- Media streaming optimizations: one year

	- CACHING AND RULES:
		- CACHING RULES: (endpoint level)
			- Global rules (affects all requests to the endpoint, overrides any http cache header)
			- Custom rules (match specific paths, file extensions, overrides global caching)

			- STANDARD RULES ENGINE:
				- Rule = match conditions + action

		- QUERY STRING CACHING:
			- Uses query string

	- REQUIREMENTS:
		- Azure subscription
		- Azure CDN profile (collection of cdn endpoints which users can customize with cdn behavior and access)
		- Pricing gets applied at CDN profile level
	- FEATURES:
		- Dynamic site acceleration
		- CDN caching rules
		- HTTPS custom domain support
		- Azure diagnostics logs
		- File compression
		- Geo-filtering (allow or block content in specific country/region)

	- PURGE CACHED CONTENT: (Force all edege nodes to retrieve new updated assets)
		- Endpoint level (one or more)
		- File level
		- Wildcard level (path)
		- Commands:
			 - az cdn endpoint purge: where a large amount of data is invalidated and should be updated in the cache
			 - az cdn endpoint load: where app creates a large ammount of assets and you want to imprive performance before any request (caching the assets before)

	- SDK: Microsoft.Azure.Management.Cdn (client: CdnManagementClient)



* APPLICATION INSIGHTS
	- Extenssion of AZURE MONITOR
	- Provides Application Performance monitoring (APM) features
	- Proactive (how an app is performing) and Reactive (determine the couse of an incident) monitoring
	- Store logging data, collect metrics and telemetry data (telemetry is sent to application insights for analysis and presentation)
	- Monitors:
		- request / failure rates, response times (internal and dependencies), where users are, etc
		- exceptions
		- page views load performance
		- ajax calls
		- performance counters, memory, CPU, network usage
		- host diagnostics from Docker or Azure
		- trace logs
		- custom events

	- Metrics:
		- Log-based metrics:
			- FOR DATA ANALYSIS DIAGNOSTICS
			- Behind the scene are translated into Kusto queries from stored events
			- Application insights stores all collected events as logs (manually sent from code skd or from autoinstrumentation)
		- Standard metrics:
			- FOR REAL TIME ALERTING
			- Stored as preaggregated time series (better performance at query time)
			- Application insights stores metrics as preaggregated time series, and only with key dimensions

	- Connect to webapp:
		- Download the SDK (Microsoft.Application.Insights.AspNetCore), and add Services.AddApplicationInsightsTelemetry();
		- You don't need to add application insights connection string to appsettings because it comes by default in your webapp (as an environment variable) when you enable application insights when creating the app.

	- INSTRUMENTING:
		- Is enabling an app to capture telemetry
		- two ways:
			- Automatic instrumentation (AUTOINSTRUMENTATION)
			- Manual instrumentation

		- AUTOINSTRUMENTATION:
			- Enables collection through configuration without touching the app's code (LESS CONFIGURABLE)

		- MANUAL INSTRUMENTATION:
			- Coding against application insights or OpenTelemetry API (Install sdk, and use it from code to capture dependencies not captured with autoinstrumentation)
			- Use the SDK if you require csutom events and metrics, control over telemetry flow, autoinstrumentation isn't available
			- two ways:
				- Application insigths SDKs
				- Azure Monitor OpenTelemetry Distros

		- OPEN TELEMETRY TERMS (replacing legacy application insights terms):
			- autocollectors = instrumentation libraries
			- channel = exporter
			- codeless / agent-based = autoinstrumentation
			- traces = logs
			- requests = server spans
			- dependencies = client, internal, etc
			- operation ID = trace ID
			- ID or operation parent ID = span ID

	- AVAILABILITY TESTS AND TROUBLESHOOT:
		- Application insights sends requests to webapp at regular intervals from points around the world to alert if the app isn't responding or rsponds too slowly.
		- 100 availabilty tests per application insights.
		- Availabitlity tests types:
			- Standdard test (single test http/https request)
			- Custom TrackAvailability test (from code using sdk in another app, complex requests, auth flows, etc)
			- URL ping test (classic ping, deprecated)

		- Application map (troubleshoot app performance)
			- You need to configure the app (sdk installed) and resources (enabling Distributed Tracing) to use the application map and identify bottlenecks


* AZURE APP CONFIGURATION
	- Service to centrally manage application settings
	- Stores data as key-value pairs
	- App configuration doesn't version key values. Use labels to version.
	- Each key-value is uniquely identified by its key plus a label
	- Scenarios:
		- Centralize management and distribution of hierarchical configuration data for different environments and geographies
		- Dynamically change application settings without the need to redeploy or restart an app
		- Control feature availability in real time
	
	- TO USE IN CODE:
		- App Configuration Provider for the specific programming language/framework
	
	- FORMAT:
		AppName:Service1:ApiEndpoint
		AppName:Service2:ApiEndpoint
		(invaid: "*", ",", "\", escape invalid using "\" at the begining)

	- LABEL (default: key-value has no label, used to differentiate key-values with the same Key):
		Key = AppName:DbEndpoint1 & Label = Test
		Key = AppName:DbEndpoint2 & Label = Staging
		Key = AppName:DbEndpoint3 & Label = Production
		("\0" = explicitly reference key-value without a label)

	- APPLICATION FEATURE:
		- Feature flag: Flag (true or false) to enable or disable a code block
		- Feature manager: Module of application manager to have extra functionality over feature
		- Filter: Rule for evaluating the state of a feature
			- Types:
				- Targeting filter: test feature only for specific users or groups
				- Time window filter: test feature during a specific date range
				- Custom filter

	- ENCRYPTION:
		- It encrypts sensitive information at rest using AES256
		- When customer-managed key capability is enabled, app configuration uses a manged identity assigned to the app config instance to authenticate with Microsoft Entra ID.
		- Managed identity then calls Azure Key Vault and wraps the app configuration instance's encryption key.
		- The wrapped encryption key is then stored and the unwrapped encryption key is cached within app config for ONE HOUR.
		- App config refreshes the unwrapped version of the app config instance's encryption hourly.

	- You can use private endpoints to allow clients on a virtual network to securely access data over a private link.




* API MANAGEMENT:
	- Core functionalities to ensure a successful aPI program.
	- Components:
		- API GATEWAY: route calls to backends, verify request credentials, quotas and limits, cache, logs, metrics, etc.
			- Executes CROSS-CUTTING tasks:
				- Authentication, SSL, rate limiting, quotas, routing.
			- Without API GATEWAY involves problems.
		- MANAGEMENT PLANE: api service settings, define api schema, package api -> products, policies, manage users
		- DEVELOPER PORTAL: api docs, call api via console, manage api keys
	- PRODUCTS:
		- A PRODUCT has one or more APIs, it has a title, descriptiom and terms of use.
		- OPEN: Can be used without subscription
		- PROTECTED: It needs subscription (require ADMIN APPROVAL or AUTOAPPROVED)
	- GROUPS: (limit the visibility of products to developers)
		- Administrators, Developers, Guests
		- Allows custom groups (or external microsoft entra)
	- POLICIES
		- Collection of statements executed sequentially on the request or response.
		- Simple XML doc that describes statements in sections:
			- inbound (request)
			- backend (before request is forwarded to backend)
			- outbound (response)
			- on-error (error)
		- You can use policy expressions if you need (C# statement, like razor pages), the context variable is provided by default (http context of .net)
		- There are a large number of pre defined policies you can use at different level and scope
		- You can use <base /> to inherit policies
		- Popular policies:
			
			- choose: classic if-else
							<choose>
							    <when condition="Boolean expression | Boolean constant">
							        <!â€” one or more policy statements to be applied if the above condition is true  -->
							    </when>
						    </choose>

			- forward: forward incoming request to specific backend
							<forward-request timeout="time in seconds" follow-redirects="true | false"/>

			- limit: limit number of requests
							<limit-concurrency key="expression" max-count="number">
							        <!â€” nested policy statements -->
							</limit-concurrency>

			- log: send messages to event hub
							<log-to-eventhub logger-id="id of the logger entity" partition-id="index of the partition where messages are sent" partition-key="value used for partition assignment">
							  Expression returning a string to be logged
							</log-to-eventhub>

			- mock: mock APIs and operations
							<mock-response status-code="code" content-type="media type"/>

			- retry: retries execution of child policies
							<retry
							    condition="boolean expression or literal"
							    count="number of retry attempts"
							    interval="retry interval in seconds"
							    max-interval="maximum retry interval in seconds"
							    delta="retry interval delta in seconds"
							    first-fast-retry="boolean expression or literal">
							        <!-- One or more child policies. No restrictions -->
							</retry>

			- return: abort pipeline execution policies
							<return-response response-variable-name="existing context variable">
							  <set-header/>
							  <set-body/>
							  <set-status/>
							</return-response>

	- HOSTING:
		- Managed (All API traffic flows through Azure)
		- Self-hosted (For hybrid and multicloud or on-premise scenarios)
	
	- SECURITY:
		- Subscription keys:
			- Subscription has two keys (rotation)
			- Including this in http request (header or query parameter) (Header: Ocp-Apim-Subscription-Key)
			- Devs who need to consume published APIs can get subscriptions (subscription requst for PROTECTED products)
			- Scopes: ALL APIS (all the gateway), SINGLE API (specific api), PRODUCT (colelction of apis of product)
		- OAUTH2
		- IP whitelist
		- Client certificates:
			- TLR security
			- Are signed
			- API Management gateway inspect the certificate within the request and check properties: Certificate Authority (CA), Thumbprint, Subject, Expiration Date
			- Validation: Check who issued the certificate, If it's issued by partner, check that it came from them


* AZURE EVENT GRID
	- Pub Sub message distribution
	- For event driven serverless architecture
	- Uses HTTP and MQTT
	- Maximum allowed size for event: 1 MB
	- Maximum size of events array received in Event grid: 1 MB
	* (event source) publisher ----> azure grid -----> subscriberA, subscriberB, ... subscriberN (consumer can be the publisher too)
	- publisher can be a PARTNER (sends events from its system to make them available to azure customers)
	- Event source: 
		- Azure Storaege, IoT Hub, Custom app, etc.
		- Send events to Event Grid in an array (max length size: 1 MB)
		- If an event or array > 1 MB -----> 413 PAYLOAD TOO LARGE
		- Operations are charged in 64 KB increments. (1 event of 130 KB ----> 3 events)
	- Topic: A collection of related events:
		- SYSTEM TOPICS: Provide data about the resource. Access = you can suscribe to events
		- CUSTOM TOPICS: Custom apps and third-party event sources
		- PARTNER TOPICS: To subscribe to events published by a PARTNER
	- Event subscription: Tells grid which events on a topic you want to receive - Subscribers can be azure services (storage account, functions, etc) or CUSTOM WEBHOOKS
	- Event handler: Part of grid where the event is sent. Process the event

	* SCHEMAS:
		- Event grid schema

			[
			  {
			    "topic": string,  ------> (optional)
			    "subject": string,  ------> (REQUIRED) // to give the ability to filter events to the subscriber (file extensions, paths, containers, etc)
			    "id": string,   ------> (REQUIRED)
			    "eventType": string, ------> (REQUIRED)  // event type filtering
			    "eventTime": string,------> (REQUIRED)
			    "data":{. ------> (optional)
			      object-unique-to-each-publisher  // defined by publisher for custom topics
			    },
			    "dataVersion": string,  ------> (optional)
			    "metadataVersion": string. ------> (optional)
			  }
			]

			different header: "content-type":"application/json; charset=utf-8"

		- Cloud event schema (open specification)

			{
			    "specversion": "1.0",
			    "type": "Microsoft.Storage.BlobCreated",  
			    "source": "/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.Storage/storageAccounts/{storage-account}",
			    "id": "9aeb0fdf-c01e-0131-0922-9eb54906e209",
			    "time": "2019-11-18T15:13:39.4589254Z",
			    "subject": "blobServices/default/containers/{storage-container}/blobs/{new-file}",
			    "dataschema": "#",
			    "data": {
			        object-unique-to-each-publisher
			    }
			}

			different header: "content-type":"application/cloudevents+json; charset=utf-8"
		* All remaining headers are the same

	* EVENT SUBSCRIPTION:
		- Grid tries to send each event at least once for each subscription, if there's a failure, grid retries based on RETRY SCHEDULE or RETRY POLICY
		- Grid doesn't guarantee order for event delivery
		
		- RETRY SCHEDULE:
			- status code 400, 412 for azure resources -> dead-letter event, doesn't retry
			- status code 400, 412, 401 for webhook -> dead-letter event, doesn't retry
			- (if dead-letter isn't configured, events will be dropped)
			- Normal scenario:
				- Grid waits 30sfor response, if endpoint hasn't respond, is queued for retry.
				- Exponential backoff retry policy

		- RETRY POLICY
			- An event is dropped if:
				- Maximum number of attempts (1 - 30 attempts)
				- Event TTL (Time to live) (1 - 1440 min)
			- If not, retry

		- OUTPT BATCHING: (hish-throughput scenarios)
			- Max events per batch
			- Preferred batch size in KB (target ceiling):
				- batch size < preferred (more events aren't available when publishing)
				- batch size > preferred (single event is larger than prefered)

		- DELAYED DELIVERY: If the first 10 events fail, grid add delays to all subsequent retries
		- DEAD-LETTER EVENTS:
			- Requires an storage account
			- If grid can't deliver the event within certain time or after trying to deliver a number of times, it can send the undelivered event to storage account.
			- It's disable by default
			- This process is executed if:
				- Event isn't delivered whitin TTL
				- Number of tries > limit

		- YOU CAN ADD CUSTOM DELIVERY PROPERTIES (Headers)

	* AUTHENTICATION:
		- Uses RBAC

	* WEBHOOK
		- Send a POST request to your endpoint
		- Endpoint validation:
			- SYNCHRONOUS HANDSHAKE (at subscription creation, grid sends an schema to validate)
			- ASYNCHRONOUS HANDSHAKE (for third-party services)
			- MANUAL VALIDATION (using a validationUrl valid for 5 min) for version 2018-05-01-preview

	* FILTERING:
		* event type
		* event subject
		* Grid allows ADVANCED FILTERIN using operators based on event data




* AZURE EVENT HUBS:
	- Stream million of events per second with LOW LATENCY
	- Any SOURCE ----> Any DESTINATION
	- Compatible with Apache KAFKA, Azure functions
	- Supports AMQP, Apache Kafka, HTTPS
	- AZURE SCHEMA REGISTRY: managing schemas of event streaming applications
	- AZURE STREAM ANALYTICS: Analyze streaming data
	- Allows automatically capture data in "Azure Blob Storage" or "Azure Data Lake Storage"

	* COMPONENTS:
		- Producer application (ingest data via DSK or Kafka client)
		- Namespace (Contains one or more event hubs or Kafka Topics)
		- Event Hubs/Kafka topic (Organize events into a topic, it has partitions)
		- Partition (Used to scale an event hub, lane in a freeway)
		- Consumer application (consume data by seeking through the event log and maintaining consumer offset. "Kafka consumer client" or "Event Hub SDKs client")
		- Consumer group (logical group of consumer instances reads data from a topic)

	* CAPTURE DATA:
		- Azure blob storage and Azure Data Lake Storage are used to capture data (same or different region)
		- Each partition is an independent segment of data and is consumed independently
		- Data in partition ages off based on configurable retention period
		- Captured data ----> APACHE AVRO FORMAT

	* TRAFFIC:
		- Controlled by THROUGHPUT UNITS
		- THROUGHPUT UNIT ----> 1MB -----> 1K events/second (ingress) - 2K events/second (egress)
		- Standard Event Hubs ---> 1 - 20 TU

	* PROCESSING APPLICATION:
		- Example:
			- There is a website for residents to monitor activity of their houses
			- 100K sensors sends data from houses (security)
			- Sensors pushes data to an Event Hub with 16 partitions
			- On the consuming end, it's necessary a mechanism that can read the events, consolidate them and upload to a blob storage
			- Then this data is ready to be used in the website for residents

		- Consumer client:
			- Te event procesing application may need scaling, running multiple instances of the app and balancing the load among themselves
			- EventProcessorClient (.NET and JAVA Azure Event Hubs SDKs)
			- When create an EVENT PROCESSOR, you specify the functions that process events and errors (CLIENT RESPONSABILITY HANDLE THE EVENT)
			- Execute as little processing as possible over received events.
			- Code with Retry logic to ensure the consumer processes every message at least once

			* CHECKPINT:
				- Checkpointing is a process by which an event processor marks or commits the position of the last successfully processed event within a partition. 
				- Marking a checkpoint is typically done within the function that processes the events and occurs on a per-partition basis within a consumer group.
				- If an event processor disconnects from a partition, another instance can resume processing the partition at the checkpoint that was previously committed by the last processor of that partition in that consumer group

			- Thread safety (Events from different partitions can be processed concurrently and any shared state that is accessed across partitions have to be synchronized)

		- Ownership of partitions is distributed among all the active event processor instances associated with an event hub and consumer group combination
		- For production:
			- EventProcessorClient > EventHubConsumerClient (performance)

	* ACCESS:
		- Supports Microsoft Entra ID and SAS
		- Supports authorization using MANAGED IDENTITIES, you need to sonfigure RBAC settings
		- Roles:
			- Azure event hubs DATA OWNER
			- Azure event hubs DATA SENDER
			- Azure event hubs DATA RECEIVER
		- SDK: Azure.Messaging.EventHubs




* AZURE MESSAGE QUEUES SOLUTIONS
	* COMPARISON
		* AZURE SERVICE BUS QUEUES
			(enterprise messaging)
			- Solution needs to receive messages without polling the queue (built in in bus SERVICE BUS)
			- Solution needs guaranteed FIFO order
			- Solution needs automatic duplicate detection
			- App requires to process messages as parallel long-running streams
			- 64KB < messages < 256KB or 1MB depending of tier (Service Bus supports > 100MB)
			- Solution requires rol-based access model to queues, permission for sender and receivers
		* AZURE STORAGE QUEUES
			(simple message buffering, scalable, low cost queueing)
			- NOT GUARANTEE FIFO
			- messages < 64KB
			- If the app must store > 80GB of messages in a queue
			- If want to track progress for processing a messaeg in queue (a worker can replace another based on tracking)
			- Solution requires server side logs

	* AZURE SERVICE BUS QUEUES
		- Message broker (messaging, transfer business data)
		- Decouple apps (client and service don't have to be online at the same time)
		- TOPIC has many SUBSCRIPTIONS
		- premium vs standard
			- 100MB -> 256KB max (message size)
			- throughput
			- Fixed -> pay as you go
			- Scale
		- ADVANCED FEATURES over storage queue
		- Protocols: AMQP
		
		- QUEUES:
			- POINT TO POINT PATTERN (MESSAGE PROCESSING)
			- One to one communication
			- load-leveling: enables producers and consumers to send and receive messages at different rates
			- To intermediate between message producers and consumers
			- Loose coupling between components
		
		- TOPICS AND SUBSCRIPTIONS:
			- PUBLISH SUBSCRIBE PATTERN (MESSAGE BROADCASTING)
			- One to many communication
			- Standard and next tiers offers this
			- Each published message is made available to each subscription registered with the topic
			- Consumer receive messages from subscriptions of the topic
			- A subscription resembles a virtual queue that receives copies of the messages that are sent to the topic
			- FILTER ACTIONS: Subscriptions can filter messages to the virtual subscription queue

		- RECEIVE MODES:
			- Receive and delete:
				- Message requested, bus marks message as consumed and returns it
				- If the consumer app crashes after bus marked message as consumed, it misses the message
				- For consumer apps that can tolerate not processing a message if failuer occurs.
			- Peek lock:
				- Finds the next message to be consumed, locks it to prevent other consumers from receiving it, and then, return the message to the app.
				- After the app finishes processing the message, it requests the Service Bus to complete the second stage of the receive process. Then, the service marks the message as consumed
				- If consumer app is unable to process the message, it can request to us to ABANDON the message so the message will be unlocked and will be available to be received again.
				- Timeout associated with the lock

		- MESSAGE:
			- Messages are transmitted as binary data (they need to be serialized to JSON, XML, etc)
			- Payload + metadata (key-value pairs)
			- Sometimes metadata alone is enough to carry information that the sender wants to communicate
			- BROKER PROPERTIES control message-level functionality, help the apps route messages to particualr destinations:
				- Simple request/reply
				- Multicast request/reply
				- Multiplexing
				- Multiplexed request/reply
			- Payload:
				- Opaque binary block

		- USE:
			- SDK for .NET
			- Dispose client and sender after usage
			- Initialize the client with conn string or credentials (AZURE Identity platform)
			- Initialize sender specifying queue name or topic


	* AZURE QUEUE STORAGE
		- For storing large numbers of messages
		- Access messages from anywhere in the world via authenticated calls using http or https
		- Max queue size: 64KB
		- Queue may contain millions of messages, up to total capacity limit of storage account.
		- Queues are commonly used to create a backlog of work to process asynchronously
		- COMPONENTS:
			- Url format (queues are addressable uing url)
			- Storage account
			- Queue (contains a set of messages, name of queue in lowercase)
			- Message (up to 64KB, supports TTL)
				- Up to 64KB
				- Supports TTL (time to live), -1 indicates no expiration, default TTL = 7 days

		- USE:
			- SDK for .NET
			- You need to manage deque if using PeekMessages (read the message without deletion, default messages number = 1)
			- If you use ReceiveMessages, the message is deleted from the queue after visibilityTimeout. You need to delete manually to ensure dequeue

















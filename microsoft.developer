jean.leon.v@outlook.com


* IAAS vs PAAS:
	- App Service => Platform as a Service
	- Virtual Machines => Infrastructure as a Service  
Repasar imagen


* AZURE APP SERVICE
	* FEATURES:
		- Windows (all tiers), Linux (isn't supported on Shared tier)
		- Built-in AUTO scale (scale up/down -> RESOURCES, scale out/in -> INSTANCES)
		- Containers support
			- Pull images from private registry, docker hub or Azure Container Registry
		- CI/CD
			- Azure DevOps, Github, Bitbucket, FTP, local git repo, Azure Container Registry
		* DEPLOY:
			- Automatic: Using CI/CD
			- Manual: Git, CLI (az webapp up), ZIP (using curl), FTPS/FTP
			- Containers:
				- Build and tag the image (Build pipeline step)
				- Push tagged image (Push to registry)
				- Update the slot with new image tag (Updating the tag automaticlly restarts the app app and pulls the new image)
		* COMMANDS:
			- create/update webapp (static page)
				- az webapp up -g $resourceGroup -n $appName --html
				… New-AzWebApp
		* NETWORKING:
			- INBOUND FEATURES: App-assigned address, Access restrictions, Service endpoints, Private endpoints
			- OUTBOUND FEATURES: Hybrid Connections, Gateway-required virtual network integration, Virtual network integration
	
	* SLOTS:
		- Live apps with their own host names (separate deployment)
		- For STANDARD tier or better
		- Standard (5 max), Premium (20 max)
		- App content and config can be swapped between two slots (for example, staging <-> production)
		- Recommended: Deploy new app version to "staging" slot, then swap "staging" and "production" slots. This allows fast rollback if needed
		- Internal Swap process:
			1) Apply settings (slot app settings, conn strings, CD settings, auth settings) from target slot to all instances of source slot
				(after this, all source instances are triggered to restart)
			2) Wait for every source slot instance to complete their restart (if restart fails, swap is reverted)
			3) If local cache is enabled, trigger local cache init by making an HTTP request to app root ("/") on each slot instance. Wait for any instance response.
				This triggers another restart on each instance
			4) If autoswapp is enabled with custom warm-up, trigger app init by making an HTTP request to app root ("/") on each slot instance.
				- If you need to run scripts or some init-resources before swap, you need to use "applicationInitialization" in web.config file (custom init action)
					(WEBSITE_SWAP_WARMUP_PING_PATH and WEBSITE_SWAP_WARMUP_PING_STATUSES)
				- If an instance returns any response, it's considered warmed up.
			5) If all instances on the source slot are WARMED UP, swap by switching routing rules
			6) Apply all source settings to swapped content in source slot and restart again.
			#########################################################################################################################
			- All work of initializing the swapped apps (and warmed up) happens on the source slot, the target slot remains online.
			- The production slot should be always the target (so swap doesn't affect production app)
			- Settings that are swapped:
				- General settings, app settings, connection strings, handler/path mappings, public certificates, Webjobs content
			- Settings that aren't swapped:
				- Publishing endpoints, custom domain, non public certs and tls/ssl settings, scale settings, webjos schedules, IP restrictions,
					always on, diagnostic log settings, CORS, virtual nw integration, managed identities
			- To make settings swappable set variable WEBSITE_OVERRIDE_PRESERVE_DEFAULT_STICKY_SLOT_SETTINGS = 0 / false
		- Swap with preview
			- Apply settings from target to source and you can validate the "result" before to swap to target (prod). You can continue or cancel the swap
		- Autoswap (only for Windows)
			- Enable and select target in general settings
			- For Azure DevOps scenarios (CI/CD, every time push the changes to slot, app service swap app into prod)
		- If swap fails, swap the same two slots INMEDIATELY and inspect activity log
		- Route traffic:
			- You can route a portion of the traffic (to prod slot) to another slot (to receive user feedback for features that aren't ready to release)
			- Once the client is routed to slot, it's pinned to that slot during the session's life
			- You give the user a button to "test beta" (url + route cookie) and route to non-prod slot.
			- Default routing rule (prod o specific slot) in new slot is 0%
	
	* APP SERVICE PLAN:
		- Define the set of compute resources and pricing:
			- Operating system
			- Region
			- Number of VMs
			- Size of VMs (small, medium, large)
			- Pricing tier categories:
				- SHARED COMPUTE (No scale-out)
					- Free (F1), Shared (D1)
				- DEDICATED COMPUTE (Allow scale-out, only apps in the same plan share resources)
					- Basic (B1, B2, B3) (3 instances), Standard () (10 instances max), Premium, PremiumV2, PremiumV3 (30 instances max)
				- ISOLATED (Maximun scale-out capabilities)
					- Isolated, IsolatedV2
		- An app always runs in an app service plan.
		- App service plan ------many----> App
		- Instances:
			- App service plan tier has INSTANCES
			- App runs on all VMs configured in APP SERVICE PLAN
			- All apps in the same APP SERVICE PLAN share the same VMs
			- All app slots run on the same VMs
			- Diagnostic logs, perform backups, webjobs (if enabled) use CPU and memory on these VMs
	
	* SCALING:
		- Keys
			- Autoscaling performs scaling IN and OUT (not UP and DOWN). It doesn't have any effect on CPU, memory, etc, It only changes the number of web servers of the APP SERVICE PLAN
			- Autoscaling is not an effective approach when the web app performs resource-intensive as part of each requuest (It needs scale-up)
			- Autoscaling isn't the best approach to handling LONG-TERM growth because autoscaling will end up being more expensive (if you can anticipate the rate of growth, do it manually)
			- For STANDARD tier or better
			- If you want to improve an specific app performance, you can try to moving the app to a separate APP SERVICE PLAN (different region, resource-intensive app, etc)
			- Autoscaling can't create more instances than the limit (tier limit), to prevent runaway autoscaling
		- Autoscaling Types:
			- Autoscaling with Azure autoscale
				- Types of condition:
					- Based on metrics rules (you define)
					- To a specific number of instances according to a schedule (you define)
				- Metric rules:
					- Autoscaling -> conditions -> rules
					- Azure can monitor:
						- CPU%, memory%, disk queue length (I/O requests), http queue length (client requests), data in, data out
					- Azure analyzes metrics:
						- Aggregates values for a metric of ALL INSTANCES within the TIME GRAIN (1 min default)
						- DURATION time is defined to aggregates the aggregation values within TIME GRAIN
						- Aggregation can be:
							- AVERAGE, MINIMUM, MAXIMUM, SUM, LAST, COUNT
					- Autoscale actions is triggered when a metric treshold has crossed based on rule (operators like greater then, equal to, etc)
					- Autoscale has a cool down period to stabilize between autoscale events
				- Combining Rules in the same condition
					- SCALE OUT RULE is performed if ANY of the scale-out rules are met
					- SCALE IN RULE is performed if ALL of the scale-in rules are met
					- You can perform scale-in rules independentlly separating scale-in rule in different condition
				- You can create multiple conditions of the two types and you can combine them
				- There is a default condition, and this doesn't have schedule
				- A rule specifies threshold for a metric and triggers an autoscale event when threshold is crossed.
				- Deallocate resources when workload has diminished
			- Azure app service automatic scaling
				- For PREMIUMV2 or better
				- Based on parameters you've selected
				- Azure automaticlly handles scaling decisions for web apps and app service plans
				- Use it if you have you webapp connected to legacy system wich may not scale as fast as the webapp setting the maximun number of instances to scale)
		- Best practices
			- Ensure the maximum and minimum values are different and have an adequate margin between them
			- Choose the appropriate statistic for your diagnostics metric (AVERAGE, MINIMUM, MAXIMUM, SUM, LAST, COUNT)
			- Choose the thresholds carefully for all metric types
			- Always select a safe default instance count
			- Configure autoscale notifications


	* APP CONFIGURATION:
		- General settings:
			- Stack settings:
				- Language, SDK, Start-up command (only for linux and containers)
			- Platform settings:
				- Platform bitness (32-b / 64-b, only for Windows)
				- FTP state
				- Http version
				- Web sockets (enable or disable)
				- Always On:
					- Enabled: Azure send continuos ping preventing the ap from being unloaded
					- Disabled: App is unloaded after 20 min without requests (high latency for new requests after sleep)
				- Session affinity (ensure a client is routed to the same instance during the session, for statefull apps)
				- Https only
				- Minimum TLS version
			- Debugging
			- Incoming client certificates (Require client certificates in mutual authentication)
		- Path mappings:
			- Handler mappings: If you need to csutomize how certain files are processde (e.g., directing .php files to specific handler)
				- Specific .py file should be handled for specific pyhton handler (define handler for .py)
			- Virtual application directory: If you need to manage app's folder structure or specific URL path (serve files via url)
				- https://yourapp.azurewebsites.net/media --> point to a sample storage directory
				- For custom paths, webapp serve files from site/wwwroot directly

		- ENVIRONMENT VARIABLES (application settings):
			- App service variables, for ASP.NET Core, It's like setting variables in appsettings.json (app service values override appsettings values).
			- App settings are always encrypted when stored
			- You can enable deployment slot settings (swappable setting)
			- For linux replace ":" by "_" for nested keys
			- Connection strings follow the same flow as app settings

	* DIAGNOSTIC LOGGING
		- Logging types:
			- Application logging (w, l): Messages and logs generated by the applitcation (framework/language) | app service file system / azure storage blobs
			- Web server logging (w): Raw http requests/responses data (emthod, uri, ip, port, status code, etc) | app service file system / azure storage blobs
			- Detailed error messages (w): Copies of the .html error pages (400 or greater, in no production env) | app service file system
			- Failed request tracing (w)
			- Deployment logging (w, l): Automatic logging for failed deployments | app service file system
		- Enable app logging in windows:
			- Enable file system (debug purposes) or blob logging (long-term), set category:
				Disabled: None
				Error: Error, Critical
				Warning: Warning, Error, Critical
				Information: Info, Warning, Error, Critical
				Verbose: Trace, Debug, Info, Warning, Error, Critical (all categories)
		- Enable app logging in linux/container:
			- Enable file system
			- Set disk quota and retention period.
		- Enable web server logging:
			- Enable file system or blob logging
			- Set retention days
		- ACCESS LOGS:
			- You can send specific traces to logs using packages of your language/framework
			- Log stream
			- az webapp log tail --name appname --resource-group myResourceGroup
			- Kudu
			- Consume blobs or download zip file if using file system

		- ADVANCED TOOLS (Kudu):
			- You can run cmd or powershell to interact with files of webapp in real time
			- You can view files and configuration of webapp (files deployed)
			- It's good for diagnostic (debug when It works in local but doesn't work in cloud environment)

	* SECURITY:
		- Certificates:
			- App service plan's resource group + region combination = WEBSPACE
			- A certifciate is accesible to other apps in the same WEBSPACE
			- Options:
				- Free app service managed private cert
				- Purchase app service private cert
				- Import cert from Azure Key Vault
				- Upload private/public cert
			- Free managed cert:
				- For BASIC tier or better
				- Renewed continuously and automaticlly in six month increments, 45 dayse before expiration
				- Vreate the cert and bind to a custom domain.

		- AUTHENTICATION / AUTHORIZATION:
			- Run in the same sandbox as the app code (for containers and linux, run in a separate container)
			- When enabled, it acts like a middleware (receive every incoming request)
			- Provides built-in authentication and authorization support
			- Out-of-the-box auth with federated identity providers (endpoints for each provider):
				- Microsoft identity platform, Facebook, Google, X, OpenID Connect provider, GitHub
			- Authentication Without SDK (with browser) / with SDK (browser-less app)
			- Authorization:
				- Allow unauthenticated request (authorization managed by the app)
				- Require authentication (apply to all requests)
			- Built-in token store
			- Auth traces are collected if application logging is enabled


* AZURE CONTAINERS

	* AZURE CONTAINER REGISTRY (ACR):
		- Store container images (Windows and Linux) (read only snapshots)
		- Pull/push images from azure services (AKS, App service, Service fabric, Batch) or in deployment workflows
		- You can use more than INCLUDED STORAGE up to storage limit (40TB for all ties) paying extra rate per GB
		- Tiers: (Similar capabilities, different storage and throughput)
				- BASIC = (INCLUDED STORAGE 10GB)
				- STANDARD = (INCLUDED STORAGE 100GB)
				- PREMIUM = (INCLUDED STORAGE 500GB) (high volume scenarios)

		* FEATURES:
			- Encryption-at-rest (ALL TIERS, encrypt before storing images, decrypt when use)
			- Geo-replication (PREMIUM)
			- Zone redundancy (PREMIUM)
			- Scalable storage (ALL TIERS, up to storage limit)
			- Create -----> az acr create
			- List -----> az acr repository list
			- Versions -----> az acr repository show-tags
			- Run -----> az acr run (like docker run)

		* ACR Taks:
			- Builds images for LinuxOS and AMD64 arch.
			- Streamline building, testing, pushing, deployment in Azure (automation)
			- Has associated source code context (Git repo specifying branch), tasks use
			- Build automaticlly when commits code to source
			- USE-CASES:
				- Quick task -----> Like "docker build", "docker push" in azure (without docker engine, on demand) -----> az acr build
				- Triggered tasks -----> Trigges to build on code update, base image update, schedule -----> az acr task create / track base image dependency / schedule
				- Multi-step task -----> It uses a .yaml file to define steps (custom workflow) 

	* AZURE CONTAINER INSTANCES (ACI):
		- No VM is needed, allows Windows and Linux
		- Focus: Ideal for tasks like batch/background jobs, simple stateless apis, isolated workloads without scaling or complex orchestration (dependencies)
		- Container instances are billed by the second (you need to stop when not using)

		* Container groups:
			- Similar to kubernetes POD, collection of container on the same host machine
			- You can specify the volumes to create, and the AZURE FILES SHARES to mount, you can only mount AZURE FILES SHARES to Linux containers
				("volumeMounts" and "volumes" properties of .yaml), you can do that using .yaml, command or Resource manager template
			- Exposes single public IP with port, can be assigned to a DNS name label, use AZURE FILES SHARES as volume mount (for each container)
			- Multi-containers for LINUX containers, Single-container for WINDOWS containers
			- Deploy using command, .YAML or Resource manager template (recommended .yaml)
			- All containers share one IP address and a common port namespace
			- Doesn't support port mapping (exposed port is the same as container port, if exposed). Containers internally comunicates each other via localhost and port
			- Restart policy:
				- ACI stops the container when its app, or script exits
				- ALWAYS (default), NEVER, ONFAILURE

		* Environment Variables:
			- az container create ... --environment-variables 'MaxProcesses'='10'
			- To secure values use "secureValue" property in .yaml file when set the variable

		* Commands:
			- az container create ... --file deploy.yaml
			- az container show
			- az container create ... --restart-policy OnFailure

	* AZURE CONTAINER APPS (ACA):
		- Supports any Linux-based x86-64 linux/amd64 image
		- No overhead of managing complex infrastructure (this service runs on top of AKS)
		- Focus: Microservices applications and robust autoscaling, even-driven apps, background procesing apps, api endpoints
		- Scaling based on: Http traffic, event-driven processing, CPU, memory, etc (KEDA-supported scaler)
		- Can split traffic across multiple versions of an app
		- A container app can contain multiple containers (SIDECAR pattern == app and a log agent for example) (Advanced scenario)
		
		- MICROSERVICES:
			- Independent scaling, versioning, upgrades
			- Service discovery
			- Native DAPR integration
			- DAPR integration provides: observability, pub/sub, service-to-service, retries, etc.

		* CONTAINER APPS ENVIRONMENT:
			- Secure boundary around groups of container apps
			- An environment can contain many container apps
			- Manage related services
			- Instrument DAPR applications
			- Same virtual network and log to the same LOG ANALYTICS WORKSPACE, and share the same DAPR conf
			- Different environment = apps never share the same compute resources, DAPR apps can't communicate
			- Environment ---contains---> container apps ---contains---> revisions, replica

		* REVISIONS:
			- Azure container apps create versioning by creating REVISIONS (immutable snapshot)
			- Quickly revert to an earlier version
			- New revision is created when REVISION-SCOPE changes are applied (revision suffix, container conf, image, scale rules)
			- By default a first revision is created (test-app -----> test-app--1st-revision, in this case, suffix = "1st-revision")

		* SECRETS:
			- Adding, removing or changing secrets doesn't create new revisions
			- az containerapp create ...  --secrets "super-secret=$CONNECTION_STRING"
			- You can use the secrets in env-vars when creating the container app
			- Container Apps doesn't support Azure Key Vault integration. Instead, enable managed identity in the container app and use the Key Vault SDK in your app to access secrets.

		* Commands:
			- az containerapp env create ...
			- az containerapp create ...
			- az containerapp create ...  --secrets "super-secret=$CONNECTION_STRING" ------> create container app with secrets
			- az containerapp update ... ------> modify env variables, compute resources, scale, deploy diff image.
			- az containerapp revision list ... -----> list rvisions of container app

		* AUTHORIZATION AND AUTHENTICATION:
			- Provides out-of-the-box auth (built-in auth features, low or no code) with federated identity providers
			- Built-in auth SHOULD be used with "allowInsecure" disabled and HTTPS
			- Identity providers: Microsoft identity platform, facebook, githun, google, x, openID
			- When use those providers, the SIGN-IN endpoint is available for auth token
			- The authentication and authorizations feature acts using a middleware component (for each HTTP request), runs as a SIDECAR container on each replica
			- You can use "With SDK flow" (federated) or "Without SDK flow" (client)





* FUNCTION APP:
	- Create function app:
		- Runtimes: It supports .net, nodejs, python, java, powershell, custom handler
			- Custom Handler: This options is used when need another runtime or language (go, php, etc)
		- f needs an storage account, that's because underneath the function model is a file system
		- Hosting plan:
			- Consumption (Serverless): Recommended for free
			- Flex Consumption
			- Functions premium
			- App Service
			- Container App environment
		- Networking: Private or public access
			- Enable networking injection is only available in functions premium
	- Create function:
		- Select template (Http trigger, timmer trigger, etc)
		- Authorization level:
			- Function (token)
			- Anonymous (open to everyone)
			- Admin (higher level of security required)
		- Test and code:
			- Get function url with key and execute
		- Monitoring:
			- Success and failed executions
			- Logs: real time logs
		- Using function core tools:
			- func init
			- func new
			- func start
			- func azure functionapp publish "fa name"
	- Trigger types:
		- QueueTrigger
		- HttpTrigger
		- BlobTrigger
		- TimerTrigger
		- EventHubTrigger
		- ServiceBusQueueTrigger
		- ServiceBusTopicTrigger
		- EventGridTrigger
		- CosmosDBTrigger
		- DaprPublishOutputBinding
		- DaprServiceInvocationTrigger
		- DaprTopicTrigger
	- Bindings:
		(https://learn.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings?tabs=isolated-process%2Cpython-v2&pivots=programming-language-csharp)
		- Output: output method to storage the response data of function app trigger
			- Blob storage: Create an storage account, and a container, create a new container and configure in C# using the container name, wildcard name and connection string and BlobOutput attribute
		- Types:
			Blob storage
			Azure Cosmos DB
			Azure Data Explorer
			Azure SQL
			Dapr4
			Event Grid
			Event Hubs
			HTTP & webhooks
			IoT Hub
			Kafka3
			Mobile Apps (only v1.x)
			Notification Hubs (only v1.x)
			Queue storage
			Redis
			RabbitMQ3
			SendGrid
			Service Bus
			SignalR
			Table storage
			Timer
			Twilio
		- Example:
			BlobInput: Some stored file in blob storage to read the content as input
			BlobOutput: File name with name parameter like this: blob/{name}-output.txt
			BlobTrigger: New or updated blob is detected (Stream or string)

	- Durable functions:
		- Enable to have sequence of steps (functions). This steps use ActivityTrigger and the orchestrator uses OrchestrationTrigger and launch all the ActivityTrigger functions.
		  The OrchestrationTrigger can be called in another function of any trigger type.
		  Example:
		  	HttpTrigger -----calls in function-----> OrchestrationTrigger -----calls in function-----> ActivityTrigger1 ---output---> ActivityTrigger2 ---output---> output in orchestrator

		- Patterns:
			- Function chaining pattern: (READY–)
				A[Orchestration Function Starts] --> B[Function A]
			    B --> C[Function B]
			    C --> D[Function C]
			    D --> E[Orchestration Function Completes]

			- Fan in / Fan out pattern:
				F1 calls three functions in parallel (F2), It will wait for all three functions to finish before move to F3
				A[Orchestration Function Starts] --> B[Function Fan-Out]
			    B --> C1[Function C1]
			    B --> C2[Function C2]
			    B --> C3[Function C3]
			    C1 --> D[Function Fan-In]
			    C2 --> D[Function Fan-In]
			    C3 --> D[Function Fan-In]
			    D --> E[Orchestration Function Completes]

			- Asynchronous API pattern:
				F1 start long task and give register of execution as output. 
				Another function GetStatus can call the output and monitoring execution.
				Another function DoWork can call the output and monitoring execution finish to execute the work

			- Monitor pattern:
				A function is waiting something to happen in another function, and execute when "that something" has happened

			- Human interaction pattern:
				A function request approval to call other functions, the function have a return a timeout if human approval is not received

		- Enable durable functions support:
			- For Javascript runtime, create diretly from azure portal and download the npm durable-functions package, then restart the funcion app
			- For .NET runtime:
				- Create the project (durable function orchestrator) from vscode and use azurite extension for test local storage (Azurite: Start in command palette)
				- For run in mac:
					- download the nuget package: Contrib.Grpc.Core.M1
					- add this before last project tag:
						<Target Name="CopyGrpcNativeAssetsToOutDir" AfterTargets="Build"> <ItemGroup> <NativeAssetToCopy Condition="$([MSBuild]::IsOSPlatform('OSX'))" Include="$(OutDir)runtimes/osx-arm64/native/*" /> </ItemGroup> <Copy SourceFiles="@(NativeAssetToCopy)" DestinationFolder="$(OutDir).azurefunctions/runtimes/osx-arm64/native" /> </Target>
				

* AZURE STORAGE ACCOUNT
	- Managed disks (usually fixed) are more expensive than blob storage (on demand).
	- Blob storage (or unmanaged storage) is the cheapest way to store files within azure.
	- Price depends on region
	- Storage account performance:
		* You pay for storage and you pay for the number of request per month.
		- GPV2 = Standard storage: 
		- Premium: It costs way more to store it, but you save a lot of money on the access. (If you need read thousands of files per second)
	- Storage account redundancy:
		- LRS: Locally
		- GRS
		- ZRS
		- GZRS
	- Blob storage tiers:
		- Premium
		- Hot
		- Cool
		- Archive: Hard to access, intended for backups and files that are rarely used.
	- Create storage account
		- performance: GPV2 or premium
		- redundancy
		- secure transfer
		- public access
		- Secutiry access: storage account keys or/and azure AD (entra)
		- data lake storage gen2: big data scenarios, change the file system
		- Networking: 
			- public, selected virtual networks, private (private endpoint)
			- microsoft network routing VS internet routing (slower)
		- Data protection:
			- soft delete
			- versioning and tracking
		- Encryption:
			- MMK
			- CMK
	- Access tier:
		- Hot: Frequently accesed, day-to-day usage scenarios
		- Cool: Infrequently accessed, backup scenarios
	- Storage browser:
		- Tool for search data in storage account easyly (containers, files, table, queues)
	- Azure storage account contains those storage services:
		- AZURE BLOB STORAGE
		- AZURE TABLE STORAGE
		- AZURE FILE STORAGE
		- AZURE QUEUE STORAGE
	- Blob storage vs File storage:
		- file storage uses a file system. (classic) hosted on azure, blob storage is highly scalable object storage solution.

	- AZURE BLOB STORAGE (Containers):
		- To access to blobs programaticlly
		- Use the SDK to manage container, blob level objects (BlobServiceClient, BlobContainerClient, BlobCLient)
		- StartCopyFromUriAsync and SetMetadataAsync as important methods.
		- AzCopy:
			- To copy files between container, even between different subscriptions (or different cloud providers)

	- Access Keys (if chosen storage account keys on creation):
		- Azure provide a pair of keys, they can be rotated manually (invalid old keys) or using a timmer (set rotation reminder)
		- YOU DONT WANT TO GIVE AWAY YOUR KEYS (use shared access signature instead)
	- Shared access signature (SAS)
		- To provide access to clients who should not be trusted.
		- You CANNOT revoke this access directly, unless rotate the signing key (based on access key). This invalids all the SAS generated using the rotated key.
		- There is another way to revoke the SAS acces, using stored access policy.

	- Data protection:
		- Recovery:
			- Operational backup: Using azure backup vault and a backup policy (retention days)
			- Point in time restore
			- Soft delete for blobs and containers (retention days)
	- Object replication
	- Lifecycle management:
		- Add rules that will move files from the hot tier to the cool or archive tier based on the last access date (you can configure to delete instead to move files)

* AZURE COSMOS DB
	- Fully managed NoSQL and relational database service for scalable, high performance applications.
	* KEYS:
		- Unlimited elastic write and read sclability
		- 99.999% read and write availability all around the world
		- Guarantedd read and writes serverd in < 10ms (near real time reads and writes)
	* APIs:
		- COSMOS DB FOR NOSQL: Core Native api for working with documents (JSON), supports familiar SQL language. Choose this when start from scratch (there is no data). ------> NON RELATIONAL
		- COSMOS DB FOR POSTGRESQL: OpenSource postgresql. Choose this if you need tables, foreign keys, primary keys, etc ------> RELATIONAL
		- COSMOS DB FOR MONGODB: MongoDb datbase for working with documents (BSON). Choose this if you have existing mongoDb data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR APACHE CASSANDRA: Cassandra database, mongodb and cassandra APIs are compatible (you can migrate without changes). Choose this if you have existing cassandra data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR TABLE: Compatible with azure table storage (you can migrate without changes). Choose this if you have existing azure table storage data ------> NON RELATIONAL
		- COSMOS DB FOR GREMLIN: Graph database service using gremlin query language (nodes, edges). Choose if you need relationships between data ------> NON RELATIONAL
	* HIERARCHY:
		- Database account > database > container > item, sps, fucntions, triggers, conflicts
	* TYPES:
		- database: Depends on api (NOSQL, POSTGRESQL, MONGODB, CASSANDRA, TABLE, GREMLIN)
		- container and items:
			- NOSQL: container > item
			- POSTGRESQL: table > row
			- MONGODB: collection > document
			- CASSANDRA: table > row
			- TABLE: table > item
			- GREMLIN: graph > node/edge
	- Create NOSQL:
		- Capacity mode:
			- Provisioned throughput: Fix an ammount of provisioned throughput to consume (RUs/s) regardless of consumption. Use free tier for this if possible
			- Serverless: Pay as you use
		- Limit throughput: Protects your account from cost overruns
		- Global distribution:
			- geo-redundancy: I created this in west and it will also create a version in east
			- multi-region writes
		- Backup Policy (NOT EXAM):
			- Periodic backup (interval, retention, copies, redundancy)
			- Continuos
	* Global replication:
		- You can add more regions (replications) as a READ REGIONS (you can program the application to read from the closest region to the client)
		- Muti region writes:
			- Enable WRITE and READ in all selected regions, this will double the cost
		- If the cosmos db has only a WRITE region (primary region), you can perform read and write operations in that region.
	- Keys:
		- You can access to cosmos using the URI (or endpoint, exposed on internet or azure virtual network) and a primary or secondary key
		- You have primary/seconday keys for Read-write regions, and read-only regions
	- Data explorer:
		- Create new container:
			- Create new db or use existing
			- Autoscale or manual
			- Set the max RU/s
			- Partition key: Distribute data across partitions for scalability
			- Unique keys: Specific unique keys in a partition
			- id key is required in each item and is a default unique key
		- Connect
			- When new Item is created, cosmos add 5 properties to object:
				- rid (row id), _self (self reference)
	* Data consistency:
		- Strong > Bounded staleness > session > consistent prefix > eventual
		- Cosmos uses "session" consistency as default
	* SDK:
		- code
	- Sp, trigger, user-defined functions:
		- sp: for large data executions. You can execute sp from sdk
		- triggers: to validate or add something before (pre trigger) and after (post trigger) creation/deletion/update
		- user-defined functions: function that you can use inside a query. To calculate fields.

	* CHANGE FEED NOTIFICATIONS:
		- Change feed in Azure Cosmos DB is a persistent record of changes to a container in the order they occur
		- This doesn't recognize item deletion or filter between insert and update operations. (Use soft delete + TTL to overcome deletion)
		- READ:
			- Push model: change feed push work to client, client has bussines loginc for processing work. Delegate hard work to cosmos (RECOMMENDED)
				- Azure functions:
					- Use azure functions (azure cosmos trigger) to execute some task when an item is created or updated.
				- Change feed processor library:
					- The monitored container: Container that has the data from with the change feed is generated
					- The lease container: state storage and coordinate processing the change feed across multiple workers
					- The compute instance: Instance that is listening for changes
					- The delegate: What the devs want to do with each bearch of changes

			- Pull model: client pull work from server, client has bussines logic for processing work, store state and handles load balancing. Extra low level control (NOT RECOMMENDED)


* MICROSOFT IDENTITY PLATFORM (ENTRA ID)

	* Components:
		- OAuth 2.0 and OpenID Connect standard-compliant authentication service
		- Open-source libraries
		- Microsoft identity platform endpoint
		- Application management portal
		- Application configuration API and powershell

	* Register app in the Azure portal:
		- When register an app with microsoft entra ID:
			- You're creating an IDENTITY CONFIGURATION.
			- Your must choose whether it is SINGLE TENANT or MULTI TENANT
			- An APPLICATION OBJECT (Globally unique app instance) is created in the home tenant.
			- A SERVICE PRINCIPAL OBJECT is created in the home tenant
			- You have a globally UNIQUE ID for the app (app or clientID)
			- You can add secrets, certificates, scopes and so on
	* IDENTITY CONFIGURATION:
		- Allows to integrate with microsoft entra ID
	* TENANT:
		- An instance of Azure Microsoft Entra ID that an organization or individual uses to manage users, apps, etc.
	* APPLICATION OBJECT:
		- Application ------------ one to one ------------ Application object
	* SCHEMA:
		- https://app.diagrams.net/#G1l1P8atQcV2vNu8CWlNm4b7_yU5Etr9_D#%7B%22pageId%22%3A%22MnA-WK4r-ufKpTyJempP%22%7D
	* INTEGRATION:
		- You need AzureAd in appsettings.json and use:
		
			- Cookie based auth (stateless or session):
				services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApp(Configuration.GetSection("AzureAd"));

	        - JWT based auth (stateless):
	        	services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApi(Configuration.GetSection("AzureAd"));

		- AzureAd section:
			- Instance: From directory
			- Domain: From directory
			- ClientId: From application (application Id)
			- TenantId: From directory could be:
				- "common": Auth request an organization account, allow users (and personal account) from any organization, multitenant
				- "organizations": Intended only for organizational users, not Microsoft personal accounts
				- "specific-tenant": Specific tenantID of the directory}
	* Permissions and consent:
		- permissions = scopes
		- Permission types:
			- DELEGATED ACCESS:
				- App has a signed-in user. The app acts as a signed-in users when it makes calls to target resource. (Microsoft entra id)
			- APP ONLY ACCESS:
				- App doesn't have a signed-in user. Only administrator can consent permissions for this app
		- Consents:
			- STATIC USER CONSENT:
				- Azure portal defined permissions (app configuration)
			- INCREMENTAL AND DYNAMIC USER CONSENT:
				- Ignore static and request permissions incrementally
				- To do so, specify the scopes your app need at any time by including new scopes in "scope" parameter when requesting access token (without the need to predefine them in the app registration)
				- The user has to consent the new permissions (ONLY APPLIES TO DELEGATED PERMISSIONS!!!!!!)
			- ADMIN CONSENT:
				- Is required when your app needs access to certain high-privilege permissions
		- Conditional access: TO PRACTICE

	* MSAL:
		The library enables devs to get tokens from Microsoft identity platform to authenticate and access to apis like microsoft graph, thir party apis, etc.
		Authentication flows:
			- Authorization code
			- Client credentials
			- Device code
			- Implicit grant
			- On-behalf-of (OBO)
			- Username/password (ROPC)
			- Integrated Windows authentication (IWA)
		Client applications:
			- Public (desktop, browserless APIs, mobile, browser apps)
			- Confidentials (web apps, web api apps, deamon/service apps)
		Complete frontend-backend flow:
			- Steps:
				*** DELEGATED ACCESS ***
				- Register client (spa) app registration
				- Register api app registration
				- Configure client app to authenticate against azure entra id using its client id and tenant of directory
				- Configure api app to validate incomming tokens using azure ad (jwt schema and azure configuration), and protect endpoints using Authorize
				- Add new scopes in api app registration in "expose API", set owner for this app
				- Register the created api scopes in client app registration, so the client can obtain a token to request api protected endpoints
				- Add the required scopes inthe login scopes in the client app so you can get a token authorized for all the needed scopes (the login prompt ask you for authorize that permissions)
				- Validate claims in the backend if needed (roles, policies, etc)
				- The client will be able to request microsoft graph or other resources (depending of the scopes) using its token (avoid this, and delegate this responsability for the backend)
				*** APP ONLY ACCESS ***
				- If you need request microsoft graph or other services from the backend, you need to use client credentials or certificate
			- https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-configure-app-expose-web-apis

	* SAS
		- To provide access to clients who should not be trusted
		- Signed URI that point to one or more storage resources
		- Includes a Token that indicates how the resources might be accessed by the client
		
		- Types:
			- USER DELEGATION SAS: (RECOMMENDED)
				Delegation SAS secured with microsoft entra credentials and permissions for the SAS. (BLOB STORAGE)
			- SERVICE SAS:
				Service SAS secured with storage account key. (BLOB, QUEUE, TABLE, AZURE FILES)
			- ACCOUNT SAS:
				Account SAS secured with storage account key. Delegate access to resources in one or more of the storage services.
			- SAS = URI + Authorization params
				- sp (access rights)
				- st, se (start access datatime, end access datetime)
				- sv (version of the storage API)
				- sr (kind of storage, blob, queue, etc)
				- sig (cryptographic signature)
			- When to use:
				- Service where users read and write their own data to my storage account. (front end proxy service / SAS provider service)
				- SAS is required when you want to copy Blobs or files to other blobs/storage accounts
			- STORED ACCESS POLICIES:
				- Consists of the start time, expiry time, and permissions for the signature (SERVICE LEVEL SAS).
				- You can create service level SAS (BLOB, QUEUE, TABLE, AZURE FILES) using existing stored access policies or setting custom properties
				- Services that support (BLOB, QUEUE, TABLE, AZURE FILES)

	* MANAGED IDENTITIES:
		* Apps can use managed identities to obtain Microsoft Entra tokens withous having to manage any credentials.
		* It's like giving your webapp a unique "personality" in the form of credentials that can interact with other resources
		* YOU MUST CHOOSE BETWEEN TRADITIONAL AUTHENTICATION (DELEGATED OR APP ONLY ACCESS VIA MICROSOFT ENTRA ID) and MANAGED IDENTITIES depends on the type of the application when you need to consume azure resources from a resource
		* MANAGED IDENTITY is simpler to use when the app (webapp, function, etc) needs access to azure resources WITHOUT TO STORE OR MANAGE CREDENTIALS
		* IF YOU USE MANAGED IDENTITY, YOU'LL BE USING DefaultAzureCredential AS CREDENTIALS WHEN USE THE SDK OF THE RESOURCES TO CONSUME

		* SYSTEM ASSIGNED MANAGED IDENTITY:
			- Created as part of an azure resource (webapp, vm, etc, tied to the azure resource)
			- Can't be shared
		* USER ASSIGNED MANAGED IDENTITY:
			- Created as stand-alone azure resource
			- Can be shared

		* Can be used to authenticate servicees that support Microsoft Entra auth.

		* WEB APP EXAMPLE:
			- ENABLE SYSTEM ASSIGNED MANAGED IDENTITY
			- IN WEBAPP SET THE SPECIFIC PERMISSION (in this case READER PERMISSION over a key vault rsource) (option 1)
				- Or Add the ROLE ASSIGNMENT in the key vault (IAM SECTION) pointing to the webapp (option 2)
				- The key vault must be RBAC (not access policy)
			- USE DefaultAzureCredential from code (SDK) so you can retrieve keys and secrets from key vault (DefaultAzureCredential try to authenticate via multiple mechanisms, environment variable, interactive, etc)

	* CONFIGURE MICROSOFT ENTRA ID ACCESS:
		* In some service, go to IAM, and add role assignment, add role, and member:
			- User, group, service principal (for a custom app - APP REGISTRATION - for example)
			- Managed Identity (related with an azure service)
		* For app registration:
			- Get secrets and consume the service from SDK using ClientSecretCredential or specific credentials type.
		* For delegated access:
			- Get the token using authentication/authorization at app level, or use PublicClientApplicationBuilder to acquire token manually, then make a request to the desired service (similar to consume graph).



* MICROSOFT GRAPH
	
	- Microsoft graph Connectors: Incoming data direction (external data sources: Jira, Google drive, etc)
	- Microsoft graph Data Connect: Cached data serves as data source for azure development tools
	- You need auth tokens of an app registration. to request microsoft graph api
	- Resources: me, teams, groups, etc
	- Query parameters: filters

	* SDK
		- Sevice library: models, request builders from graph metadata
		- Core library: features to work with graph services
		- TO REQUEST:
			- YOU MUST CHOOSE THE AUTHENTICATION PROVIDER:
				- Authorization code provider - (DELEGATED)
				- Client credentials provider - (APP ONLY)
				- On-behalf-of provider - (DELEGATED)
				- Interactive provider - (DELEGATED)
				- Integrated Windows provider - (DELEGATED)
				- Username/password provider - (DELEGATED)
				- Device code provider - (DELEGATED)
					########## - YOU NEED CERTIFICATE OR CLIENT SECRET in client credentials case ##########
					########## - YOU NEED TO ENABLE MOBILE AND DESKTOP FLOWS in device code case ##########
			- YOU MUST ADD THE ENOUGH PERMISSIONS FOR THE APP REGISTRATION
			- YOU NEED TO GRANT PERMISSION AS ADMIN USER IF NEEDED
			- You can interact with all the services of microsoft graph using the selected authentication provider
			- There are some endpints that don't work using client credentials (/me because this requires a delegated flow)
		- https://learn.microsoft.com/en-us/graph/sdks/choose-authentication-providers?tabs=csharp

	* Request:
		- You can use MSAL to acquire access token to microsoft graph (specifying scopes)

	* BEST PRACTICES:
		- Use least privilege
		- Use the correct permission type based on scenarios
		- Consider the end user and admin experience (consents)
		- Consider multi-tenant applications
		- Handle responses effectively (pagination, Evolvable enumerations)
		- Cache locally for specific scenarios to avoid make a lot of requests to graph


* AZURE KEY VAULT:
	- Service for securely storing and accessing secrets (API keys, passwords, certificates, cryptographic keys)
	- Container types:
		- Vaults (software and HSM-backed keys, secrets and certificates)
		- Managed hardware security module (HSM) pools (HSM-backed keys)
	- Access requires authentication and authorization (user o application)
		- Authentication: Microsoft Entra Id
		- Authorization: Azure RBAC or Key Vault access policy
	- Diagnostic Settings:
		- Enable logging for vaults to:
			- Archive to a storage account
			- Stream to an event hub
			- Send logs to Azure Monitor Log

	- BEST PRACTICES:
		- Authentication:
			- Managed identities for Azure resources (BEST PRACTICE)
			- Service principal and certificate (NOT RECOMMENDED)
			- Service principal and secret (NOT RECOMMENDED)
		- Azure key vault use TLS (to protect data when it's traveling between Key Vault and clients) and PFS (to protect connection between customers)
		
		- A vault per application per environment
		- Control access to vault (sensitive data)
		- Backup
		- Logging
		- Recovery (soft delete)

	- AUTHENTICATION TO KEY VAULT:
		- System-assigned managed identity.
		- Using Azure SDK MSAL

	- USE:
		- SDK, CLI, PORTAL
		- CLI:
			- az group create
			- az keyvault create
			- az keyvault secret show
			- az keyvault secret set



* AZURE CACHE FOR REDIS:
	
	- Provides in memory data store absed on REDIS
	- To process large volumes of app requests bykeeping frequently accessed data in the server memory (read and write quickly)
	- Offers REDIS OPEN SOURCE and REDIS ENTERPRISE

	* SCENARIOS:
		- DATA CACHE (load data into the cache only as needed)
		- CONTENT CACHE (quick access to static content compared to backend datastores - templates, headers, footers)
		- SESSION STORE (user session data, shooping carts, us cookies as key to query the inmemory cache, faster than relational db)
		- JOB AND MESSAGE QUEUING (for requests that take time to execute)
		- DISTRIBUTED TRANSACTIONS (Redis supports executing a batch of commands as a single transaction)

	* TIERS: (determines the size and performance)
		- BASIC < STANDARD < PREMIUM < ENTERPRISE < ENTERPRISE FLASH
		- ENTERPRISE and ENTERPRISE FLASH allows clustering to automaticlly split dataset amoing multiple nodes

	* CONFIGURE AN INSTANCE:
		- The name contains numbers, letters and '-' (1 - 63 char), cannot startor end with '-', consecutive '-' invalid
		- Always place the cache instance and app in the same region (or as close as possible, even outside of azure), different region = more latency, less reliability
		- Microsoft recommends to use Standard tier or higher for production systems.

	* ACCESS:
		- Redis command line tool
		- Add TTL (Time to live - expiration time) to keys if needed. Redis stores the date when a key expires
		- Expire time resolution is always 1 ms, and expiration can be set using s or ms
		- Use from client:
			- The host name is the public internet addess of the cache instance. Use access key (primary or secondary)
			- Use from .NET
				- Add package StackExchange.Redis
				- Use host address, port, access key or CONNECTION STRING from azure
				- Connection string contains "ssl" (encrypted communication) and "abortConnection" (allows a connection to be created even if the server is unavailable at that moment)
				- The ConnectionMultiplexer class is optimized to manage connections efficiently
				- Get the db instance from the redis connection (after execute ConnectionMultiplexer.Connect)
				- You can execute specific commands in db object, or defined methods
				- You can store complex values (classes) and then serialize to JSON or XML and then retrieve and deserialize to class again.
				- Dispose the connection object when finish


* CDN:
	- Content Delivery Network, uses CDN POPs (point of precense this contains edge servers)
	- PROS:
		- Better performance
		- Improved user experience
		- Less traffic is sent to the origin server
		- Large scaling
	- HOW IT WORKS:
		- USER1 request content
		- DNS routes the request to the best POP location (geographically closets)
		- If the edge servers in POP don't have the content in cache, POP request the content from the origin server
		- Origin returns the content to edge server (updated copy)
		- An edge server in POP caches the content and return the content to USER1
		- Content remains cached on edge server until TTL (time to live, DEFAULT 7 DAYS if origin didn't specify that)
		- USER2 request content
		- If DNS routes to the same POP and the content hasn't expired, edge server returns the content from cache

	 - TIME TO LIVE:
	 	- TTL gets determined by the Cache-Control HEADER in the response from the origin
	 	- Default values:
	 		- Generalized web delivery optimizations: seven days (one week)
			- Large file optimizations: one day
			- Media streaming optimizations: one year

	- CACHING AND RULES:
		- CACHING RULES: (endpoint level)
			- Global rules (affects all requests to the endpoint, overrides any http cache header)
			- Custom rules (match specific paths, file extensions, overrides global caching)

			- STANDARD RULES ENGINE:
				- Rule = match conditions + action

		- QUERY STRING CACHING:
			- Uses query string

	- REQUIREMENTS:
		- Azure subscription
		- Azure CDN profile (collection of cdn endpoints which users can customize with cdn behavior and access)
		- Pricing gets applied at CDN profile level
	- FEATURES:
		- Dynamic site acceleration
		- CDN caching rules
		- HTTPS custom domain support
		- Azure diagnostics logs
		- File compression
		- Geo-filtering (allow or block content in specific country/region)

	- PURGE CACHED CONTENT: (Force all edege nodes to retrieve new updated assets)
		- Endpoint level (one or more)
		- File level
		- Wildcard level (path)
		- Commands:
			 - az cdn endpoint purge: where a large amount of data is invalidated and should be updated in the cache
			 - az cdn endpoint load: where app creates a large ammount of assets and you want to imprive performance before any request (caching the assets before)

	- SDK: Microsoft.Azure.Management.Cdn (client: CdnManagementClient)



* APPLICATION INSIGHTS
	- Extenssion of AZURE MONITOR
	- Provides Application Performance monitoring (APM) features
	- Proactive (how an app is performing) and Reactive (determine the couse of an incident) monitoring
	- Store logging data, collect metrics and telemetry data (telemetry is sent to application insights for analysis and presentation)
	- Monitors:
		- request / failure rates, response times (internal and dependencies), where users are, etc
		- exceptions
		- page views load performance
		- ajax calls
		- performance counters, memory, CPU, network usage
		- host diagnostics from Docker or Azure
		- trace logs
		- custom events

	- Metrics:
		- Log-based metrics:
			- FOR DATA ANALYSIS DIAGNOSTICS
			- Behind the scene are translated into Kusto queries from stored events
			- Application insights stores all collected events as logs (manually sent from code skd or from autoinstrumentation)
		- Standard metrics:
			- FOR REAL TIME ALERTING
			- Stored as preaggregated time series (better performance at query time)
			- Application insights stores metrics as preaggregated time series, and only with key dimensions

	- Connect to webapp:
		- Download the SDK (Microsoft.Application.Insights.AspNetCore), and add Services.AddApplicationInsightsTelemetry();
		- You don't need to add application insights connection string to appsettings because it comes by default in your webapp (as an environment variable) when you enable application insights when creating the app.

	- INSTRUMENTING:
		- Is enabling an app to capture telemetry
		- two ways:
			- Automatic instrumentation (AUTOINSTRUMENTATION)
			- Manual instrumentation

		- AUTOINSTRUMENTATION:
			- Enables collection through configuration without touching the app's code (LESS CONFIGURABLE)

		- MANUAL INSTRUMENTATION:
			- Coding against application insights or OpenTelemetry API (Install sdk, and use it from code to capture dependencies not captured with autoinstrumentation)
			- Use the SDK if you require csutom events and metrics, control over telemetry flow, autoinstrumentation isn't available
			- two ways:
				- Application insigths SDKs
				- Azure Monitor OpenTelemetry Distros

		- OPEN TELEMETRY TERMS (replacing legacy application insights terms):
			- autocollectors = instrumentation libraries
			- channel = exporter
			- codeless / agent-based = autoinstrumentation
			- traces = logs
			- requests = server spans
			- dependencies = client, internal, etc
			- operation ID = trace ID
			- ID or operation parent ID = span ID

	- AVAILABILITY TESTS AND TROUBLESHOOT:
		- Application insights sends requests to webapp at regular intervals from points around the world to alert if the app isn't responding or rsponds too slowly.
		- 100 availabilty tests per application insights.
		- Availabitlity tests types:
			- Standdard test (single test http/https request)
			- Custom TrackAvailability test (from code using sdk in another app, complex requests, auth flows, etc)
			- URL ping test (classic ping, deprecated)

		- Application map (troubleshoot app performance)
			- You need to configure the app (sdk installed) and resources (enabling Distributed Tracing) to use the application map and identify bottlenecks


* AZURE APP CONFIGURATION
	- Service to centrally manage application settings
	- Stores data as key-value pairs
	- App configuration doesn't version key values. Use labels to version.
	- Each key-value is uniquely identified by its key plus a label
	- Scenarios:
		- Centralize management and distribution of hierarchical configuration data for different environments and geographies
		- Dynamically change application settings without the need to redeploy or restart an app
		- Control feature availability in real time
	
	- TO USE IN CODE:
		- App Configuration Provider for the specific programming language/framework
	
	- FORMAT:
		AppName:Service1:ApiEndpoint
		AppName:Service2:ApiEndpoint
		(invaid: "*", ",", "\", escape invalid using "\" at the begining)

	- LABEL (default: key-value has no label, used to differentiate key-values with the same Key):
		Key = AppName:DbEndpoint1 & Label = Test
		Key = AppName:DbEndpoint2 & Label = Staging
		Key = AppName:DbEndpoint3 & Label = Production
		("\0" = explicitly reference key-value without a label)

	- APPLICATION FEATURE:
		- Feature flag: Flag (true or false) to enable or disable a code block
		- Feature manager: Module of application manager to have extra functionality over feature
		- Filter: Rule for evaluating the state of a feature
			- Types:
				- Targeting filter: test feature only for specific users or groups
				- Time window filter: test feature during a specific date range
				- Custom filter

	- ENCRYPTION:
		- It encrypts sensitive information at rest using AES256
		- When customer-managed key capability is enabled, app configuration uses a manged identity assigned to the app config instance to authenticate with Microsoft Entra ID.
		- Managed identity then calls Azure Key Vault and wraps the app configuration instance's encryption key.
		- The wrapped encryption key is then stored and the unwrapped encryption key is cached within app config for ONE HOUR.
		- App config refreshes the unwrapped version of the app config instance's encryption hourly.

	- You can use private endpoints to allow clients on a virtual network to securely access data over a private link.




* API MANAGEMENT:
	- Core functionalities to ensure a successful aPI program.
	- Components:
		- API GATEWAY: route calls to backends, verify request credentials, quotas and limits, cache, logs, metrics, etc.
			- Executes CROSS-CUTTING tasks:
				- Authentication, SSL, rate limiting, quotas, routing.
			- Without API GATEWAY involves problems.
		- MANAGEMENT PLANE: api service settings, define api schema, package api -> products, policies, manage users
		- DEVELOPER PORTAL: api docs, call api via console, manage api keys
	- PRODUCTS:
		- A PRODUCT has one or more APIs, it has a title, descriptiom and terms of use.
		- OPEN: Can be used without subscription
		- PROTECTED: It needs subscription (require ADMIN APPROVAL or AUTOAPPROVED)
	- GROUPS: (limit the visibility of products to developers)
		- Administrators, Developers, Guests
		- Allows custom groups (or external microsoft entra)
	- POLICIES
		- Collection of statements executed sequentially on the request or response.
		- Simple XML doc that describes statements in sections:
			- inbound (request)
			- backend (before request is forwarded to backend)
			- outbound (response)
			- on-error (error)
		- You can use policy expressions if you need (C# statement, like razor pages), the context variable is provided by default (http context of .net)
		- There are a large number of pre defined policies you can use at different level and scope
		- You can use <base /> to inherit policies
		- Popular policies:
			
			- choose: classic if-else
							<choose>
							    <when condition="Boolean expression | Boolean constant">
							        <!— one or more policy statements to be applied if the above condition is true  -->
							    </when>
						    </choose>

			- forward: forward incoming request to specific backend
							<forward-request timeout="time in seconds" follow-redirects="true | false"/>

			- limit: limit number of requests
							<limit-concurrency key="expression" max-count="number">
							        <!— nested policy statements -->
							</limit-concurrency>

			- log: send messages to event hub
							<log-to-eventhub logger-id="id of the logger entity" partition-id="index of the partition where messages are sent" partition-key="value used for partition assignment">
							  Expression returning a string to be logged
							</log-to-eventhub>

			- mock: mock APIs and operations
							<mock-response status-code="code" content-type="media type"/>

			- retry: retries execution of child policies
							<retry
							    condition="boolean expression or literal"
							    count="number of retry attempts"
							    interval="retry interval in seconds"
							    max-interval="maximum retry interval in seconds"
							    delta="retry interval delta in seconds"
							    first-fast-retry="boolean expression or literal">
							        <!-- One or more child policies. No restrictions -->
							</retry>

			- return: abort pipeline execution policies
							<return-response response-variable-name="existing context variable">
							  <set-header/>
							  <set-body/>
							  <set-status/>
							</return-response>

	- HOSTING:
		- Managed (All API traffic flows through Azure)
		- Self-hosted (For hybrid and multicloud or on-premise scenarios)
	
	- SECURITY:
		- Subscription keys:
			- Subscription has two keys (rotation)
			- Including this in http request (header or query parameter) (Header: Ocp-Apim-Subscription-Key)
			- Devs who need to consume published APIs can get subscriptions (subscription requst for PROTECTED products)
			- Scopes: ALL APIS (all the gateway), SINGLE API (specific api), PRODUCT (colelction of apis of product)
		- OAUTH2
		- IP whitelist
		- Client certificates:
			- TLR security
			- Are signed
			- API Management gateway inspect the certificate within the request and check properties: Certificate Authority (CA), Thumbprint, Subject, Expiration Date
			- Validation: Check who issued the certificate, If it's issued by partner, check that it came from them


* AZURE EVENT GRID
	- Pub Sub message distribution
	- For event driven serverless architecture
	- Uses HTTP and MQTT
	- Maximum allowed size for event: 1 MB
	- Maximum size of events array received in Event grid: 1 MB
	* (event source) publisher ----> azure grid -----> subscriberA, subscriberB, ... subscriberN (consumer can be the publisher too)
	- publisher can be a PARTNER (sends events from its system to make them available to azure customers)
	- Event source: 
		- Azure Storaege, IoT Hub, Custom app, etc.
		- Send events to Event Grid in an array (max length size: 1 MB)
		- If an event or array > 1 MB -----> 413 PAYLOAD TOO LARGE
		- Operations are charged in 64 KB increments. (1 event of 130 KB ----> 3 events)
	- Topic: A collection of related events:
		- SYSTEM TOPICS: Provide data about the resource. Access = you can suscribe to events
		- CUSTOM TOPICS: Custom apps and third-party event sources
		- PARTNER TOPICS: To subscribe to events published by a PARTNER
	- Event subscription: Tells grid which events on a topic you want to receive - Subscribers can be azure services (storage account, functions, etc) or CUSTOM WEBHOOKS
	- Event handler: Part of grid where the event is sent. Process the event

	* SCHEMAS:
		- Event grid schema

			[
			  {
			    "topic": string,  ------> (optional)
			    "subject": string,  ------> (REQUIRED) // to give the ability to filter events to the subscriber (file extensions, paths, containers, etc)
			    "id": string,   ------> (REQUIRED)
			    "eventType": string, ------> (REQUIRED)  // event type filtering
			    "eventTime": string,------> (REQUIRED)
			    "data":{. ------> (optional)
			      object-unique-to-each-publisher  // defined by publisher for custom topics
			    },
			    "dataVersion": string,  ------> (optional)
			    "metadataVersion": string. ------> (optional)
			  }
			]

			different header: "content-type":"application/json; charset=utf-8"

		- Cloud event schema (open specification)

			{
			    "specversion": "1.0",
			    "type": "Microsoft.Storage.BlobCreated",  
			    "source": "/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.Storage/storageAccounts/{storage-account}",
			    "id": "9aeb0fdf-c01e-0131-0922-9eb54906e209",
			    "time": "2019-11-18T15:13:39.4589254Z",
			    "subject": "blobServices/default/containers/{storage-container}/blobs/{new-file}",
			    "dataschema": "#",
			    "data": {
			        object-unique-to-each-publisher
			    }
			}

			different header: "content-type":"application/cloudevents+json; charset=utf-8"
		* All remaining headers are the same

	* EVENT SUBSCRIPTION:
		- Grid tries to send each event at least once for each subscription, if there's a failure, grid retries based on RETRY SCHEDULE or RETRY POLICY
		- Grid doesn't guarantee order for event delivery
		
		- RETRY SCHEDULE:
			- status code 400, 412 for azure resources -> dead-letter event, doesn't retry
			- status code 400, 412, 401 for webhook -> dead-letter event, doesn't retry
			- (if dead-letter isn't configured, events will be dropped)
			- Normal scenario:
				- Grid waits 30sfor response, if endpoint hasn't respond, is queued for retry.
				- Exponential backoff retry policy

		- RETRY POLICY
			- An event is dropped if:
				- Maximum number of attempts (1 - 30 attempts)
				- Event TTL (Time to live) (1 - 1440 min)
			- If not, retry

		- OUTPT BATCHING: (hish-throughput scenarios)
			- Max events per batch
			- Preferred batch size in KB (target ceiling):
				- batch size < preferred (more events aren't available when publishing)
				- batch size > preferred (single event is larger than prefered)

		- DELAYED DELIVERY: If the first 10 events fail, grid add delays to all subsequent retries
		- DEAD-LETTER EVENTS:
			- Requires an storage account
			- If grid can't deliver the event within certain time or after trying to deliver a number of times, it can send the undelivered event to storage account.
			- It's disable by default
			- This process is executed if:
				- Event isn't delivered whitin TTL
				- Number of tries > limit

		- YOU CAN ADD CUSTOM DELIVERY PROPERTIES (Headers)

	* AUTHENTICATION:
		- Uses RBAC

	* WEBHOOK
		- Send a POST request to your endpoint
		- Endpoint validation:
			- SYNCHRONOUS HANDSHAKE (at subscription creation, grid sends an schema to validate)
			- ASYNCHRONOUS HANDSHAKE (for third-party services)
			- MANUAL VALIDATION (using a validationUrl valid for 5 min) for version 2018-05-01-preview

	* FILTERING:
		* event type
		* event subject
		* Grid allows ADVANCED FILTERIN using operators based on event data




* AZURE EVENT HUBS:
	- Stream million of events per second with LOW LATENCY
	- Any SOURCE ----> Any DESTINATION
	- Compatible with Apache KAFKA, Azure functions
	- Supports AMQP, Apache Kafka, HTTPS
	- AZURE SCHEMA REGISTRY: managing schemas of event streaming applications
	- AZURE STREAM ANALYTICS: Analyze streaming data
	- Allows automatically capture data in "Azure Blob Storage" or "Azure Data Lake Storage"

	* COMPONENTS:
		- Producer application (ingest data via DSK or Kafka client)
		- Namespace (Contains one or more event hubs or Kafka Topics)
		- Event Hubs/Kafka topic (Organize events into a topic, it has partitions)
		- Partition (Used to scale an event hub, lane in a freeway)
		- Consumer application (consume data by seeking through the event log and maintaining consumer offset. "Kafka consumer client" or "Event Hub SDKs client")
		- Consumer group (logical group of consumer instances reads data from a topic)

	* CAPTURE DATA:
		- Azure blob storage and Azure Data Lake Storage are used to capture data (same or different region)
		- Each partition is an independent segment of data and is consumed independently
		- Data in partition ages off based on configurable retention period
		- Captured data ----> APACHE AVRO FORMAT

	* TRAFFIC:
		- Controlled by THROUGHPUT UNITS
		- THROUGHPUT UNIT ----> 1MB -----> 1K events/second (ingress) - 2K events/second (egress)
		- Standard Event Hubs ---> 1 - 20 TU

	* PROCESSING APPLICATION:
		- Example:
			- There is a website for residents to monitor activity of their houses
			- 100K sensors sends data from houses (security)
			- Sensors pushes data to an Event Hub with 16 partitions
			- On the consuming end, it's necessary a mechanism that can read the events, consolidate them and upload to a blob storage
			- Then this data is ready to be used in the website for residents

		- Consumer client:
			- Te event procesing application may need scaling, running multiple instances of the app and balancing the load among themselves
			- EventProcessorClient (.NET and JAVA Azure Event Hubs SDKs)
			- When create an EVENT PROCESSOR, you specify the functions that process events and errors (CLIENT RESPONSABILITY HANDLE THE EVENT)
			- Execute as little processing as possible over received events.
			- Code with Retry logic to ensure the consumer processes every message at least once

			* CHECKPINT:
				- Checkpointing is a process by which an event processor marks or commits the position of the last successfully processed event within a partition. 
				- Marking a checkpoint is typically done within the function that processes the events and occurs on a per-partition basis within a consumer group.
				- If an event processor disconnects from a partition, another instance can resume processing the partition at the checkpoint that was previously committed by the last processor of that partition in that consumer group

			- Thread safety (Events from different partitions can be processed concurrently and any shared state that is accessed across partitions have to be synchronized)

		- Ownership of partitions is distributed among all the active event processor instances associated with an event hub and consumer group combination
		- For production:
			- EventProcessorClient > EventHubConsumerClient (performance)

	* ACCESS:
		- Supports Microsoft Entra ID and SAS
		- Supports authorization using MANAGED IDENTITIES, you need to sonfigure RBAC settings
		- Roles:
			- Azure event hubs DATA OWNER
			- Azure event hubs DATA SENDER
			- Azure event hubs DATA RECEIVER
		- SDK: Azure.Messaging.EventHubs




* AZURE MESSAGE QUEUES SOLUTIONS
	* COMPARISON
		* AZURE SERVICE BUS QUEUES
			(enterprise messaging)
			- Solution needs to receive messages without polling the queue (built in in bus SERVICE BUS)
			- Solution needs guaranteed FIFO order
			- Solution needs automatic duplicate detection
			- App requires to process messages as parallel long-running streams
			- 64KB < messages < 256KB or 1MB depending of tier (Service Bus supports > 100MB)
			- Solution requires rol-based access model to queues, permission for sender and receivers
		* AZURE STORAGE QUEUES
			(simple message buffering, scalable, low cost queueing)
			- NOT GUARANTEE FIFO
			- messages < 64KB
			- If the app must store > 80GB of messages in a queue
			- If want to track progress for processing a messaeg in queue (a worker can replace another based on tracking)
			- Solution requires server side logs

	* AZURE SERVICE BUS QUEUES
		- Message broker (messaging, transfer business data)
		- Decouple apps (client and service don't have to be online at the same time)
		- TOPIC has many SUBSCRIPTIONS
		- premium vs standard
			- 100MB -> 256KB max (message size)
			- throughput
			- Fixed -> pay as you go
			- Scale
		- ADVANCED FEATURES over storage queue
		- Protocols: AMQP
		
		- QUEUES:
			- POINT TO POINT PATTERN (MESSAGE PROCESSING)
			- One to one communication
			- load-leveling: enables producers and consumers to send and receive messages at different rates
			- To intermediate between message producers and consumers
			- Loose coupling between components
		
		- TOPICS AND SUBSCRIPTIONS:
			- PUBLISH SUBSCRIBE PATTERN (MESSAGE BROADCASTING)
			- One to many communication
			- Standard and next tiers offers this
			- Each published message is made available to each subscription registered with the topic
			- Consumer receive messages from subscriptions of the topic
			- A subscription resembles a virtual queue that receives copies of the messages that are sent to the topic
			- FILTER ACTIONS: Subscriptions can filter messages to the virtual subscription queue

		- RECEIVE MODES:
			- Receive and delete:
				- Message requested, bus marks message as consumed and returns it
				- If the consumer app crashes after bus marked message as consumed, it misses the message
				- For consumer apps that can tolerate not processing a message if failuer occurs.
			- Peek lock:
				- Finds the next message to be consumed, locks it to prevent other consumers from receiving it, and then, return the message to the app.
				- After the app finishes processing the message, it requests the Service Bus to complete the second stage of the receive process. Then, the service marks the message as consumed
				- If consumer app is unable to process the message, it can request to us to ABANDON the message so the message will be unlocked and will be available to be received again.
				- Timeout associated with the lock

		- MESSAGE:
			- Messages are transmitted as binary data (they need to be serialized to JSON, XML, etc)
			- Payload + metadata (key-value pairs)
			- Sometimes metadata alone is enough to carry information that the sender wants to communicate
			- BROKER PROPERTIES control message-level functionality, help the apps route messages to particualr destinations:
				- Simple request/reply
				- Multicast request/reply
				- Multiplexing
				- Multiplexed request/reply
			- Payload:
				- Opaque binary block

		- USE:
			- SDK for .NET
			- Dispose client and sender after usage
			- Initialize the client with conn string or credentials (AZURE Identity platform)
			- Initialize sender specifying queue name or topic


	* AZURE QUEUE STORAGE
		- For storing large numbers of messages
		- Access messages from anywhere in the world via authenticated calls using http or https
		- Max queue size: 64KB
		- Queue may contain millions of messages, up to total capacity limit of storage account.
		- Queues are commonly used to create a backlog of work to process asynchronously
		- COMPONENTS:
			- Url format (queues are addressable uing url)
			- Storage account
			- Queue (contains a set of messages, name of queue in lowercase)
			- Message (up to 64KB, supports TTL)
				- Up to 64KB
				- Supports TTL (time to live), -1 indicates no expiration, default TTL = 7 days

		- USE:
			- SDK for .NET
			- You need to manage deque if using PeekMessages (read the message without deletion, default messages number = 1)
			- If you use ReceiveMessages, the message is deleted from the queue after visibilityTimeout. You need to delete manually to ensure dequeue

















jean.leon.v@outlook.com


* IAAS vs PAAS:
	- App Service => Platform as a Service
	- Virtual Machines => Infrastructure as a Service  
Repasar imagen


* WEBAPP:
	- App Service Plan: Pricing model for apps and hosting options. B1, S1, etc. Is a resource that
	- Manual Scaling: 
		- Scale up (B1, S1, Premium, etc), Scale out (number of instances).
		- B: 3 parallel instances
		- S: 10 parallel instances
		- When upgrade the number of instances in Scale out, Azure takes car of load balancing betweeen instances (traffic coming to a one url is divided)
	- Auto Scaling:
		- Based on rules
		- Based on defined number of instances
	- Publishing apps: 
		- I can publish from VS Code.
		- I can create slots for differents environments (staging or development for example)
		- I can deploy a different version of the app to a slot
		- There is the "swap" functionallity to exchange the content of the webapp (main) and any slot. This is important when we don't want to lose the production version, if there is an error or bug in the new version swapped to main webapp, we can swap another time to return the last production version to main webapp.
		- When swap occurs, the settings of the app aren't changed (for example consume staging or production database)
	- App service logs:
		- Application logging (Filesystem and blob), level, web server logging, retention period, failed request tracing.
	- Diagnostic settings:
		- Send to log analytics workspace
	- Log stream:
		Real time logging (It has delay)
	- To create (PowerShell, cli):
		- Create Resource group (Name, Location <====> --name or -n, --location or -l)
		- Create App service plan (Name, Location, ResourceGroupName, Tier <====> --name or -n, --location or -l, --resource-group or -g, --sku)
		- Create WebApp (Name, Location, ResourceGroupName, AppServicePlan <====> --name or -n, --location or -l, --resource-group or -g, --plan)
		- PowerShell:
			Connect-AzAccount
			New-AzResourceGroup -Location 'EastUs' -Name ''
			New-AzAppServicePlan -ResourceGroupName '' -Name '' -Location 'EastUS' -Tier 'Free'
			New-AzWebApp -ResourceGroupName '' -Name ''  -Location 'EastUS' -AppServicePlan ''
		- CLI:
			az login
			az group create --name .... --location eastus
			az appservice plan create --resource-group ... --name ... --sku FREE --location eastus
			az webapp create --resource-group ... --name ... --plan ...
	- To create resources and deploy quickly:
		- CLI:
			az webapp list-runtimes
			az webapp up -n ... --runtime dotnet:8 --location eastus
	- Console:
		- Interacting with webapp folder, can run read commands
	- Advanced Tools (Kudu):
		- You can run cmd or powershell to interact with files of webapp in real time
		- You can view files and configuration of webapp (files deployed)
		- It's good for diagnostic (debug when It works in local but doesn't work in cloud environment)
	- Docker Container:
		- You can create a webapp based on an image fo azure container registry


* CONTAINER INSTANCE:
	- Create container instance: You can specify the following features:
		- Images source: quickstart, azure registry, other registry
		- Networking configuration: Ports, dns, public or private network
		- When create container from azure registry, this identify the os type of the image.


* CONTAINER REGISTRY:
	- Create new registry: You can specify the following features:
		- Pracing plan: Basic, Standart, Premium
		- Networking: Private is only available for premium pricing plan
		- Encryption: Enable is only available for premium pricing plan
	- Prepare the images:
		Use vs for mac for use "add docker support", look for another solution in vscode or similar
	- You can deploy images using vscode or console:
		az acr login --name mytestcompany
		docker tag testcontainer:latest mytestcompany.azurecr.io/testcontainer:latest
		docker push mytestcompany.azurecr.io/testcontainer:latest

		If you want to use vscode, change the azure account for docker and azure extensions
	- You need to enable "Admin user" in order to use the user to deploy the docker (registry name as admin username)
	- You can create webapp using Docker container option


* FUNCTION APP:
	- Create function app:
		- Runtimes: It supports .net, nodejs, python, java, powershell, custom handler
			- Custom Handler: This options is used when need another runtime or language (go, php, etc)
		- Functions needs an storage account, that's because underneath the function model is a file system
		- Hosting plan:
			- Consumption (Serverless): Recommended for free
			- Flex Consumption
			- Functions premium
			- App Service
			- Container App environment
		- Networking: Private or public access
			- Enable networking injection is only available in functions premium
	- Create function:
		- Select template (Http trigger, timmer trigger, etc)
		- Authorization level:
			- Function (token)
			- Anonymous (open to everyone)
			- Admin (higher level of security required)
		- Test and code:
			- Get function url with key and execute
		- Monitoring:
			- Success and failed executions
			- Logs: real time logs
		- Using function core tools:
			- func init
			- func new
			- func start
			- func azure functionapp publish "fa name"
	- Trigger types:
		- QueueTrigger
		- HttpTrigger
		- BlobTrigger
		- TimerTrigger
		- EventHubTrigger
		- ServiceBusQueueTrigger
		- ServiceBusTopicTrigger
		- EventGridTrigger
		- CosmosDBTrigger
		- DaprPublishOutputBinding
		- DaprServiceInvocationTrigger
		- DaprTopicTrigger
	- Bindings:
		(https://learn.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings?tabs=isolated-process%2Cpython-v2&pivots=programming-language-csharp)
		- Output: output method to storage the response data of function app trigger
			- Blob storage: Create an storage account, and a container, create a new container and configure in C# using the container name, wildcard name and connection string and BlobOutput attribute
		- Types:
			Blob storage
			Azure Cosmos DB
			Azure Data Explorer
			Azure SQL
			Dapr4
			Event Grid
			Event Hubs
			HTTP & webhooks
			IoT Hub
			Kafka3
			Mobile Apps (only v1.x)
			Notification Hubs (only v1.x)
			Queue storage
			Redis
			RabbitMQ3
			SendGrid
			Service Bus
			SignalR
			Table storage
			Timer
			Twilio
		- Example:
			BlobInput: Some stored file in blob storage to read the content as input
			BlobOutput: File name with name parameter like this: blob/{name}-output.txt
			BlobTrigger: New or updated blob is detected (Stream or string)

	- Durable functions:
		- Enable to have sequence of steps (functions). This steps use ActivityTrigger and the orchestrator uses OrchestrationTrigger and launch all the ActivityTrigger functions.
		  The OrchestrationTrigger can be called in another function of any trigger type.
		  Example:
		  	HttpTrigger -----calls in function-----> OrchestrationTrigger -----calls in function-----> ActivityTrigger1 ---output---> ActivityTrigger2 ---output---> output in orchestrator

		- Patterns:
			- Function chaining pattern: (READYâ€“)
				A[Orchestration Function Starts] --> B[Function A]
			    B --> C[Function B]
			    C --> D[Function C]
			    D --> E[Orchestration Function Completes]

			- Fan in / Fan out pattern:
				F1 calls three functions in parallel (F2), It will wait for all three functions to finish before move to F3
				A[Orchestration Function Starts] --> B[Function Fan-Out]
			    B --> C1[Function C1]
			    B --> C2[Function C2]
			    B --> C3[Function C3]
			    C1 --> D[Function Fan-In]
			    C2 --> D[Function Fan-In]
			    C3 --> D[Function Fan-In]
			    D --> E[Orchestration Function Completes]

			- Asynchronous API pattern:
				F1 start long task and give register of execution as output. 
				Another function GetStatus can call the output and monitoring execution.
				Another function DoWork can call the output and monitoring execution finish to execute the work

			- Monitor pattern:
				A function is waiting something to happen in another function, and execute when "that something" has happened

			- Human interaction pattern:
				A function request approval to call other functions, the function have a return a timeout if human approval is not received

		- Enable durable functions support:
			- For Javascript runtime, create diretly from azure portal and download the npm durable-functions package, then restart the funcion app
			- For .NET runtime:
				- Create the project (durable function orchestrator) from vscode and use azurite extension for test local storage (Azurite: Start in command palette)
				- For run in mac:
					- download the nuget package: Contrib.Grpc.Core.M1
					- add this before last project tag:
						<Target Name="CopyGrpcNativeAssetsToOutDir" AfterTargets="Build"> <ItemGroup> <NativeAssetToCopy Condition="$([MSBuild]::IsOSPlatform('OSX'))" Include="$(OutDir)runtimes/osx-arm64/native/*" /> </ItemGroup> <Copy SourceFiles="@(NativeAssetToCopy)" DestinationFolder="$(OutDir).azurefunctions/runtimes/osx-arm64/native" /> </Target>
				

* AZURE STORAGE ACCOUNT
	- Managed disks (usually fixed) are more expensive than blob storage (on demand).
	- Blob storage (or unmanaged storage) is the cheapest way to store files within azure.
	- Price depends on region
	- Storage account performance:
		* You pay for storage and you pay for the number of request per month.
		- GPV2 = Standard storage: 
		- Premium: It costs way more to store it, but you save a lot of money on the access. (If you need read thousands of files per second)
	- Storage account redundancy:
		- LRS: Locally
		- GRS
		- ZRS
		- GZRS
	- Blob storage tiers:
		- Premium
		- Hot
		- Cool
		- Archive: Hard to access, intended for backups and files that are rarely used.
	- Create storage account
		- performance: GPV2 or premium
		- redundancy
		- secure transfer
		- public access
		- Secutiry access: storage account keys or/and azure AD (entra)
		- data lake storage gen2: big data scenarios, change the file system
		- Networking: 
			- public, selected virtual networks, private (private endpoint)
			- microsoft network routing VS internet routing (slower)
		- Data protection:
			- soft delete
			- versioning and tracking
		- Encryption:
			- MMK
			- CMK
	- Access tier:
		- Hot: Frequently accesed, day-to-day usage scenarios
		- Cool: Infrequently accessed, backup scenarios
	- Storage browser:
		- Tool for search data in storage account easyly (containers, files, table, queues)
	- Azure storage account contains those storage services:
		- AZURE BLOB STORAGE
		- AZURE TABLE STORAGE
		- AZURE FILE STORAGE
		- AZURE QUEUE STORAGE
	- Blob storage vs File storage:
		- file storage uses a file system. (classic) hosted on azure, blob storage is highly scalable object storage solution.

	- AZURE BLOB STORAGE (Containers):
		- To access to blobs programaticlly
		- Use the SDK to manage container, blob level objects (BlobServiceClient, BlobContainerClient, BlobCLient)
		- StartCopyFromUriAsync and SetMetadataAsync as important methods.
		- AzCopy:
			- To copy files between container, even between different subscriptions (or different cloud providers)

	- Access Keys (if chosen storage account keys on creation):
		- Azure provide a pair of keys, they can be rotated manually (invalid old keys) or using a timmer (set rotation reminder)
		- YOU DONT WANT TO GIVE AWAY YOUR KEYS (use shared access signature instead)
	- Shared access signature (SAS)
		- To provide access to clients who should not be trusted.
		- You CANNOT revoke this access directly, unless rotate the signing key (based on access key). This invalids all the SAS generated using the rotated key.
		- There is another way to revoke the SAS acces, using stored access policy.

	- Data protection:
		- Recovery:
			- Operational backup: Using azure backup vault and a backup policy (retention days)
			- Point in time restore
			- Soft delete for blobs and containers (retention days)
	- Object replication
	- Lifecycle management:
		- Add rules that will move files from the hot tier to the cool or archive tier based on the last access date (you can configure to delete instead to move files)

* AZURE COSMOS DB
	- Fully managed NoSQL and relational database service for scalable, high performance applications.
	* KEYS:
		- Unlimited elastic write and read sclability
		- 99.999% read and write availability all around the world
		- Guarantedd read and writes serverd in < 10ms (near real time reads and writes)
	* APIs:
		- COSMOS DB FOR NOSQL: Core Native api for working with documents (JSON), supports familiar SQL language. Choose this when start from scratch (there is no data). ------> NON RELATIONAL
		- COSMOS DB FOR POSTGRESQL: OpenSource postgresql. Choose this if you need tables, foreign keys, primary keys, etc ------> RELATIONAL
		- COSMOS DB FOR MONGODB: MongoDb datbase for working with documents (BSON). Choose this if you have existing mongoDb data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR APACHE CASSANDRA: Cassandra database, mongodb and cassandra APIs are compatible (you can migrate without changes). Choose this if you have existing cassandra data and plan to migrate to azure ------> NON RELATIONAL
		- COSMOS DB FOR TABLE: Compatible with azure table storage (you can migrate without changes). Choose this if you have existing azure table storage data ------> NON RELATIONAL
		- COSMOS DB FOR GREMLIN: Graph database service using gremlin query language (nodes, edges). Choose if you need relationships between data ------> NON RELATIONAL
	* HIERARCHY:
		- Database account > database > container > item, sps, fucntions, triggers, conflicts
	* TYPES:
		- database: Depends on api (NOSQL, POSTGRESQL, MONGODB, CASSANDRA, TABLE, GREMLIN)
		- container and items:
			- NOSQL: container > item
			- POSTGRESQL: table > row
			- MONGODB: collection > document
			- CASSANDRA: table > row
			- TABLE: table > item
			- GREMLIN: graph > node/edge
	- Create NOSQL:
		- Capacity mode:
			- Provisioned throughput: Fix an ammount of provisioned throughput to consume (RUs/s) regardless of consumption. Use free tier for this if possible
			- Serverless: Pay as you use
		- Limit throughput: Protects your account from cost overruns
		- Global distribution:
			- geo-redundancy: I created this in west and it will also create a version in east
			- multi-region writes
		- Backup Policy (NOT EXAM):
			- Periodic backup (interval, retention, copies, redundancy)
			- Continuos
	* Global replication:
		- You can add more regions (replications) as a READ REGIONS (you can program the application to read from the closest region to the client)
		- Muti region writes:
			- Enable WRITE and READ in all selected regions, this will double the cost
		- If the cosmos db has only a WRITE region (primary region), you can perform read and write operations in that region.
	- Keys:
		- You can access to cosmos using the URI (or endpoint, exposed on internet or azure virtual network) and a primary or secondary key
		- You have primary/seconday keys for Read-write regions, and read-only regions
	- Data explorer:
		- Create new container:
			- Create new db or use existing
			- Autoscale or manual
			- Set the max RU/s
			- Partition key: Distribute data across partitions for scalability
			- Unique keys: Specific unique keys in a partition
			- id key is required in each item and is a default unique key
		- Connect
			- When new Item is created, cosmos add 5 properties to object:
				- rid (row id), _self (self reference)
	* Data consistency:
		- Strong > Bounded staleness > session > consistent prefix > eventual
		- Cosmos uses "session" consistency as default
	* SDK:
		- code
	- Sp, trigger, user-defined functions:
		- sp: for large data executions. You can execute sp from sdk
		- triggers: to validate or add something before (pre trigger) and after (post trigger) creation/deletion/update
		- user-defined functions: function that you can use inside a query. To calculate fields.

	* CHANGE FEED NOTIFICATIONS:
		- Change feed in Azure Cosmos DB is a persistent record of changes to a container in the order they occur
		- This doesn't recognize item deletion or filter between insert and update operations. (Use soft delete + TTL to overcome deletion)
		- READ:
			- Push model: change feed push work to client, client has bussines loginc for processing work. Delegate hard work to cosmos (RECOMMENDED)
				- Azure functions:
					- Use azure functions (azure cosmos trigger) to execute some task when an item is created or updated.
				- Change feed processor library:
					- The monitored container
					- The lease container
					- The compute instance
					- The delegate

			- Pull model: client pull work from server, client has bussines logic for processing work, store state and handles load balancing. Extra low level control (NOR RECOMMENDED)


* MICROSOFT IDENTITY PLATFORM (ENTRA ID)

	* Components:
		- OAuth 2.0 and OpenID Connect standard-compliant authentication service
		- Open-source libraries
		- Microsoft identity platform endpoint
		- Application management portal
		- Application configuration API and powershell

	* Register app in the Azure portal:
		- When register an app with microsoft entra ID:
			- You're creating an IDENTITY CONFIGURATION.
			- Your must choose whether it is SINGLE TENANT or MULTI TENANT
			- An APPLICATION OBJECT (Globally unique app instance) is created in the home tenant.
			- A SERVICE PRINCIPAL OBJECT is created in the home tenant
			- You have a globally UNIQUE ID for the app (app or clientID)
			- You can add secrets, certificates, scopes and so on
	* IDENTITY CONFIGURATION:
		- Allows to integrate with microsoft entra ID
	* TENANT:
		- An instance of Azure Microsoft Entra ID that an organization or individual uses to manage users, apps, etc.
	* APPLICATION OBJECT:
		- Application ------------ one to one ------------ Application object
	* SCHEMA:
		- https://app.diagrams.net/#G1l1P8atQcV2vNu8CWlNm4b7_yU5Etr9_D#%7B%22pageId%22%3A%22MnA-WK4r-ufKpTyJempP%22%7D
	* INTEGRATION:
		- You need AzureAd in appsettings.json and use:
		
			- Cookie based auth (stateless or session):
				services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApp(Configuration.GetSection("AzureAd"));

	        - JWT based auth (stateless):
	        	services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
	                .AddMicrosoftIdentityWebApi(Configuration.GetSection("AzureAd"));

		- AzureAd section:
			- Instance: From directory
			- Domain: From directory
			- ClientId: From application (application Id)
			- TenantId: From directory could be:
				- "common": Auth request an organization account, allow users (and personal account) from any organization, multitenant
				- "organizations": Intended only for organizational users, not Microsoft personal accounts
				- "specific-tenant": Specific tenantID of the directory}
	* Permissions and consent:
		- permissions = scopes
		- Permission types:
			- DELEGATED ACCESS:
				- App has a signed-in user. The app acts as a signed-in users when it makes calls to target resource. (Microsoft entra id)
			- APP ONLY ACCESS:
				- App doesn't have a signed-in user. Only administrator can consent permissions for this app
		- Consents:
			- STATIC USER CONSENT:
				- Azure portal defined permissions (app configuration)
			- INCREMENTAL AND DYNAMIC USER CONSENT:
				- Ignore static and request permissions incrementally
				- To do so, specify the scopes your app need at any time by including new scopes in "scope" parameter when requesting access token (without the need to predefine them in the app registration)
				- The user has to consent the new permissions (ONLY APPLIES TO DELEGATED PERMISSIONS!!!!!!)
			- ADMIN CONSENT:
				- Is required when your app needs access to certain high-privilege permissions
		- Conditional access: TO PRACTICE

	* MSAL:
		The librery enables devs to get tokens from Microsoft identity platform to authenticate and access to apis like microsoft graph, thir party apis, etc.
		Authentication flows:
			- Authorization code
			- Client credentials
			- Device code
			- Implicit grant
			- On-behalf-of (OBO)
			- Username/password (ROPC)
			- Integrated Windows authentication (IWA)
		Client applications:
			- Public (desktop, browserless APIs, mobile, browser apps)
			- Confidentials (web apps, web api apps, deamon/service apps)
		Complete frontend-backend flow:
			- Steps:
				*** DELEGATED ACCESS ***
				- Register client (spa) app registration
				- Register api app registration
				- Configure client app to authenticate against azure entra id using its client id and tenant of directory
				- Configure api app to validate incomming tokens using azure ad (jwt schema and azure configuration), and protect endpoints using Authorize
				- Add new scopes in api app registration in "expose API", set owner for this app
				- Register the created api scopes in client app registration, so the client can obtain a token to request api protected endpoints
				- Add the required scopes inthe login scopes in the client app so you can get a token authorized for all the needed scopes (the login prompt ask you for authorize that permissions)
				- Validate claims in the backend if needed (roles, policies, etc)
				- The client will be able to request microsoft graph or other resources (depending of the scopes) using its token (avoid this, and delegate this responsability for the backend)
				*** APP ONLY ACCESS ***
				- If you need request microsoft graph or other services from the backend, you need to use client credentials or certificate
			- https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-configure-app-expose-web-apis

	* SAS
		- Signed URI that point to one or more storage resources
		- Includes a Token that indicates how the resources might be accessed by the client
		
		- Types:
			- USER DELEGATION SAS: (RECOMMENDED)
				Delegation SAS secured with microsoft entra credentials and permissions for the SAS. (BLOB STORAGE)
			- SERVICE SAS:
				Service SAS secured with storage account key. (BLOB, QUEUE, TABLE, AZURE FILES)
			- ACCOUNT SAS:
				Account SAS secured with storage account key. Delegate access to resources in one or more of the storage services.
			- SAS = URI + Authorization params
				- sp (access rights)
				- st, se (start access datatime, end access datetime)
				- sv (version of the storage API)
				- sr (kind of storage, blob, queue, etc)
				- sig (cryptographic signature)
			- When to use:
				- Service where users read and write their own data to my storage account. (front end proxy service / SAS provider service)
				- SAS is required when you want to copy Blobs or files to other blobs/storage accounts
			- STORED ACCESS POLICIES:
				- Consists of the start time, expiry time, and permissions for the signature (SERVICE LEVEL SAS).
				- You can create service level SAS (BLOB, QUEUE, TABLE, AZURE FILES) using existing stored access policies or setting custom properties
				- Services that support (BLOB, QUEUE, TABLE, AZURE FILES)


* MICROSOFT GRAPH
	
	- Microsoft graph Connectors: Incoming data direction (external data sources: Jira, Google drive, etc)
	- Microsoft graph Data Connect: Cached data serves as data source for azure development tools
	- You need auth tokens of an app registration. to request microsoft graph api
	- Resources: me, teams, groups, etc
	- Query parameters: filters

	* SDK
		- Sevice library: models, request builders from graph metadata
		- Core library: features to work with graph services
		- TO REQUEST:
			- YOU MUST CHOOSE THE AUTHENTICATION PROVIDER:
				- Authorization code provider - (DELEGATED)
				- Client credentials provider - (APP ONLY)
				- On-behalf-of provider - (DELEGATED)
				- Interactive provider - (DELEGATED)
				- Integrated Windows provider - (DELEGATED)
				- Username/password provider - (DELEGATED)
				- Device code provider - (DELEGATED)
					########## - YOU NEED CERTIFICATE OR CLIENT SECRET in client credentials case ##########
					########## - YOU NEED TO ENABLE MOBILE AND DESKTOP FLOWS in device code case ##########
			- YOU MUST ADD THE ENOUGH PERMISSIONS FOR THE APP REGISTRATION
			- YOU NEED TO GRANT PERMISSION AS ADMIN USER IF NEEDED
			- You can interact with all the services of microsoft graph using the selected authentication provider
			- There are some endpints that don't work using client credentials (/me)

	* Request:
		- You can use MSAL to acquire access token to microsoft graph (specifying scopes)

	* BEST PRACTICES:
		- Use least privilege
		- Use the correct permission type based on scenarios
		- Consider the end user and admin experience (consents)
		- Consider multi-tenant applications
		- Handle responses effectively (pagination, Evolvable enumerations)
		- Cache locally for specific scenarios to avoid make a lot of requests to graph


* AZURE CACHE FOR REDIS:
	
	- Provides in memory data store absed on REDIS
	- To process large volumes of app requests bykeeping frequently accessed data in the server memory (read and write quickly)
	- Offers REDIS OPEN SOURCE and REDIS ENTERPRISE

	* SCENARIOS:
		- DATA CACHE (load data into the cache only as needed)
		- CONTENT CACHE (quick access to static content compared to backend datastores - templates, headers, footers)
		- SESSION STORE (user session data, shooping carts, us cookies as key to query the inmemory cache, faster than relational db)
		- JOB AND MESSAGE QUEUING (for requests that take time to execute)
		- DISTRIBUTED TRANSACTIONS (Redis supports executing a batch of commands as a single transaction)

	* TIERS: (determines the size and performance)
		- BASIC < STANDARD < PREMIUM < ENTERPRISE < ENTERPRISE FLASH
		- ENTERPRISE and ENTERPRISE FLASH allows clustering to automaticlly split dataset amoing multiple nodes

	* CONFIGURE AN INSTANCE:
		- The name contains numbers, letters and '-' (1 - 63 char), cannot startor end with '-', consecutive '-' invalid
		- Always place the cache instance and app in the same region (or as close as possible, even outside of azure), different region = more latency, less reliability
		- Microsoft recommends to use Standard tier or higher for production systems.

	* ACCESS:
		- Redis command line tool
		- Add TTL (Time to live - expiration time) to keys if needed. Redis stores the date when a key expires
		- Expire time resolution is always 1 ms, and expiration can be set using s or ms
		- Use from client:
			- The host name is the public internet addess of the cache instance. Use access key (primary or secondary)
			- Use from .NET
				- Add package StackExchange.Redis
				- Use host address, port, access key or CONNECTION STRING from azure
				- Connection string contains "ssl" (encrypted communication) and "abortConnection" (allows a connection to be created even if the server is unavailable at that moment)
				- The ConnectionMultiplexer class is optimized to manage connections efficiently
				- Get the db instance from the redis connection (after execute ConnectionMultiplexer.Connect)
				- You can execute specific commands in db object, or defined methods
				- You can store complex values (classes) and then serialize to JSON or XML and then retrieve and deserialize to class again.
				- Dispose the connection object when finish













